@book{abramowitzHandbookMathematicalFunctions1965,
  title = {Handbook of {{Mathematical Functions}}: {{With Formulas}}, {{Graphs}}, and {{Mathematical Tables}}},
  shorttitle = {Handbook of {{Mathematical Functions}}},
  author = {Abramowitz, Milton and Stegun, Irene A.},
  year = {1965},
  month = jan,
  publisher = {{Courier Corporation}},
  abstract = {Vast compendium - 29 sets of tables, some to as high as 20 places.},
  googlebooks = {MtU8uP7XMvoC},
  isbn = {978-0-486-61272-0},
  langid = {english},
  keywords = {mathematics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Courier Corporation/1965/Abramowitz_Stegun_1965_Handbook of Mathematical Functions.pdf}
}

@article{ajroldiConformalPredictionBands2023,
  title = {Conformal Prediction Bands for Two-Dimensional Functional Time Series},
  author = {Ajroldi, Niccol{\`o} and Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
  year = {2023},
  month = nov,
  journal = {Computational Statistics \& Data Analysis},
  volume = {187},
  pages = {107821},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2023.107821},
  urldate = {2023-10-25},
  abstract = {Time evolving surfaces can be modeled as two-dimensional Functional time series, exploiting the tools of Functional data analysis. Leveraging this approach, a forecasting framework for such complex data is developed. The main focus revolves around Conformal Prediction, a versatile nonparametric paradigm used to quantify uncertainty in prediction problems. Building upon recent variations of Conformal Prediction for Functional time series, a probabilistic forecasting scheme for two-dimensional functional time series is presented, while providing an extension of Functional Autoregressive Processes of order one to this setting. Estimation techniques for the latter process are introduced, and their performance are compared in terms of the resulting prediction regions. Finally, the proposed forecasting procedure and the uncertainty quantification technique are applied to a real dataset, collecting daily observations of Sea Level Anomalies of the Black Sea.},
  keywords = {conformal-prediction,functional-autoregressive-process,functional-time-series,multidimensional-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2023/Ajroldi et al_2023_Conformal prediction bands for two-dimensional functional time series2.pdf;/Users/steven/Zotero/storage/FBHLPVDH/S0167947323001329.html}
}

@inproceedings{allenMultiwayFunctionalPrincipal2013,
  title = {Multi-Way Functional Principal Components Analysis},
  booktitle = {2013 5th {{IEEE International Workshop}} on {{Computational Advances}} in {{Multi-Sensor Adaptive Processing}} ({{CAMSAP}})},
  author = {Allen, Genevera I.},
  year = {2013},
  month = dec,
  pages = {220--223},
  doi = {10.1109/CAMSAP.2013.6714047},
  urldate = {2024-01-15},
  abstract = {Many examples of multi-way or tensor-valued data, such as in climate studies, neuroimaging, chemometrics, and hyperspectral imaging, are structured meaning that variables are associated with locations. Tensor decompositions, or higher-order principal components analysis (HOPCA), are a classical method for dimension reduction and pattern recognition for this multi-way data. In this paper, we introduce novel methods for Functional HOPCA that decompose the tensor data into components that are smooth with respect to the known data structure. Through numerical experiments we demonstrate the comparative advantages of our methods for smooth signal recovery from multi-way data.},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2013/Allen_2013_Multi-way functional principal components analysis.pdf;/Users/steven/Zotero/storage/6T6QBASA/6714047.html}
}

@article{alvarezKernelsVectorValuedFunctions2012,
  title = {Kernels for {{Vector-Valued Functions}}: {{A Review}}},
  shorttitle = {Kernels for {{Vector-Valued Functions}}},
  author = {{\'A}lvarez, Mauricio A. and Rosasco, Lorenzo and Lawrence, Neil D.},
  year = {2012},
  month = mar,
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {4},
  number = {3},
  pages = {195--266},
  issn = {1935-8237},
  doi = {10.1561/2200000036},
  urldate = {2023-10-27},
  abstract = {Kernel methods are among the most popular techniques in machine learning. From a regularization perspective they play a central role in regularization theory as they provide a natural choice for the hypotheses space and the regularization functional through the notion of reproducing kernel Hilbert spaces. From a probabilistic perspective they are the key in the context of Gaussian processes, where the kernel function is known as the covariance function. Traditionally, kernel methods have been used in supervised learning problems with scalar outputs and indeed there has been a considerable amount of work devoted to designing and learning kernels. More recently there has been an increasing interest in methods that deal with multiple outputs, motivated partially by frameworks like multitask learning. In this monograph, we review different methods to design or learn valid kernel functions for multiple outputs, paying particular attention to the connection between probabilistic and functional methods.},
  keywords = {kernel-models,multivariate-functional-data,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Foundations and Trends® in Machine Learning/2012/Álvarez et al_2012_Kernels for Vector-Valued Functions.pdf}
}

@article{aneiros-perezFunctionalMethodsTime2011,
  title = {Functional Methods for Time Series Prediction: A Nonparametric Approach},
  shorttitle = {Functional Methods for Time Series Prediction},
  author = {{Aneiros-P{\'e}rez}, Germ{\'a}n and Cao, Ricardo and {Vilar-Fern{\'a}ndez}, Juan M.},
  year = {2011},
  journal = {Journal of Forecasting},
  volume = {30},
  number = {4},
  pages = {377--392},
  issn = {1099-131X},
  doi = {10.1002/for.1169},
  urldate = {2023-09-19},
  abstract = {The problem of prediction in time series using nonparametric functional techniques is considered. An extension of the local linear method to regression with functional explanatory variable is proposed. This forecasting method is compared with the functional Nadaraya{\textendash}Watson method and with finite-dimensional nonparametric predictors for several real-time series. Prediction intervals based on the bootstrap and conditional distribution estimation for those nonparametric methods are also compared. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {functional-data-analysis,functional-time-series,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Forecasting/2011/Aneiros-Pérez et al_2011_Functional methods for time series prediction.pdf;/Users/steven/Zotero/storage/ELJP9FBY/for.html}
}

@article{aneirosCommentsProbabilityEnhanced2016,
  title = {Comments on: {{Probability}} Enhanced Effective Dimension Reduction for Classifying Sparse Functional Data},
  shorttitle = {Comments On},
  author = {Aneiros, Germ{\'a}n and Vieu, Philippe},
  year = {2016},
  month = mar,
  journal = {TEST},
  volume = {25},
  number = {1},
  pages = {27--32},
  issn = {1863-8260},
  doi = {10.1007/s11749-015-0471-1},
  urldate = {2023-10-29},
  langid = {english},
  keywords = {dimension-reduction,functional-data-analysis,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/TEST/2016/Aneiros_Vieu_2016_Comments on.pdf}
}

@article{anscombeGraphsStatisticalAnalysis1973,
  title = {Graphs in {{Statistical Analysis}}},
  author = {Anscombe, F. J.},
  year = {1973},
  month = feb,
  journal = {The American Statistician},
  volume = {27},
  number = {1},
  pages = {17--21},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1080/00031305.1973.10478966},
  urldate = {2023-09-13},
  keywords = {data-visualisation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The American Statistician/1973/Anscombe_1973_Graphs in Statistical Analysis.pdf}
}

@article{anteroMenstrualCycleHormonal2023,
  title = {Menstrual Cycle and Hormonal Contraceptive Phases' Effect on Elite Rowers' Training, Performance and Wellness},
  author = {Antero, Juliana and Golovkine, Steven and Niffoi, Louis and Meigni{\'e}, Alice and Chassard, Tom and Delarochelambert, Quentin and Duclos, Martine and Maitre, Carole and Maciejewski, Hugo and Diry, Allison and Toussaint, Jean-Fran{\c c}ois},
  year = {2023},
  month = feb,
  journal = {Frontiers in Physiology},
  volume = {14},
  pages = {1110526},
  issn = {1664-042X},
  doi = {10.3389/fphys.2023.1110526},
  urldate = {2023-10-24},
  abstract = {Objectives: To investigate the effect of menstrual cycle (MC) and hormonal contraception (HC) phases in elite rowers training, performance and wellness monitoring.,  Methods: Twelve French elite rowers were follow-up for 4,2 cycles on average in their final preparation for the Olympics and Paralympics Games in Tokyo 2021 through an on-site longitudinal study based on repeated measures. Daily self-reported evaluation using Likert rating scales of wellness (sleep quality, fitness, mood, injuries' pain), menstrual symptoms and training parameters (perceived exertion and self-assessment of performance) were collected (n = 1,281) in parallel to a coach evaluation of rowers' performance (n = 136), blinded to theirs MC and HC phases. Salivary samples of estradiol and progesterone were collected in each cycle to help to classify the MC into 6 phases and HC into 2{\textendash}3 phases depending on the pills' hormone concentration. A chi-square test normalized by each rower was used to compare the upper quintile scores of each studied variable across phases. A Bayesian ordinal logistic regression was applied to model the rowers' self-reported performance.,  Results: Rowers with a natural cycle, n = 6 ( + 1 amenorrhea) evaluate their performance and wellness with significant higher score indices at the middle of their cycle. Top assessments are rarer at the premenstrual and menses phases, when they more frequently experience menstrual symptoms which are negatively correlated with their performance. The HC rowers, n = 5, also better evaluate their performance when taking the pills and more frequently experience menstrual symptoms during the pill withdrawal. The athletes self-reported performance is correlated with their coach's evaluation.,  Conclusion: It seems important to integrate MC and HC data in the wellness and training monitoring of female athletes since these parameters vary across hormonal phases affecting training perception of both athlete and coach.},
  pmcid = {PMC9981658},
  pmid = {36875020},
  keywords = {menstrual-cycle,rowing,sport-science,women},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Frontiers in Physiology/2023/Antero et al_2023_Menstrual cycle and hormonal contraceptive phases’ effect on elite rowers’.pdf}
}

@article{antonModelbasedClusteringFunctional2023,
  title = {Model-Based Clustering of Functional Data via Mixtures of t Distributions},
  author = {Anton, Cristina and Smith, Iain},
  year = {2023},
  month = may,
  journal = {Advances in Data Analysis and Classification},
  issn = {1862-5355},
  doi = {10.1007/s11634-023-00542-w},
  urldate = {2023-10-24},
  abstract = {We propose a procedure, called T-funHDDC, for clustering multivariate functional data with outliers which extends the functional high dimensional data clustering (funHDDC) method (Schmutz et al. in Comput Stat 35:1101{\textendash}1131, 2020) by considering a mixture of multivariate t distributions. We define a family of latent mixture models following the approach used for the parsimonious models considered in funHDDC and also constraining or not the degrees of freedom of the multivariate t distributions to be equal across the mixture components. The parameters of these models are estimated using an expectation maximization algorithm. In addition to proposing the T-funHDDC method, we add a family of parsimonious models to C-funHDDC, which is an alternative method for clustering multivariate functional data with outliers based on a mixture of contaminated normal distributions (Amovin-Assagba et al. in Comput Stat Data Anal 174:107496, 2022). We compare T-funHDDC, C-funHDDC, and other existing methods on simulated functional data with outliers and for real-world data. T-funHDDC outperforms funHDDC when applied to functional data with outliers, and its good performance makes it an alternative to C-funHDDC. We also apply the T-funHDDC method to the analysis of traffic flow in Edmonton, Canada.},
  langid = {english},
  keywords = {clustering,functional-data-analysis,functional-principal-components,model-based-clustering,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Advances in Data Analysis and Classification/2023/Anton_Smith_2023_Model-based clustering of functional data via mixtures of t distributions.pdf}
}

@article{arnoneAnalyzingDataComplicated2023,
  title = {Analyzing Data in Complicated {{3D}} Domains: {{Smoothing}}, Semiparametric Regression, and Functional Principal Component Analysis},
  shorttitle = {Analyzing Data in Complicated {{3D}} Domains},
  author = {Arnone, Eleonora and Negri, Luca and Panzica, Ferruccio and Sangalli, Laura M.},
  year = {2023},
  journal = {Biometrics},
  volume = {79},
  number = {4},
  pages = {3510--3521},
  issn = {1541-0420},
  doi = {10.1111/biom.13845},
  urldate = {2024-01-28},
  abstract = {In this work, we introduce a family of methods for the analysis of data observed at locations scattered in three-dimensional (3D) domains, with possibly complicated shapes. The proposed family of methods includes smoothing, regression, and functional principal component analysis for functional signals defined over (possibly nonconvex) 3D domains, appropriately complying with the nontrivial shape of the domain. This constitutes an important advance with respect to the literature, because the available methods to analyze data observed in 3D domains rely on Euclidean distances, which are inappropriate when the shape of the domain influences the phenomenon under study. The common building block of the proposed methods is a nonparametric regression model with differential regularization. We derive the asymptotic properties of the methods and show, through simulation studies, that they are superior to the available alternatives for the analysis of data in 3D domains, even when considering domains with simple shapes. We finally illustrate an application to a neurosciences study, with neuroimaging signals from functional magnetic resonance imaging, measuring neural activity in the gray matter, a nonconvex volume with a highly complicated structure.},
  copyright = {{\copyright} 2023 The Authors. Biometrics published by Wiley Periodicals LLC on behalf of International Biometric Society.},
  langid = {english},
  keywords = {functional data analysis,functional principal component analysis,neuroimaging,smoothing},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/2023/Arnone et al_2023_Analyzing data in complicated 3D domains.pdf}
}

@article{aueTestingStationarityFunctional2020,
  title = {Testing for Stationarity of Functional Time Series in the Frequency Domain},
  author = {Aue, Alexander and van Delft, Anne},
  year = {2020},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  pages = {2505--2547},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1895},
  urldate = {2023-09-19},
  abstract = {Interest in functional time series has spiked in the recent past with papers covering both methodology and applications being published at a much increased pace. This article contributes to the research in this area by proposing a new stationarity test for functional time series based on frequency domain methods. The proposed test statistics is based on joint dimension reduction via functional principal components analysis across the spectral density operators at all Fourier frequencies, explicitly allowing for frequency-dependent levels of truncation to adapt to the dynamics of the underlying functional time series. The properties of the test are derived both under the null hypothesis of stationary functional time series and under the smooth alternative of locally stationary functional time series. The methodology is theoretically justified through asymptotic results. Evidence from simulation studies and an application to annual temperature curves suggests that the test works well in finite samples.},
  keywords = {functional-data-analysis,functional-time-series,spectral-analysis,stationarity-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2020/Aue_Delft_2020_Testing for stationarity of functional time series in the frequency domain.pdf}
}

@article{averyRKHSbasedFunctionalNonparametric2014,
  title = {{{RKHS-based}} Functional Nonparametric Regression for Sparse and Irregular Longitudinal Data},
  author = {Avery, Matthew and Wu, Yichao and Helen Zhang, Hao and Zhang, Jiajia},
  year = {2014},
  journal = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
  volume = {42},
  number = {2},
  eprint = {43185178},
  eprinttype = {jstor},
  pages = {204--216},
  publisher = {{[Statistical Society of Canada, Wiley]}},
  issn = {0319-5724},
  urldate = {2023-10-27},
  abstract = {This paper focuses on sparse and irregular longitudinal data with a scalar response. The predictor consists of sparse and irregular observations on predictor trajectories, potentially contaminated with measurement errors. For this type of data, Yao, M{\"u}ller, \& Wang (2005a) proposed a principal components analysis through conditional expectation (PACE) approach, which is capable of predicting each predictor trajectory based on sparse and irregular observations. Nonparametric functional data analysis provides an attractive alternative due to its high flexibility. Early work includes functional additive models as in M{\"u}ller \& Yao (2008) and Ferraty \& Vieu (2006), which are mainly based on kernel smoothing methods. In this work, we propose a new functional nonparametric regression framework based on reproducing kernel Hubert spaces (RKHS). The proposed method involves two steps. The first step is to estimate each predictor trajectory based on sparse and irregular observations using PACE. The second step is to conduct a RKHS-based nonparametric regression using the estimated predictor trajectories. Our approach shows improvement over existing methods in simulation studies as well as in a real data example. Les auteurs se penchent sur les donn{\'e}es longitudinales {\'e}parses et irr{\'e}guli{\`e}res pr{\'e}sentant une variable r{\'e}ponse scalaire. Les pr{\'e}dicteurs sont des observations {\'e}parses et irr{\'e}guli{\`e}res sur leur trajectoire qui peuvent {\^e}tre contamin{\'e}es par des erreurs de mesure. Pour ce type de donn{\'e}es, Yao, M{\"u}ller et Wang (2005a) proposent une analyse en composantes principales par les esp{\'e}rances conditionnelles (APEC) qui permet de pr{\'e}dire la trajectoire de chaque pr{\'e}dicteur sur la base de donn{\'e}es {\'e}parses et irr{\'e}guli{\`e}res. L'analyse fonctionnelle non param{\'e}trique de donn{\'e}es pr{\'e}sente une option int{\'e}ressante compte tenu de sa grande flexibilit{\'e}. Les travaux de M{\"u}ller et Yao (2008) et ceux de Ferraty et Vieu (2006) traitent d'ailleurs des mod{\`e}les fonctionnels additifs en se basant principalement sur le lissage par la m{\'e}thode du noyau. Dans le pr{\'e}sent article, les auteurs proposent un nouveau cadre pour la r{\'e}gression fonctionnelle non param{\'e}trique bas{\'e} sur un espace de Hubert {\`a} noyau reproduisant (EHNR). La m{\'e}thode propos{\'e}e comporte deux {\'e}tapes. D'abord, la trajectoire de chaque pr{\'e}dicteur est estim{\'e}e {\`a} partir de donn{\'e}es {\'e}parses et irr{\'e}guli{\`e}res {\`a} l'aide de l'APEC. Ensuite, une r{\'e}gression non param{\'e}trique bas{\'e}e sur l'EHNR est ajust{\'e}e en se basant sur la trajectoire estim{\'e}e des pr{\'e}dicteurs. L'approche propos{\'e}e pr{\'e}sente des am{\'e}liorations par rapport aux m{\'e}thodes existantes dans le cadre d'{\'e}tudes de simulation et d'exemples bas{\'e}s sur des donn{\'e}es r{\'e}elles.},
  keywords = {functional-data-analysis,functional-regression,reproducing-kernel-hilbert-space,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Canadian Journal of Statistics / La Revue Canadienne de Statistique/2014/Avery et al_2014_RKHS-based functional nonparametric regression for sparse and irregular.pdf}
}

@article{banfieldModelBasedGaussianNonGaussian1993,
  title = {Model-{{Based Gaussian}} and {{Non-Gaussian Clustering}}},
  author = {Banfield, Jeffrey D. and Raftery, Adrian E.},
  year = {1993},
  journal = {Biometrics},
  volume = {49},
  number = {3},
  eprint = {2532201},
  eprinttype = {jstor},
  pages = {803--821},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2532201},
  urldate = {2023-11-21},
  abstract = {The classification maximum likelihood approach is sufficiently general to encompass many current clustering algorithms, including those based on the sum of squares criterion and on the criterion of Friedman and Rubin (1967, Journal of the American Statistical Association 62, 1159-1178). However, as currently implemented, it does not allow the specification of which features (orientation, size, and shape) are to be common to all clusters and which may differ between clusters. Also, it is restricted to Gaussian distributions and it does not allow for noise. We propose ways of overcoming these limitations. A reparameterization of the covariance matrix allows us to specify that some, but not all, features be the same for all clusters. A practical framework for non-Gaussian clustering is outlined, and a means of incorporating noise in the form of a Poisson process is described. An approximate Bayesian method for choosing the number of clusters is given. The performance of the proposed methods is studied by simulation, with encouraging results. The methods are applied to the analysis of a data set arising in the study of diabetes, and the results seem better than those of previous analyses. A magnetic resonance image (MRI) of the brain is also analyzed, and the methods appear successful in extracting the main features of anatomical interest. The methods described here have been implemented in both Fortran and S-PLUS versions, and the software is freely available through StatLib.},
  keywords = {clustering,functional-magnetic-resonance-imaging,model-based-clustering,non-gaussian-model},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/1993/Banfield_Raftery_1993_Model-Based Gaussian and Non-Gaussian Clustering.pdf}
}

@article{basMathematicalModellingGranola2011,
  title = {Mathematical Modelling of Granola Breakage during Pipe Pneumatic Conveying},
  author = {Ba{\c s}, Nur{\c s}in and Pathare, Pankaj B. and Catak, Muammer and Fitzpatrick, John J. and Cronin, Kevin and Byrne, Edmond P.},
  year = {2011},
  month = jan,
  journal = {Powder Technology},
  series = {9th {{International Symposium}} on {{Agglomeration}} and 4th {{International Granulation Workshop}}, 2009},
  volume = {206},
  number = {1},
  pages = {170--176},
  issn = {0032-5910},
  doi = {10.1016/j.powtec.2010.06.015},
  urldate = {2023-07-07},
  abstract = {Granola is a baked aggregated food product which serves as a breakfast cereal or snack consisting of oats, cereals, nuts and honey. Particle breakage of aggregated granola can occur during conveying as product is transferred as part of the production process on its way to packaging. Such breakage occurs as a result of particle{\textendash}particle and particle{\textendash}wall collisions with the conveying equipment. In this work, a population balance model is developed to describe the breakage of granola as it is conveyed through a pneumatic conveying pipeline rig. The model incorporates the influence of conveying pressure, exposure time and pipeline geometry, and is also related to parameters associated with aggregate formation such as granulator mixing speed and time. The aggregates were formed in a high shear granulator subject to impeller agitation of 300rpm for 9min and were then propelled through a pipeline with a 90{$^\circ$} bend at a number of different flow rates. Trials were carried out by applying compressed air at pressures of 200kPa, 300kPa and 400kPa while the aggregates were subjected to a number of recycles through the rig. Modelling of this breakage process was achieved by constructing the population balance equations (PBEs) in the form of a mass balance on the granola aggregates. The solutions to the PBEs were obtained by means of discretization through the application of the Markov chain method. When the size range of the system was divided into an appropriate number of states, the Markov chain method for the population balances exhibited a reasonable approximation for predicting the particle size distribution (PSD) over time particularly during the initial rig cycles.},
  langid = {english},
  keywords = {breakage-equation,markov-process,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Powder Technology/2011/Baş et al_2011_Mathematical modelling of granola breakage during pipe pneumatic conveying.pdf;/Users/steven/Zotero/storage/JZTXBPSM/S0032591010003141.html}
}

@article{benkoCommonFunctionalPrincipal2009,
  title = {Common Functional Principal Components},
  author = {Benko, Michal and H{\"a}rdle, Wolfgang and Kneip, Alois},
  year = {2009},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {37},
  number = {1},
  pages = {1--34},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/07-AOS516},
  urldate = {2023-09-01},
  abstract = {Functional principal component analysis (FPCA) based on the Karhunen{\textendash}Lo{\`e}ve decomposition has been successfully applied in many applications, mainly for one sample problems. In this paper we consider common functional principal components for two sample problems. Our research is motivated not only by the theoretical challenge of this data situation, but also by the actual question of dynamics of implied volatility (IV) functions. For different maturities the log-returns of IVs are samples of (smooth) random functions and the methods proposed here study the similarities of their stochastic behavior. First we present a new method for estimation of functional principal components from discrete noisy data. Next we present the two sample inference for FPCA and develop the two sample theory. We propose bootstrap tests for testing the equality of eigenvalues, eigenfunctions, and mean functions of two functional samples, illustrate the test-properties by simulation study and apply the method to the IV analysis.},
  keywords = {functional-principal-components,non-parametric-statistics,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2009/Benko et al_2009_Common functional principal components.pdf}
}

@article{berrenderoMahalanobisDistanceFunctional2020,
  title = {On {{Mahalanobis Distance}} in {{Functional Settings}}},
  author = {Berrendero, Jos{\'e} R. and {Bueno-Larraz}, Beatriz and Cuevas, Antonio},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {9},
  pages = {1--33},
  issn = {1533-7928},
  urldate = {2023-11-02},
  abstract = {Mahalanobis distance is a classical tool in multivariate analysis. We suggest here an extension of this concept to the case of functional data. More precisely, the proposed definition concerns those statistical problems where the sample data are real functions defined on a compact interval of the real line. The obvious difficulty for such a functional extension is the non-invertibility of the covariance operator in infinite-dimensional cases. Unlike other recent proposals, our definition is suggested and motivated in terms of the Reproducing Kernel Hilbert Space (RKHS) associated with the stochastic process that generates the data. The proposed distance is a true metric; it depends on a unique real smoothing parameter which is fully motivated in RKHS terms. Moreover, it shares some properties of its finite dimensional counterpart: it is invariant under isometries, it can be consistently estimated from the data and its sampling distribution is known under Gaussian models. An empirical study for two statistical applications, outliers detection and binary classification, is included. The results are quite competitive when compared to other recent proposals in the literature.},
  keywords = {functional-data-analysis,mahalanobis-distance,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Machine Learning Research/2020/Berrendero et al_2020_On Mahalanobis Distance in Functional Settings.pdf}
}

@article{berrenderoPrincipalComponentsMultivariate2011,
  title = {Principal Components for Multivariate Functional Data},
  author = {Berrendero, J. R. and Justel, A. and Svarc, M.},
  year = {2011},
  month = sep,
  journal = {Computational Statistics \& Data Analysis},
  volume = {55},
  number = {9},
  pages = {2619--2634},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2011.03.011},
  urldate = {2023-09-19},
  abstract = {A principal component method for multivariate functional data is proposed. Data can be arranged in a matrix whose elements are functions so that for each individual a vector of p functions is observed. This set of p curves is reduced to a small number of transformed functions, retaining as much information as possible. The criterion to measure the information loss is the integrated variance. Under mild regular conditions, it is proved that if the original functions are smooth this property is inherited by the principal components. A numerical procedure to obtain the smooth principal components is proposed and the goodness of the dimension reduction is assessed by two new measures of the proportion of explained variability. The method performs as expected in various controlled simulated data sets and provides interesting conclusions when it is applied to real data sets.},
  keywords = {dimension-reduction,functional-data-analysis,functional-principal-components,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2011/Berrendero et al_2011_Principal components for multivariate functional data.pdf;/Users/steven/Zotero/storage/QSD5A8K7/S0167947311001022.html}
}

@article{berthiauxApplicationTheoryMarkov2005,
  title = {Application of the Theory of {{Markov}} Chains to Model Different Processes in Particle Technology},
  author = {Berthiaux, Henri and Mizonov, Vadim and Zhukov, Vladimir},
  year = {2005},
  month = sep,
  journal = {Powder Technology},
  series = {4th {{French Meeting}} on {{Powder Science}} and {{Technology}}},
  volume = {157},
  number = {1},
  pages = {128--137},
  issn = {0032-5910},
  doi = {10.1016/j.powtec.2005.05.019},
  urldate = {2023-07-08},
  abstract = {The essence of almost all processes with participation of particulate solids is similar: it is a transformation of a particle property, or properties. This suggests that a unified basis for examining the changes of this property with (for example) time, may be derived. In this paper, the general strategy of building the Markov chain models and computational analysis of characteristics of a process is described, and some examples of application of the approach to model grinding, classification, grinding with internal classification, mixing, agglomeration, etc, are shown. The use of multidimensional models to consider simultaneously different properties is emphasised. The approach also allows taking into account non-linear phenomena, which are very typical of particulate processes but are very seldom considered in usual models.},
  langid = {english},
  keywords = {markov-process,particle-size-distribution,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Powder Technology/2005/Berthiaux et al. - 2005 - Application of the theory of Markov chains to mode.pdf;/Users/steven/Zotero/storage/2MFSKMNF/S0032591005002354.html}
}

@article{bessePrincipalComponentsAnalysis1986,
  title = {Principal Components Analysis of Sampled Functions},
  author = {Besse, Philippe and Ramsay, J. O.},
  year = {1986},
  month = jun,
  journal = {Psychometrika},
  volume = {51},
  number = {2},
  pages = {285--311},
  issn = {1860-0980},
  doi = {10.1007/BF02293986},
  urldate = {2023-07-08},
  abstract = {This paper describes a technique for principal components analysis of data consisting ofn functions each observed atp argument values. This problem arises particularly in the analysis of longitudinal data in which some behavior of a number of subjects is measured at a number of points in time. In such cases information about the behavior of one or more derivatives of the function being sampled can often be very useful, as for example in the analysis of growth or learning curves. It is shown that the use of derivative information is equivalent to a change of metric for the row space in classical principal components analysis. The reproducing kernel for the Hilbert space of functions plays a central role, and defines the best interpolating functions, which are generalized spline functions. An example is offered of how sensitivity to derivative information can reveal interesting aspects of the data.},
  langid = {english},
  keywords = {functional-principal-components,hilbert-space-theory,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Psychometrika/1986/Besse and Ramsay - 1986 - Principal components analysis of sampled functions.pdf}
}

@misc{bhattacharjeeGeodesicMixedEffects2023,
  title = {Geodesic {{Mixed Effects Models}} for {{Repeatedly Observed}}/{{Longitudinal Random Objects}}},
  author = {Bhattacharjee, Satarupa and M{\"u}ller, Hans-Georg},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05726},
  eprint = {2307.05726},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05726},
  urldate = {2023-10-27},
  abstract = {Mixed effect modeling for longitudinal data is challenging when the observed data are random objects, which are complex data taking values in a general metric space without linear structure. In such settings the classical additive error model and distributional assumptions are unattainable. Due to the rapid advancement of technology, longitudinal data containing complex random objects, such as covariance matrices, data on Riemannian manifolds, and probability distributions are becoming more common. Addressing this challenge, we develop a mixed-effects regression for data in geodesic spaces, where the underlying mean response trajectories are geodesics in the metric space and the deviations of the observations from the model are quantified by perturbation maps or transports. A key finding is that the geodesic trajectories assumption for the case of random objects is a natural extension of the linearity assumption in the standard Euclidean scenario. Further, geodesics can be recovered from noisy observations by exploiting a connection between the geodesic path and the path obtained by global Fr{\textbackslash}'echet regression for random objects. The effect of baseline Euclidean covariates on the geodesic paths is modeled by another Fr{\textbackslash}'echet regression step. We study the asymptotic convergence of the proposed estimates and provide illustrations through simulations and real-data applications.},
  archiveprefix = {arxiv},
  keywords = {geodesic-distance,mixed-effect-model,object-data-analysis,optimal-transport},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Bhattacharjee_Müller_2023_Geodesic Mixed Effects Models for Repeatedly Observed-Longitudinal Random.pdf;/Users/steven/Zotero/storage/CK36XUYV/2307.html}
}

@book{blumFoundationsDataScience2020,
  title = {Foundations of {{Data Science}}},
  author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  year = {2020},
  month = jan,
  publisher = {{Cambridge University Press}},
  abstract = {This book provides an introduction to the mathematical and algorithmic foundations of data science, including machine learning, high-dimensional geometry, and analysis of large networks. Topics include the counterintuitive nature of data in high dimensions, important linear algebraic techniques such as singular value decomposition, the theory of random walks and Markov chains, the fundamentals of and important algorithms for machine learning, algorithms and analysis for clustering, probabilistic models for large networks, representation learning including topic modelling and non-negative matrix factorization, wavelets and compressed sensing. Important probabilistic techniques are developed including the law of large numbers, tail inequalities, analysis of random projections, generalization guarantees in machine learning, and moment methods for analysis of phase transitions in large random graphs. Additionally, important structural and complexity measures are discussed such as matrix norms and VC-dimension. This book is suitable for both undergraduate and graduate courses in the design and analysis of algorithms for data.},
  googlebooks = {koHCDwAAQBAJ},
  isbn = {978-1-108-48506-7},
  langid = {english},
  keywords = {algorithm-theory,data-science-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Cambridge University Press/2020/Blum et al_2020_Foundations of Data Science.pdf}
}

@misc{bonieceChangepointDetectionFunctional2023,
  title = {On Changepoint Detection in Functional Data Using Empirical Energy Distance},
  author = {Boniece, B. Cooper and Horv{\'a}th, Lajos and Trapani, Lorenzo},
  year = {2023},
  month = oct,
  number = {arXiv:2310.04853},
  eprint = {2310.04853},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.04853},
  urldate = {2023-10-27},
  abstract = {We propose a novel family of test statistics to detect the presence of changepoints in a sequence of dependent, possibly multivariate, functional-valued observations. Our approach allows to test for a very general class of changepoints, including the "classical" case of changes in the mean, and even changes in the whole distribution. Our statistics are based on a generalisation of the empirical energy distance; we propose weighted functionals of the energy distance process, which are designed in order to enhance the ability to detect breaks occurring at sample endpoints. The limiting distribution of the maximally selected version of our statistics requires only the computation of the eigenvalues of the covariance function, thus being readily implementable in the most commonly employed packages, e.g. R. We show that, under the alternative, our statistics are able to detect changepoints occurring even very close to the beginning/end of the sample. In the presence of multiple changepoints, we propose a binary segmentation algorithm to estimate the number of breaks and the locations thereof. Simulations show that our procedures work very well in finite samples. We complement our theory with applications to financial and temperature data.},
  archiveprefix = {arxiv},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Boniece et al_2023_On changepoint detection in functional data using empirical energy distance.pdf;/Users/steven/Zotero/storage/LTGZUCIE/2310.html}
}

@misc{bouveyronFunFEMClusteringDiscriminative2021a,
  title = {{{funFEM}}: {{Clustering}} in the {{Discriminative Functional Subspace}}},
  shorttitle = {{{funFEM}}},
  author = {Bouveyron, Charles},
  year = {2021},
  month = oct,
  urldate = {2024-01-10},
  abstract = {The funFEM algorithm (Bouveyron et al., 2014) allows to cluster functional data by modeling the curves within a common and discriminative functional subspace.},
  copyright = {GPL-2},
  keywords = {Cluster,FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2021/Bouveyron_2021_funFEM.pdf}
}

@misc{bouveyronFunLBMModelBasedCoClustering2022,
  title = {{{funLBM}}: {{Model-Based Co-Clustering}} of {{Functional Data}}},
  shorttitle = {{{funLBM}}},
  author = {Bouveyron, Charles and Schmutz, Julien Jacques {and} Amandine},
  year = {2022},
  month = apr,
  urldate = {2024-01-10},
  abstract = {The funLBM algorithm allows to simultaneously cluster the rows and the columns of a data matrix where each entry of the matrix is a function or a time series.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2022/Bouveyron_Schmutz_2022_funLBM.pdf}
}

@book{brezisFunctionalAnalysisSobolev2011,
  title = {Functional {{Analysis}}, {{Sobolev Spaces}} and {{Partial Differential Equations}}},
  author = {Brezis, Haim},
  year = {2011},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-70914-7},
  urldate = {2023-10-29},
  isbn = {978-0-387-70913-0 978-0-387-70914-7},
  langid = {english},
  keywords = {differential-equation,functional-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2011/Brezis_2011_Functional Analysis, Sobolev Spaces and Partial Differential Equations2.pdf}
}

@book{brillingerTimeSeriesData2001,
  title = {Time {{Series}}: {{Data Analysis}} and {{Theory}}},
  shorttitle = {Time {{Series}}},
  author = {Brillinger, David R.},
  year = {2001},
  month = sep,
  publisher = {{Society for Industrial and Applied Mathematics}},
  abstract = {Intended for students and researchers, this text employs basic techniques of univariate and multivariate statistics for the analysis of time series and signals. It provides a broad collection of theorems, placing the techniques on firm theoretical ground. The techniques, which are illustrated by data analyses, are discussed in both a heuristic and a formal manner, making the book useful for both the applied and the theoretical worker. An extensive set of original exercises is included. Time Series: Data Analysis and Theory takes the Fourier transform of a stretch of time series data as the basic quantity to work with and shows the power of that approach. It considers second- and higher-order parameters and estimates them equally, thereby handling non-Gaussian series and nonlinear systems directly. The included proofs, which are generally short, are based on cumulants.},
  googlebooks = {PX5HExMKER0C},
  isbn = {978-0-89871-501-9},
  langid = {english},
  keywords = {time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Society for Industrial and Applied Mathematics/2001/Brillinger_2001_Time Series.pdf}
}

@article{brockhausBoostingFunctionalRegression2020,
  title = {Boosting {{Functional Regression Models}} with {{FDboost}}},
  author = {Brockhaus, Sarah and R{\"u}gamer, David and Greven, Sonja},
  year = {2020},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {94},
  pages = {1--50},
  issn = {1548-7660},
  doi = {10.18637/jss.v094.i10},
  urldate = {2024-01-10},
  abstract = {The R add-on package FDboost is a flexible toolbox for the estimation of functional regression models by model-based boosting. It provides the possibility to fit regression models for scalar and functional response with effects of scalar as well as functional covariates, i.e., scalar-on-function, function-on-scalar and function-on-function regression models. In addition to mean regression, quantile regression models as well as generalized additive models for location scale and shape can be fitted with FDboost. Furthermore, boosting can be used in high-dimensional data settings with more covariates than observations. We provide a hands-on tutorial on model fitting and tuning, including the visualization of results. The methods for scalar-on-function regression are illustrated with spectrometric data of fossil fuels and those for functional response regression with a data set including bioelectrical signals for emotional episodes.},
  copyright = {Copyright (c) 2020 Sarah Brockhaus, David R{\"u}gamer, Sonja Greven},
  langid = {english},
  keywords = {function-on-function regression,function-on-scalar regression,functional data analysis,gradient boosting,model-based boosting,scalar-on-function regression},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Statistical Software/2020/Brockhaus et al_2020_Boosting Functional Regression Models with FDboost.pdf}
}

@techreport{caiNonparametricCovarianceFunction,
  title = {Nonparametric {{Covariance Function Estimation}} for {{Functional}} and {{Longitudinal Data}}},
  author = {Cai, T Tony and Yuan, Ming},
  abstract = {Covariance function plays a critical role in functional and longitudinal data analysis. In this paper, we consider nonparametric covariance function estimation using a reproducing kernel Hilbert space framework. A regularization method is introduced through a careful characterization of the function space in which a covariance function resides. It is shown that the procedure enjoys desirable theoretical and numerical properties. In particular, even though the covariance function is bivariate, the rates of convergence attained by the regularization method are very similar to those typically achieved for estimating univariate functions. Our results generalize and improve some of the known results in the literature both for estimating the covariance function and for estimating the functional principal components. The procedure is easy to implement and its numerical performance is investigated using both simulated and real data. In particular our method is illustrated in an analysis of a longitudinal CD4 count data from an HIV study.},
  langid = {english},
  keywords = {covariance-operator-estimation,functional-data-analysis,reproducing-kernel-hilbert-space,smoothing-splines},
  file = {/Users/steven/Zotero/storage/X49M7L7P/Cai and Yuan - Nonparametric Covariance Function Estimation for F.pdf}
}

@article{caiOptimalEstimationMean2011,
  title = {Optimal Estimation of the Mean Function Based on Discretely Sampled Functional Data: {{Phase}} Transition},
  shorttitle = {Optimal Estimation of the Mean Function Based on Discretely Sampled Functional Data},
  author = {Cai, T. Tony and Yuan, Ming},
  year = {2011},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {39},
  number = {5},
  pages = {2330--2355},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/11-AOS898},
  urldate = {2023-08-24},
  abstract = {The problem of estimating the mean of random functions based on discretely sampled data arises naturally in functional data analysis. In this paper, we study optimal estimation of the mean function under both common and independent designs. Minimax rates of convergence are established and easily implementable rate-optimal estimators are introduced. The analysis reveals interesting and different phase transition phenomena in the two cases. Under the common design, the sampling frequency solely determines the optimal rate of convergence when it is relatively small and the sampling frequency has no effect on the optimal rate when it is large. On the other hand, under the independent design, the optimal rate of convergence is determined jointly by the sampling frequency and the number of curves when the sampling frequency is relatively small. When it is large, the sampling frequency has no effect on the optimal rate. Another interesting contrast between the two settings is that smoothing is necessary under the independent design, while, somewhat surprisingly, it is not essential under the common design.},
  keywords = {functional-data-analysis,mean-function-estimation,minimax-theory,reproducing-kernel-hilbert-space,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2011/Cai_Yuan_2011_Optimal estimation of the mean function based on discretely sampled functional2.pdf}
}

@article{camelettiSpatiotemporalModelingParticulate2013,
  title = {Spatio-Temporal Modeling of Particulate Matter Concentration through the {{SPDE}} Approach},
  author = {Cameletti, Michela and Lindgren, Finn and Simpson, Daniel and Rue, H{\aa}vard},
  year = {2013},
  month = apr,
  journal = {AStA Advances in Statistical Analysis},
  volume = {97},
  number = {2},
  pages = {109--131},
  issn = {1863-818X},
  doi = {10.1007/s10182-012-0196-3},
  urldate = {2023-10-24},
  abstract = {In this work, we consider a hierarchical spatio-temporal model for particulate matter (PM) concentration in the North-Italian region Piemonte. The model involves a Gaussian Field (GF), affected by a measurement error, and a state process characterized by a first order autoregressive dynamic model and spatially correlated innovations. This kind of model is well discussed and widely used in the air quality literature thanks to its flexibility in modelling the effect of relevant covariates (i.e. meteorological and geographical variables) as well as time and space dependence. However, Bayesian inference{\textemdash}through Markov chain Monte Carlo (MCMC) techniques{\textemdash}can be a challenge due to convergence problems and heavy computational loads. In particular, the computational issue refers to the infeasibility of linear algebra operations involving the big dense covariance matrices which occur when large spatio-temporal datasets are present. The main goal of this work is to present an effective estimating and spatial prediction strategy for the considered spatio-temporal model. This proposal consists in representing a GF with Mat{\'e}rn covariance function as a Gaussian Markov Random Field (GMRF) through the Stochastic Partial Differential Equations (SPDE) approach. The main advantage of moving from a GF to a GMRF stems from the good computational properties that the latter enjoys. In fact, GMRFs are defined by sparse matrices that allow for computationally effective numerical methods. Moreover, when dealing with Bayesian inference for GMRFs, it is possible to adopt the Integrated Nested Laplace Approximation (INLA) algorithm as an alternative to MCMC methods giving rise to additional computational advantages. The implementation of the SPDE approach through the R-library INLA (www.r-inla.org) is illustrated with reference to the Piemonte PM data. In particular, providing the step-by-step R-code, we show how it is easy to get prediction and probability of exceedance maps in a reasonable computing time.},
  langid = {english},
  keywords = {ecology,random-fields,stochastic-differential-equation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/AStA Advances in Statistical Analysis/2013/Cameletti et al_2013_Spatio-temporal modeling of particulate matter concentration through the SPDE.pdf}
}

@misc{carrenoGAAUAMScikitfdaVersion2023,
  title = {{{GAA-UAM}}/Scikit-Fda: {{Version}} 0.9},
  shorttitle = {{{GAA-UAM}}/Scikit-Fda},
  author = {Carre{\~n}o, Carlos Ramos and {hzzhyj} and {mellamansanchez} and Marcos, Pablo and {pedrorponga} and del Val, David and Pablo and Fern{\'a}ndez, David Garc{\'i}a and Mart{\'i}n and Berrocal, Miguel Carbajo and ElenaPetrunina and Sierra, Pablo Cuesta and Hidalgo, Rafa and Lejeune, Cl{\'e}ment and {amandaher} and {dSerna4} and {ego-thales} and {pedrog99} and Duque, Jorge and Parida, Saumyaranjan and {lena123315} and {jltorrecilla} and Tucker, Derek and E105D104U125 and Grimonprez, Quentin and Johnsen, Sean and {eliegoudout} and Castillo, {\'A}lvaro},
  year = {2023},
  month = oct,
  doi = {10.5281/zenodo.10016930},
  urldate = {2024-01-10},
  abstract = {What's Changed Document conda installation. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/511 Deprecate 3 as the default number of components in FPCA by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/510 Add an example showing how to create new bases for scikit-fda. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/517 New example for creating interpolation/extrapolation methods. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/518 FPCA Regression by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/466 Add CSV loading example to the tutorial. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/520 Update THANKS.txt by @jltorrecilla in https://github.com/GAA-UAM/scikit-fda/pull/523 Fix inner product integrate by @m5signorini in https://github.com/GAA-UAM/scikit-fda/pull/522 Simplification example make\_gaussian\_process by @jltorrecilla in https://github.com/GAA-UAM/scikit-fda/pull/526 Fix code coverage. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/532 Add missing data imputation via interpolation by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/533 Make CustomBasis documentation accesible by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/534 SRSF formula doc fix by @ego-thales in https://github.com/GAA-UAM/scikit-fda/pull/542 Speed up the calculation of the penalization matrix for FDataGrid by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/519 Change type of axis in FData's mean method to accept zero. by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/546 Covariance function as tensor product by @m5signorini in https://github.com/GAA-UAM/scikit-fda/pull/505 Polish fpca regression example by @Ddelval in https://github.com/GAA-UAM/scikit-fda/pull/552 Fix IndexError in FPCAPlot by @Quentin62 in https://github.com/GAA-UAM/scikit-fda/pull/554 ddof parameter in fdatacov by @pcuestas in https://github.com/GAA-UAM/scikit-fda/pull/556 Improve list of contributors by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/557 FDataGrid.restrict option with\_bounds by @ego-thales in https://github.com/GAA-UAM/scikit-fda/pull/561 Deleted forgotten \_\_doc\_\_ bits by @eliegoudout in https://github.com/GAA-UAM/scikit-fda/pull/565 Add a version switcher for the docs in the web. by @vnmabus in https://github.com/GAA-UAM/scikit-fda/pull/571 Update citation and See also in DTMClassifier class by @E105D104U125 in https://github.com/GAA-UAM/scikit-fda/pull/573 Change ddof argument to correction in var and cov functions by @pcuestas in https://github.com/GAA-UAM/scikit-fda/pull/578 BSplineBasis initialization incorrect error message by @pcuestas in https://github.com/GAA-UAM/scikit-fda/pull/577 Smoothing in several dimensions by @ElenaPetrunina in https://github.com/GAA-UAM/scikit-fda/pull/436 Set correction=0 by default in cov and var functions by @pcuestas in https://github.com/GAA-UAM/scikit-fda/pull/582 New Contributors @jltorrecilla made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/523 @ego-thales made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/542 @Quentin62 made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/554 @pcuestas made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/556 @allcontributors made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/564 @eliegoudout made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/565 @E105D104U125 made their first contribution in https://github.com/GAA-UAM/scikit-fda/pull/573 Full Changelog: https://github.com/GAA-UAM/scikit-fda/compare/0.8.1...0.9},
  howpublished = {Zenodo},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Zenodo/2023/Carreño et al_2023_GAA-UAM-scikit-fda.pdf;/Users/steven/Zotero/storage/PSFHH9SN/10016930.html}
}

@article{catakDiscreteSolutionBreakage2010,
  title = {Discrete {{Solution}} of the {{Breakage Equation Using Markov Chains}}},
  author = {Catak, Muammer and Bas, Nursin and Cronin, Kevin and Fitzpatrick, John J. and Byrne, Edmond P.},
  year = {2010},
  month = sep,
  journal = {Industrial \& Engineering Chemistry Research},
  volume = {49},
  number = {17},
  pages = {8248--8257},
  publisher = {{American Chemical Society}},
  issn = {0888-5885},
  doi = {10.1021/ie100216g},
  urldate = {2023-07-08},
  abstract = {Analytical solution of population balance equations (PBEs) may be impossible except for some simple cases. In the literature there are a number of methods to solve PBEs including discrete methods, Monte Carlo simulation, and method of moments. In this paper, the Markov chain is presented as a discrete solution for a population balance equation of a breakage process for determining the particle size distribution (PSD) over time. The transition matrix P, which is the key operator of a Markov chain, is built using breakage equations. Thereafter, from calculating transition matrix, P, the particle size distribution of the system is easily evaluated using the Markov chain. According to simulation results, if the size range of the system is divided into a sufficient number of states and an appropriate transition time step was chosen, then results from the Markov chain are in agreement with the analytical solution of PBEs governed by the same breakage functions. In addition to theoretical illustration, the Markov theory was employed to model the breakage process of aggregated food products passing through a pneumatic conveying pipeline rig.},
  keywords = {breakage-equation,markov-process,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Industrial & Engineering Chemistry Research/2010/Catak et al. - 2010 - Discrete Solution of the Breakage Equation Using M.pdf}
}

@article{ceroveckiCLTDiscreteFourier2017,
  title = {On the {{CLT}} for Discrete {{Fourier}} Transforms of Functional Time Series},
  author = {Cerovecki, Cl{\'e}ment and H{\"o}rmann, Siegfried},
  year = {2017},
  month = feb,
  journal = {Journal of Multivariate Analysis},
  volume = {154},
  pages = {282--295},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2016.11.006},
  urldate = {2023-09-19},
  abstract = {The purpose of this paper is to derive sharp conditions for the asymptotic normality of a discrete Fourier transform of a functional time series (Xt:t{$\geq$}1) defined, for all {\texttheta}{$\in$}(-{$\pi$},{$\pi$}], by Sn({\texttheta})=Xte-i{\texttheta}+{$\cdots$}+Xte-in{\texttheta}. Assuming that the function space is a Hilbert space we prove that a Central Limit Theorem (CLT) holds for almost all frequencies {\texttheta} if the process (Xt) is stationary, ergodic and purely non-deterministic. Under slightly stronger assumptions we formulate versions which provide a CLT for fixed frequencies as well as for Sn({\texttheta}n), when {\texttheta}n{$\rightarrow\theta$}0 is a sequence of fundamental frequencies. In particular we also deduce the regular CLT ({\texttheta}=0) under new and very mild assumptions. We show that our results apply to the most commonly studied functional time series.},
  keywords = {central-limit-theorem,functional-time-series,spectral-analysis,stationarity-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Multivariate Analysis/2017/Cerovecki_Hörmann_2017_On the CLT for discrete Fourier transforms of functional time series.pdf;/Users/steven/Zotero/storage/QEVKVVAI/S0047259X16301592.html}
}

@misc{chakravortyDistributedEstimationParallel2023,
  title = {Distributed Estimation through Parallel Approximants},
  author = {Chakravorty, Aritra and Cleveland, William S. and Wolfe, Patrick J.},
  year = {2023},
  month = aug,
  number = {arXiv:2112.15572},
  eprint = {2112.15572},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.15572},
  urldate = {2023-10-24},
  abstract = {Designing scalable estimation algorithms is a core challenge in modern statistics. Here we introduce a framework to address this challenge based on parallel approximants, which yields estimators with provable properties that operate on the entirety of very large, distributed data sets. We first formalize the class of statistics which admit straightforward calculation in distributed environments through independent parallelization. We then show how to use such statistics to approximate arbitrary functional operators in appropriate spaces, yielding a general estimation framework that does not require data to reside entirely in memory. We characterize the \$L\^2\$ approximation properties of our approach and provide fully implemented examples of sample quantile calculation and local polynomial regression in a distributed computing environment. A variety of avenues and extensions remain open for future work.},
  archiveprefix = {arxiv},
  keywords = {distributed-learning,non-parametric-statistics,parallel-algorithm},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Chakravorty et al_2023_Distributed estimation through parallel approximants.pdf;/Users/steven/Zotero/storage/8J8KGFAS/2112.html}
}

@book{chenHandbookDataVisualization2008,
  title = {Handbook of {{Data Visualization}}},
  author = {Chen, Chun-houh and H{\"a}rdle, Wolfgang and Unwin, Antony},
  year = {2008},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-33037-0},
  urldate = {2023-09-13},
  isbn = {978-3-540-33036-3 978-3-540-33037-0},
  langid = {english},
  keywords = {data-visualisation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2008/Chen et al_2008_Handbook of Data Visualization.pdf}
}

@article{chenMultidimensionalFunctionalPrincipal2017,
  title = {Multi-Dimensional Functional Principal Component Analysis},
  author = {Chen, Lu-Hung and Jiang, Ci-Ren},
  year = {2017},
  month = sep,
  journal = {Statistics and Computing},
  volume = {27},
  number = {5},
  pages = {1181--1192},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9679-5},
  urldate = {2023-10-24},
  abstract = {Functional principal component analysis is one of the most commonly employed approaches in functional and longitudinal data analysis and we extend it to analyze functional/longitudinal data observed on a general d-dimensional domain. The computational issues emerging in the extension are fully addressed with our proposed solutions. The local linear smoothing technique is employed to perform estimation because of its capabilities of performing large-scale smoothing and of handling data with different sampling schemes (possibly on irregular domain) in addition to its nice theoretical properties. Besides taking the fast Fourier transform strategy in smoothing, the modern GPGPU (general-purpose computing on graphics processing units) architecture is applied to perform parallel computation to save computation time. To resolve the out-of-memory issue due to large-scale data, the random projection procedure is applied in the eigendecomposition step. We show that the proposed estimators can achieve the classical nonparametric rates for longitudinal data and the optimal convergence rates for functional data if the number of observations per sample is of the order \$\$(n/ {\textbackslash}log n)\^\{d/4\}\$\$. Finally, the performance of our approach is demonstrated with simulation studies and the fine particulate matter (PM 2.5) data measured in Taiwan.},
  langid = {english},
  keywords = {ecology,functional-data-analysis,gpu-computation,local-polynomial-smoothing,spectral-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistics and Computing/2017/Chen_Jiang_2017_Multi-dimensional functional principal component analysis.pdf}
}

@article{chiouMultivariateFunctionalPrincipal2014,
  title = {Multivariate {{Functional Principal Component Analysis}}: {{A Normalization Approach}}},
  shorttitle = {Multivariate {{Functional Principal Component Analysis}}},
  author = {Chiou, Jeng-Min and Chen, Yu-Ting and Yang, Ya-Fang},
  year = {2014},
  journal = {Statistica Sinica},
  volume = {24},
  number = {4},
  eprint = {24310959},
  eprinttype = {jstor},
  pages = {1571--1596},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  urldate = {2023-07-08},
  abstract = {We propose an extended version of the classical Karhunen-Lo{\`e}ve expansion of a multivariate random process, termed a normalized multivariate functional principal component (mFPCn) representation. This takes variations between the components of the process into account and takes advantage of component dependencies through the pairwise cross-covariance functions. This approach leads to a single set of multivariate functional principal component scores, which serve well as a proxy for multivariate functional data. We derive the consistency properties for the estimates of the mFPCn, and the asymptotic distributions for statistical inferences. We illustrate the finite sample performance of this approach through the analysis of a traffic flow data set, including an application to clustering and a simulation study. The mFPCn approach serves as a basic and useful statistical tool for multivariate functional data analysis.},
  keywords = {functional-principal-components,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistica Sinica/2014/Chiou et al_2014_Multivariate Functional Principal Component Analysis.pdf}
}

@misc{christenDynamicSurvivalAnalysis2023,
  title = {Dynamic Survival Analysis: Modelling the Hazard Function via Ordinary Differential Equations},
  shorttitle = {Dynamic Survival Analysis},
  author = {Christen, J. A. and Rubio, F. J.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05205},
  eprint = {2308.05205},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.05205},
  urldate = {2023-10-27},
  abstract = {The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework.},
  archiveprefix = {arxiv},
  keywords = {differential-equation,survival-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Christen_Rubio_2023_Dynamic survival analysis.pdf;/Users/steven/Zotero/storage/B477JNIM/2308.html}
}

@article{clevelandRobustLocallyWeighted1979,
  title = {Robust {{Locally Weighted Regression}} and {{Smoothing Scatterplots}}},
  author = {Cleveland, William S.},
  year = {1979},
  journal = {Journal of the American Statistical Association},
  volume = {74},
  number = {368},
  eprint = {2286407},
  eprinttype = {jstor},
  pages = {829--836},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2286407},
  urldate = {2023-07-08},
  abstract = {The visual information on a scatterplot can be greatly enhanced, with little additional cost, by computing and plotting smoothed points. Robust locally weighted regression is a method for smoothing a scatterplot, (x\textsubscript{i}, y\textsubscript{i}), i = 1, {$\cdots$}, n, in which the fitted value at x\textsubscript{k} is the value of a polynomial fit to the data using weighted least squares, where the weight for (x\textsubscript{i}, y\textsubscript{i}) is large if x\textsubscript{i} is close to x\textsubscript{k} and small if it is not. A robust fitting procedure is used that guards against deviant points distorting the smoothed points. Visual, computational, and statistical issues of robust locally weighted regression are discussed. Several examples, including data on lead intoxication, are used to illustrate the methodology.},
  keywords = {kernel-smoothing,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/1979/Cleveland_1979_Robust Locally Weighted Regression and Smoothing Scatterplots.pdf}
}

@article{coffeyCommonFunctionalPrincipal2011,
  title = {Common Functional Principal Components Analysis: {{A}} New Approach to Analyzing Human Movement Data},
  shorttitle = {Common Functional Principal Components Analysis},
  author = {Coffey, N. and Harrison, A. J. and Donoghue, O. A. and Hayes, K.},
  year = {2011},
  month = dec,
  journal = {Human Movement Science},
  volume = {30},
  number = {6},
  pages = {1144--1166},
  issn = {0167-9457},
  doi = {10.1016/j.humov.2010.11.005},
  urldate = {2023-09-19},
  abstract = {In many human movement studies angle-time series data on several groups of individuals are measured. Current methods to compare groups include comparisons of the mean value in each group or use multivariate techniques such as principal components analysis and perform tests on the principal component scores. Such methods have been useful, though discard a large amount of information. Functional data analysis (FDA) is an emerging statistical analysis technique in human movement research which treats the angle-time series data as a function rather than a series of discrete measurements. This approach retains all of the information in the data. Functional principal components analysis (FPCA) is an extension of multivariate principal components analysis which examines the variability of a sample of curves and has been used to examine differences in movement patterns of several groups of individuals. Currently the functional principal components (FPCs) for each group are either determined separately (yielding components that are group-specific), or by combining the data for all groups and determining the FPCs of the combined data (yielding components that summarize the entire data set). The group-specific FPCs contain both within and between group variation and issues arise when comparing FPCs across groups when the order of the FPCs alter in each group. The FPCs of the combined data may not adequately describe all groups of individuals and comparisons between groups typically use t-tests of the mean FPC scores in each group. When these differences are statistically non-significant it can be difficult to determine how a particular intervention is affecting movement patterns or how injured subjects differ from controls. In this paper we aim to perform FPCA in a manner allowing sensible comparisons between groups of curves. A statistical technique called common functional principal components analysis (CFPCA) is implemented. CFPCA identifies the common sources of variation evident across groups but allows the order of each component to change for a particular group. This allows for the direct comparison of components across groups. We use our method to analyze a biomechanical data set examining the mechanisms of chronic Achilles tendon injury and the functional effects of orthoses.},
  keywords = {biomechanics,functional-data-analysis,functional-principal-components,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Human Movement Science/2011/Coffey et al_2011_Common functional principal components analysis.pdf;/Users/steven/Zotero/storage/BJRZS454/S0167945710001867.html}
}

@article{colombiHiddenMarkovModels2023,
  title = {Hidden {{Markov}} Models for Longitudinal Rating Data with Dynamic Response Styles},
  author = {Colombi, Roberto and Giordano, Sabrina and Kateri, Maria},
  year = {2023},
  month = sep,
  journal = {Statistical Methods \& Applications},
  issn = {1613-981X},
  doi = {10.1007/s10260-023-00717-x},
  urldate = {2023-10-24},
  abstract = {This work deals with the analysis of longitudinal ordinal responses. The novelty of the proposed approach is in modeling simultaneously the temporal dynamics of a latent trait of interest, measured via the observed ordinal responses, and the answering behaviors influenced by response styles, through hidden Markov models (HMMs) with two latent components. This approach enables the modeling of (i) the substantive latent trait, controlling for response styles; (ii) the change over time of latent trait and answering behavior, allowing also dependence on individual characteristics. For the proposed HMMs, estimation procedures, methods for standard errors calculation, measures of goodness of fit and classification, and full-conditional residuals are discussed. The proposed model is fitted to ordinal longitudinal data from the Survey on Household Income and Wealth (Bank of Italy) to give insights on the evolution of households financial capability.},
  langid = {english},
  keywords = {longitudinal-data,markov-process,ordinal-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistical Methods & Applications/2023/Colombi et al_2023_Hidden Markov models for longitudinal rating data with dynamic response styles.pdf}
}

@article{cuiFastMultilevelFunctional2023a,
  title = {Fast {{Multilevel Functional Principal Component Analysis}}},
  author = {Cui, Erjia and Li, Ruonan and Crainiceanu, Ciprian M. and Xiao, Luo},
  year = {2023},
  journal = {Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
  volume = {32},
  number = {2},
  pages = {366--377},
  issn = {1061-8600},
  doi = {10.1080/10618600.2022.2115500},
  urldate = {2024-01-28},
  abstract = {We introduce fast multilevel functional principal component analysis (fast MFPCA), which scales up to high dimensional functional data measured at multiple visits. The new approach is orders of magnitude faster than and achieves comparable estimation accuracy with the original MFPCA (). Methods are motivated by the National Health and Nutritional Examination Survey (NHANES), which contains minute-level physical activity information of more than 10000 participants over multiple days and 1440 observations per day. While MFPCA takes more than five days to analyze these data, fast MFPCA takes less than five minutes. A theoretical study of the proposed method is also provided. The associated function mfpca.face() is available in the R package refund.},
  pmcid = {PMC10260118},
  pmid = {37313008},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of computational and graphical statistics  a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America/2023/Cui et al_2023_Fast Multilevel Functional Principal Component Analysis.pdf}
}

@article{daiDerivativePrincipalComponent2018,
  title = {Derivative {{Principal Component Analysis}} for {{Representing}} the {{Time Dynamics}} of {{Longitudinal}} and {{Functional Data}}},
  author = {Dai, Xiongtao and M{\"u}ller, Hans-Georg and Tao, Wenwen},
  year = {2018},
  journal = {Statistica Sinica},
  volume = {28},
  number = {3},
  eprint = {26492959},
  eprinttype = {jstor},
  pages = {1583--1609},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  urldate = {2023-11-03},
  abstract = {We propose a nonparametric method to explicitly model and represent the derivatives of smooth underlying trajectories for longitudinal data. This representation is based on a direct Karhunen-Lo{\`e}ve expansion of the unobserved derivatives and leads to the notion of derivative principal component analysis, which complements functional principal component analysis, one of the most popular tools of functional data analysis. The proposed derivative principal component scores can be obtained for irregularly spaced and sparsely observed longitudinal data, as typically encountered in biomedical studies, as well as for functional data which are densely measured. Novel consistency results and asymptotic convergence rates for the proposed estimates of the derivative principal component scores and other components of the model are derived under a unified scheme for sparse or dense observations and mild conditions. We compare the proposed representations for derivatives with alternative approaches in simulation settings and also in a wallaby growth curve application. It emerges that representations using the proposed derivative principal component analysis recover the underlying derivatives more accurately compared to principal component analysis-based approaches especially in settings where the functional data are represented with only a very small number of components or are densely sampled. In a second wheat spectra classification example, derivative principal component scores were found to be more predictive for the protein content of wheat than the conventional functional principal component scores.},
  keywords = {derivatives,functional-data-analysis,functional-principal-components},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistica Sinica/2018/Dai et al_2018_Derivative Principal Component Analysis for Representing the Time Dynamics of.pdf}
}

@phdthesis{debalyModelesSeriesTemporelles2022,
  type = {These de Doctorat},
  title = {Mod{\`e}les de S{\'e}ries Temporelles Multivari{\'e}es Non-Lin{\'e}aires Avec R{\'e}gresseurs Exog{\`e}nes},
  author = {Debaly, Zinsou Max},
  year = {2022},
  month = oct,
  urldate = {2023-10-24},
  abstract = {Dans cette th{\`e}se, on s'int{\'e}resse aux propri{\'e}t{\'e}s probabilistes et statistiques de mod{\`e}les de s{\'e}ries temporelles non-lin{\'e}aires qui prennent en compte des covariables exog{\`e}nes. Les s{\'e}ries temporelles de comptage ou cat{\'e}gorielles sont en particulier consid{\'e}r{\'e}es ainsi que la mod{\'e}lisation de donn{\'e}es mixtes en multivari{\'e}. Des propri{\'e}t{\'e}s de stationnarit{\'e} sont {\'e}tablies pour ces mod{\`e}les {\`a} partir de techniques d'it{\'e}rations d'application al{\'e}atoires d{\'e}pendantes. Dans le cas multivari{\'e}, des approches par pseudo-vraisemblance et/ou utilisation de copules sont utilis{\'e}es pour l'inf{\'e}rence statistique. Enfin,  une application de certains de ces m{\'e}thodes dans le cadre de l'{\'e}cologie est pr{\'e}sent{\'e}e.},
  collaborator = {Truquet, Lionel},
  copyright = {Licence Etalab},
  school = {Rennes, {\'E}cole Nationale de la Statistique et de l'Analyse de l'Information},
  keywords = {ecology,multivariate-time-series,time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Rennes, École Nationale de la Statistique et de l'Analyse de l'Information/2022/Debaly_2022_Modèles de séries temporelles multivariées non-linéaires avec régresseurs.pdf}
}

@article{delceyPhysicsinformedNeuralNetworks2023,
  title = {Physics-Informed Neural Networks for Gravity Currents Reconstruction from Limited Data},
  author = {Delcey, Micka{\"e}l and Cheny, Yoann and {Kiesgen de Richter}, S{\'e}bastien},
  year = {2023},
  month = feb,
  journal = {Physics of Fluids},
  volume = {35},
  number = {2},
  pages = {027124},
  issn = {1070-6631},
  doi = {10.1063/5.0136886},
  urldate = {2023-10-24},
  abstract = {The present work investigates the use of physics-informed neural networks (PINNs) for the three-dimensional (3D) reconstruction of unsteady gravity currents from limited data. In the PINN context, the flow fields are reconstructed by training a neural network whose objective function penalizes the mismatch between the network predictions and the observed data and embeds the underlying equations using automatic differentiation. This study relies on a high-fidelity numerical experiment of the canonical lock-exchange configuration. This allows us to benchmark quantitatively the PINNs reconstruction capabilities on several training databases that mimic state-of-the-art experimental measurement techniques for density and velocity. Notably, spatially averaged density measurements by the light attenuation technique (LAT) are employed for the training procedure. We propose an experimental setup that combines density measurement by LAT and two independent planar velocity measurements by particle image velocimetry (PIV). The so-called LAT-2PIV setup gives the most promising results for flow reconstruction by PINNs, with respect to its accuracy and cost efficiency.},
  keywords = {image-analysis,neural-network,physics-informed-model},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Physics of Fluids/2023/Delcey et al_2023_Physics-informed neural networks for gravity currents reconstruction from.pdf;/Users/steven/Zotero/storage/E5NNJFP9/Physics-informed-neural-networks-for-gravity.html}
}

@misc{deplaenDualFormulationProbabilistic2023,
  title = {A {{Dual Formulation}} for {{Probabilistic Principal Component Analysis}}},
  author = {De Plaen, Henri and Suykens, Johan A. K.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.10078},
  eprint = {2307.10078},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.10078},
  urldate = {2023-10-27},
  abstract = {In this paper, we characterize Probabilistic Principal Component Analysis in Hilbert spaces and demonstrate how the optimal solution admits a representation in dual space. This allows us to develop a generative framework for kernel methods. Furthermore, we show how it englobes Kernel Principal Component Analysis and illustrate its working on a toy and a real dataset.},
  archiveprefix = {arxiv},
  keywords = {probabilist-principal-components},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/De Plaen_Suykens_2023_A Dual Formulation for Probabilistic Principal Component Analysis.pdf;/Users/steven/Zotero/storage/SNAW5FKD/2307.html}
}

@misc{deyGraphconstrainedAnalysisMultivariate2023,
  title = {Graph-Constrained {{Analysis}} for {{Multivariate Functional Data}}},
  author = {Dey, Debangan and Banerjee, Sudipto and Lindquist, Martin and Datta, Abhirup},
  year = {2023},
  month = aug,
  number = {arXiv:2209.06294},
  eprint = {2209.06294},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.06294},
  urldate = {2023-10-27},
  abstract = {Functional Gaussian graphical models (GGM) used for analyzing multivariate functional data customarily estimate an unknown graphical model representing the conditional relationships between the functional variables. However, in many applications of multivariate functional data, the graph is known and existing functional GGM methods cannot preserve a given graphical constraint. In this manuscript, we demonstrate how to conduct multivariate functional analysis that exactly conforms to a given inter-variable graph. We first show the equivalence between partially separable functional GGM and graphical Gaussian processes (GP), proposed originally for constructing optimal covariance functions for multivariate spatial data that retain the conditional independence relations in a given graphical model. The theoretical connection help design a new algorithm that leverages Dempster's covariance selection to calculate the maximum likelihood estimate of the covariance function for multivariate functional data under graphical constraints. We also show that the finite term truncation of functional GGM basis expansion used in practice is equivalent to a low-rank graphical GP, which is known to oversmooth marginal distributions. To remedy this, we extend our algorithm to better preserve marginal distributions while still respecting the graph and retaining computational scalability. The insights obtained from the new results presented in this manuscript will help practitioners better understand the relationship between these graphical models and in deciding on the appropriate method for their specific multivariate data analysis task. The benefits of the proposed algorithms are illustrated using empirical experiments and an application to functional modeling of neuroimaging data using the connectivity graph among regions of the brain.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,gaussian-process,graphical-model,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Dey et al_2023_Graph-constrained Analysis for Multivariate Functional Data.pdf;/Users/steven/Zotero/storage/VNGTEE5Q/2209.html}
}

@phdthesis{dietApprocheBoutBout2021,
  title = {Une Approche de Bout En Bout Du Tol{\'e}rancement Statistique Sous Contraintes Industrielles : Contribution Au Jumeau Virtuel Industriel},
  shorttitle = {Une Approche de Bout En Bout Du Tol{\'e}rancement Statistique Sous Contraintes Industrielles},
  author = {Diet, Ambre},
  year = {2021},
  month = apr,
  urldate = {2024-01-10},
  abstract = {Dans le processus de fabrication d'un produit, diverses {\'e}tapes d'assemblage sont n{\'e}cessaires. Plusieurs types d'exigences sont {\`a} respecter {\`a} chaque niveau et ils impliquent de consid{\'e}rer les incertitudes de dimensions sur les pi{\`e}ces {\`a} assembler. Le tol{\'e}rancement est l'activit{\'e} en charge de la gestion de ces incertitudes et intervient {\`a} la fois en phase de d{\'e}veloppement du produit et en phase s{\'e}rie. Dans le contexte de l'industrie a{\'e}ronautique, en particulier en ce qui concerne le tol{\'e}rancement sur les a{\'e}rostructures, des sp{\'e}cificit{\'e}s sont {\`a} prendre en compte pour l'{\'e}laboration de m{\'e}thodes et outils ad{\'e}quats. Avant la mise en production, une des probl{\'e}matiques principales du tol{\'e}rancement est l'allocation de limites de tol{\'e}rance adapt{\'e}es {\`a} un certain taux acceptable de rebut. Le but est de permettre aux acteurs concern{\'e}s par les intervalles de tol{\'e}rance de s'accorder sur une valeur de tol{\'e}rance coh{\'e}rente et robuste. Une m{\'e}thodologie statistique bas{\'e}e sur une approche type borne de Chernov appliqu{\'e}e {\`a} une somme de distributions uniformes est propos{\'e}e. En phase de production, la disponibilit{\'e} de donn{\'e}es de mesure permet de raffiner la d{\'e}marche du tol{\'e}rancement statistique. Le mod{\`e}le lin{\'e}aire consid{\'e}r{\'e} peut {\^e}tre corrig{\'e} {\`a} la faveur de nouvelles approches. Une m{\'e}thodologie de gestion des crit{\`e}res d'acceptation sur les valeurs de tol{\'e}rance est {\'e}galement propos{\'e}e, en basant l'outil d'aide {\`a} la d{\'e}cision sur des notions de risques d{\'e}finies en ad{\'e}quation avec les acteurs industriels. Dans le cadre de la r{\'e}vision du partage de tol{\'e}rances dans un assemblage, un probl{\`e}me d'optimisation est formul{\'e} avec des co{\^u}ts industriels appropri{\'e}s afin de proposer le re-partage optimal de tol{\'e}rances dans une cha{\^i}ne de c{\^o}te. Enfin, les m{\'e}thodologies propos{\'e}es sont impl{\'e}ment{\'e}es dans les outils permettant le traitement industriel et la gestion de bout en bout des tol{\'e}rances depuis les pi{\`e}ces {\'e}l{\'e}mentaires jusqu'{\`a} l'assemblage final du produit, contribuant ainsi {\`a} l'{\'e}laboration du jumeau virtuel du produit.},
  langid = {english},
  school = {Universit{\'e} de Toulouse, Universit{\'e} Toulouse III - Paul Sabatier},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Université de Toulouse, Université Toulouse III - Paul Sabatier/2021/Diet_2021_Une approche de bout en bout du tolérancement statistique sous contraintes.pdf;/Users/steven/Zotero/storage/5JX8VRCK/4938.html}
}

@article{difernandRelativeAgeEffects2023,
  title = {Relative Age Effects in Track-and-Field: {{Identification}} and Performance Rebalancing},
  shorttitle = {Relative Age Effects in Track-and-Field},
  author = {Difernand, Audrey and De Larochelambert, Quentin and Homo, S{\'e}bastien and Rousseau, Florian and Antero, Juliana and Toussaint, Jean-Fran{\c c}ois and Sedeaud, Adrien},
  year = {2023},
  journal = {Frontiers in Physiology},
  volume = {13},
  issn = {1664-042X},
  urldate = {2023-10-24},
  abstract = {Introduction: Relative Age Effect (RAE) consists of a biased distribution of the dates of birth in a same-age group.Objectives: This study aimed to investigate Relative Age Effect among French athletes in different track-and-field events, and propose a corrective adjustment method to highlight the true potential of an athlete with respect to his/her relative age.Methods: 358,610 performances from 2009 to 2019 of female and male athletes between 12 and 21~years old were collected. Relative age distributions of performances were analyzed by level of competitiveness (``All,'' ``Top50\%,'' ``Top10\%'' where ``all'' represents all athletes, top50\% and top10\% represent the best 50\% and 10\% of athletes per age category respectively) and age category, with chi-square and odd-ratio statistics. A linear relationship between distribution of performances and age leads to a calibration coefficient allowing to rebalance the performance by considering the effect of Relative Age Effect. Validation is obtained by Wilcoxon statistical test on actual athlete data.Results: Relative Age Effect is present in all types of events. It is larger when the level of competitiveness increases. In male 100~m sprint, 1~year difference between two athletes birth date represents an average gain of 931.01~ms (6.5\%) in the U13 (Under 13~years old) and 229.65~ms (1.9\%) in the U17 (Under 17~years old) categories. Our validated rebalancing methods allows to compensate for the biases induced by the relative age effect. By comparing the rebalanced performance and the realised performance of each athlete, we cannot say that they are significantly different. On average, there is no significant difference between these two performances.Conclusion: This study showed that there is a relative age effect among young French athletes, with an even greater effect as the level of competition increases. Thanks to the rebalancing method that has been validated, performances can now be better appreciated according to category and event.},
  keywords = {sport-science,track-and-field},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Frontiers in Physiology/2023/Difernand et al_2023_Relative age effects in track-and-field.pdf}
}

@misc{diksNoiseReductionFunctional2023,
  title = {Noise Reduction for Functional Time Series},
  author = {Diks, Cees and Wouters, Bram},
  year = {2023},
  month = jul,
  number = {arXiv:2307.02154},
  eprint = {2307.02154},
  primaryclass = {q-fin, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.02154},
  urldate = {2023-10-27},
  abstract = {A novel method for noise reduction in the setting of curve time series with error contamination is proposed, based on extending the framework of functional principal component analysis (FPCA). We employ the underlying, finite-dimensional dynamics of the functional time series to separate the serially dependent dynamical part of the observed curves from the noise. Upon identifying the subspaces of the signal and idiosyncratic components, we construct a projection of the observed curve time series along the noise subspace, resulting in an estimate of the underlying denoised curves. This projection is optimal in the sense that it minimizes the mean integrated squared error. By applying our method to similated and real data, we show the denoising estimator is consistent and outperforms existing denoising techniques. Furthermore, we show it can be used as a pre-processing step to improve forecasting.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,functional-principal-components,functional-time-series,noise-reduction},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Diks_Wouters_2023_Noise reduction for functional time series.pdf;/Users/steven/Zotero/storage/H5BJX9PS/2307.html}
}

@phdthesis{duboisProblemesEstimationSelection2021,
  type = {These de Doctorat},
  title = {Probl{\`e}mes d'estimation, de S{\'e}lection de Variables et de Tests Sous Contraintes de Confidentialit{\'e} Diff{\'e}rentielle Locale},
  author = {Dubois, Amandine},
  year = {2021},
  month = jun,
  urldate = {2023-10-24},
  abstract = {La notion de confidentialit{\'e} diff{\'e}rentielle a {\'e}t{\'e} introduite pour permettre de r{\'e}aliser des analyses statistiques tout en fournissant des garanties de protection des donn{\'e}es personnelles analys{\'e}es. Dans cette th{\`e}se, on s'int{\'e}resse {\`a} trois probl{\`e}mes d'inf{\'e}rence statistique sous contraintes de confidentialit{\'e} diff{\'e}rentielle locale. Dans un premier temps, on s'int{\'e}resse {\`a} l'estimation non-param{\'e}trique d'une densit{\'e} de probabilit{\'e}. Nous {\'e}tudions le risque minimax Lr sur les ellipso{\"i}des de Besov, et nous int{\'e}ressons {\`a} la question de l'adaptation au param{\`e}tre de r{\'e}gularit{\'e}. On s'int{\'e}resse ensuite {\`a} l'identification du support de l'esp{\'e}rance d'une variable al{\'e}atoire suivant une loi normale d-dimensionnelle. Sous des hypoth{\`e}ses de sparsit{\'e}, nous {\'e}tudions le risque minimax li{\'e} {\`a} la distance de Hamming, et en d{\'e}duisons des conditions n{\'e}cessaires et suffisantes pour que l'identification du support soit possible. Enfin, nous {\'e}tudions un probl{\`e}me de test d'ad{\'e}quation pour des densit{\'e}s H{\"o}ld{\'e}riennes dont le support n'est pas suppos{\'e} born{\'e}. Pour chaque probl{\`e}me, nous mettons en {\'e}vidence l'influence des contraintes de confidentialit{\'e} sur les vitesses minimax.},
  collaborator = {Saumard, Adrien and Butucea, Cristina},
  copyright = {Licence Etalab},
  school = {Rennes, {\'E}cole Nationale de la Statistique et de l'Analyse de l'Information},
  keywords = {density-estimation,differential-privacy,minimax-theory,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Rennes, École Nationale de la Statistique et de l'Analyse de l'Information/2021/Dubois_2021_Problèmes d’estimation, de sélection de variables et de tests sous contraintes.pdf}
}

@article{dworkAlgorithmicFoundationsDifferential2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume = {9},
  number = {3-4},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  urldate = {2023-10-24},
  langid = {english},
  keywords = {algorithm-theory,differential-privacy},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Foundations and Trends® in Theoretical Computer Science/2013/Dwork_Roth_2013_The Algorithmic Foundations of Differential Privacy.pdf}
}

@article{edelmanAssociationMenstrualCycle2022,
  title = {Association between Menstrual Cycle Length and Covid-19 Vaccination: Global, Retrospective Cohort Study of Prospectively Collected Data},
  shorttitle = {Association between Menstrual Cycle Length and Covid-19 Vaccination},
  author = {Edelman, Alison and Boniface, Emily R. and Male, Victoria and Cameron, Sharon T. and Benhar, Eleonora and Han, Leo and Matteson, Kristen A. and Lamsweerde, Agathe Van and Pearson, Jack T. and Darney, Blair G.},
  year = {2022},
  month = sep,
  journal = {BMJ Medicine},
  volume = {1},
  number = {1},
  publisher = {{BMJ Specialist Journals}},
  issn = {2754-0413},
  doi = {10.1136/bmjmed-2022-000297},
  urldate = {2023-10-24},
  abstract = {Objectives To identify whether covid-19 vaccines are associated with menstrual changes in order to address concerns about menstrual cycle disruptions after covid-19 vaccination. Design Global, retrospective cohort study of prospectively collected data. Setting International users of the menstrual cycle tracking application, Natural Cycles. Participants 19 622 individuals aged 18-45 years with cycle lengths of 24-38 days and consecutive data for at least three cycles before and one cycle after covid (vaccinated group; n=14 936), and those with at least four consecutive cycles over a similar time period (unvaccinated group; n=4686). Main outcome measures The mean change within individuals was assessed by vaccination group for cycle and menses length (mean of three cycles before vaccination to the cycles after first and second dose of vaccine and the subsequent cycle). Mixed effects models were used to estimate the adjusted difference in change in cycle and menses length between the vaccinated and unvaccinated. Results Most people (n=15 713; 80.08\%) were younger than 35 years, from the UK (n=6222; 31.71\%), US and Canada (28.59\%), or Europe (33.55\%). Two thirds (9929 (66.48\%) of 14 936) of the vaccinated cohort received the Pfizer-BioNTech (BNT162b2) covid-19 vaccine, 17.46\% (n=2608) received Moderna (mRNA-1273), 9.06\% (n=1353) received Oxford-AstraZeneca (ChAdOx1 nCoV-19), and 1.89\% (n=283) received Johnson \& Johnson (Ad26.COV2.S). Individuals who were vaccinated had a less than one day adjusted increase in the length of their first and second vaccine cycles, compared with individuals who were not vaccinated (0.71 day increase (99.3\% confidence interval 0.47 to 0.96) for first dose; 0.56 day increase (0.28 to 0.84) for second dose). The adjusted difference was larger in people who received two doses in a cycle (3.70 days increase (2.98 to 4.42)). One cycle after vaccination, cycle length was similar to before the vaccine in individuals who received one dose per cycle (0.02 day change (99.3\% confidence interval -0.10 to 0.14), but not yet for individuals who received two doses per cycle (0.85 day change (99.3\% confidence interval 0.24 to 1.46)) compared with unvaccinated individuals. Changes in cycle length did not differ by the vaccine's mechanism of action (mRNA, adenovirus vector, or inactivated virus). Menses length was unaffected by vaccination. Conclusions Covid-19 vaccination is associated with a small and likely to be temporary change in menstrual cycle length but no change in menses length.},
  chapter = {Research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2022. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an open access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited, appropriate credit is given, any changes made indicated, and the use is non-commercial. See:~http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  keywords = {menstrual-cycle,women},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/BMJ Medicine/2022/Edelman et al_2022_Association between menstrual cycle length and covid-19 vaccination.pdf}
}

@book{fanLocalPolynomialModelling2017,
  title = {Local {{Polynomial Modelling}} and {{Its Applications}}: {{Monographs}} on {{Statistics}} and {{Applied Probability}} 66},
  shorttitle = {Local {{Polynomial Modelling}} and {{Its Applications}}},
  author = {Fan, Jianqing and Gijbels, Irene},
  year = {2017},
  month = oct,
  publisher = {{Routledge}},
  address = {{New York}},
  doi = {10.1201/9780203748725},
  abstract = {Data-analytic approaches to regression problems, arising from many scientific disciplines are described in this book. The aim of these nonparametric methods is to relax assumptions on the form of a regression function and to let data search for a suitable function that describes the data well. The use of these nonparametric functions with parametric techniques can yield very powerful data analysis tools. Local polynomial modeling and its applications provides an up-to-date picture on state-of-the-art nonparametric regression techniques. The emphasis of the book is on methodologies rather than on theory, with a particular focus on applications of nonparametric techniques to various statistical problems. High-dimensional data-analytic tools are presented, and the book includes a variety of examples. This will be a valuable reference for research and applied statisticians, and will serve as a textbook for graduate students and others interested in nonparametric regression.},
  isbn = {978-0-203-74872-5},
  keywords = {local-polynomial-smoothing,non-parametric-statistics,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Routledge/2017/Fan_Gijbels_2017_Local Polynomial Modelling and Its Applications.pdf}
}

@article{ferreiraEigenvaluesIntegralOperators2009,
  title = {Eigenvalues of {{Integral Operators Defined}} by {{Smooth Positive Definite Kernels}}},
  author = {Ferreira, J. C. and Menegatto, V. A.},
  year = {2009},
  month = may,
  journal = {Integral Equations and Operator Theory},
  volume = {64},
  number = {1},
  pages = {61--81},
  issn = {1420-8989},
  doi = {10.1007/s00020-009-1680-3},
  urldate = {2023-10-24},
  abstract = {We consider integral operators defined by positive definite kernels \$\$K : X {\textbackslash}times X {\textbackslash}rightarrow \{{\textbackslash}mathbb\{C\}\}\$\$, where X is a metric space endowed with a strictly-positive measure. We update upon connections between two concepts of positive definiteness and upgrade on results related to Mercer like kernels. Under smoothness assumptions on K, we present decay rates for the eigenvalues of the integral operator, employing adapted to our purposes multidimensional versions of known techniques used to analyze similar problems in the case where X is an interval. The results cover the case when X is a subset of \$\$\{{\textbackslash}mathbb\{R\}\}\^\{m\}\$\$endowed with the induced Lebesgue measure and the case when X is a subset of the sphere Smendowed with the induced surface Lebesgue measure.},
  langid = {english},
  keywords = {integral-operator,operator-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Integral Equations and Operator Theory/2009/Ferreira_Menegatto_2009_Eigenvalues of Integral Operators Defined by Smooth Positive Definite Kernels.pdf}
}

@article{fluryCommonPrincipalComponents1984,
  title = {Common {{Principal Components}} in {{K Groups}}},
  author = {Flury, Bernhard N.},
  year = {1984},
  journal = {Journal of the American Statistical Association},
  volume = {79},
  number = {388},
  eprint = {2288721},
  eprinttype = {jstor},
  pages = {892--898},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2288721},
  urldate = {2023-10-02},
  abstract = {This article generalizes the method of principal components to so-called "common principal components" as follows: Consider the hypothesis that the covariance matrices {$\Sigma$}i for k populations are simultaneously diagonalizable. That is, there is an orthogonal matrix {$\beta$} such that {$\beta$}'{$\Sigma$}i {$\beta$} is diagonal for i = 1,..., k. I derive the normal-theory maximum likelihood estimates of the common component {$\Sigma$}i matrices and the log-likelihood-ratio statistics for testing this hypothesis. The solution has some favorable properties that do not depend on normality assumptions. Numerical examples illustrate the method. Applications to data reduction, multiple regression, and nonlinear discriminant analysis are sketched.},
  keywords = {maximum-likelihood-estimation,principal-components,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/1984/Flury_1984_Common Principal Components in K Groups.pdf}
}

@phdthesis{freventContributionSpatialStatistics2022,
  type = {These de Doctorat},
  title = {Contribution to Spatial Statistics for High-Dimensional and Survival Data},
  author = {Fr{\'e}vent, Camille},
  year = {2022},
  month = dec,
  urldate = {2023-10-25},
  abstract = {Dans ce m{\'e}moire de th{\`e}se nous nous int{\'e}ressons aux m{\'e}thodes d'apprentissage statistique pour donn{\'e}es spatiales en grande dimension et donn{\'e}es de survie. L'objectif est de d{\'e}velopper des m{\'e}thodes de d{\'e}tection de clusters non supervis{\'e}es avec des statistiques de scan spatiales, {\`a} la fois dans le cadre de l'analyse de donn{\'e}es fonctionnelles, mais aussi pour l'analyse de donn{\'e}es de survie. Nous consid{\'e}rons tout d'abord des donn{\'e}es fonctionnelles univari{\'e}es ou multivari{\'e}es mesur{\'e}es spatialement dans une r{\'e}gion g{\'e}ographique. Nous proposons des statistiques de scan param{\'e}triques et non param{\'e}triques dans ce contexte. Ces approches fonctionnelles univari{\'e}es et multivari{\'e}es {\'e}vitent la perte d'information respectivement d'une m{\'e}thode univari{\'e}e ou multivari{\'e}e appliqu{\'e}e sur des observations moyennes au cours de la p{\'e}riode d'{\'e}tude. Nous {\'e}tudions {\'e}galement les performances de ces approches sur des {\'e}tudes de simulation, avant de les appliquer sur des donn{\'e}es r{\'e}elles {\'e}conomiques et environnementales. Nous nous int{\'e}ressons {\'e}galement {\`a} la d{\'e}tection de clusters spatiaux de temps de survie. Bien qu'il existe d{\'e}j{\`a} dans la litt{\'e}rature des approches de statistiques de scan spatiale dans ce cadre, celles-ci ne permettent pas de prendre en compte une {\'e}ventuelle corr{\'e}lation entre les temps de survie des individus d'une m{\^e}me unit{\'e} spatiale. De plus, la nature spatiale des donn{\'e}es implique une potentielle d{\'e}pendance entre les unit{\'e}s spatiales, qui doit {\^e}tre prise en compte. L'originalit{\'e} de l'approche que nous proposons est le d{\'e}veloppement d'une nouvelle statistique de scan spatiale bas{\'e}e sur un mod{\`e}le de Cox {\`a} fragilit{\'e} spatiale, permettant {\`a} la fois la prise en compte de la corr{\'e}lation entre les temps de survie des individus d'une m{\^e}me unit{\'e} spatiale, et une {\'e}ventuelle d{\'e}pendance entre les unit{\'e}s spatiales. Nous avons compar{\'e} les performances de cette nouvelle approche avec les m{\'e}thodes existantes et nous les avons appliqu{\'e}es sur des donn{\'e}es r{\'e}elles de temps de survie des personnes {\^a}g{\'e}es atteintes d'insuffisance r{\'e}nale chronique terminale dans le nord de la France. Enfin, nous proposons un certain nombre de perspectives {\`a} notre travail, {\`a} la fois avec des prolongements directs {\`a} cette th{\`e}se dans le cadre des statistiques de scan spatiales pour donn{\'e}es en grande dimension et donn{\'e}es de survie, mais {\'e}galement avec des perspectives dans un cadre plus large d'analyse spatiale non supervis{\'e}e (clustering spatial pour donn{\'e}es en grande dimension mod{\'e}lis{\'e}es par des tenseurs), et d'apprentissage spatial supervis{\'e} (r{\'e}gression).},
  collaborator = {Genin, Micha{\"e}l and {Dabo-Niang}, Sophie},
  copyright = {Licence Etalab},
  school = {Universit{\'e} de Lille (2022-....)},
  keywords = {clustering,functional-data-analysis,spatial-statistics,survival-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Université de Lille (2022-....)/2022/Frévent_2022_Contribution to spatial statistics for high-dimensional and survival data.pdf}
}

@article{galeanoMahalanobisDistanceFunctional2015,
  title = {The {{Mahalanobis Distance}} for {{Functional Data With Applications}} to {{Classification}}},
  author = {Galeano, Pedro and Joseph, Esdras and Lillo, Rosa E.},
  year = {2015},
  journal = {Technometrics},
  volume = {57},
  number = {2},
  eprint = {24587309},
  eprinttype = {jstor},
  pages = {281--291},
  publisher = {{[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]}},
  issn = {0040-1706},
  urldate = {2023-10-31},
  abstract = {This article presents a new semidistance for functional observations that generalizes the Mahalanobis distance for multivariate datasets. The main characteristics of the functional Mahalanobis semidistance are shown. To illustrate the applicability of this measure of proximity between functional observations, new versions of several well-known functional classification procedures are developed using the functional Mahalanobis semidistance. A Monte Carlo study and the analysis of two real examples indicate that the classification methods used in conjunction with the functional Mahalanobis semidistance give better results than other well-known functional classification procedures. This article has supplementary material online.},
  keywords = {classification,functional-data-analysis,functional-principal-components,mahalanobis-distance},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Technometrics/2015/Galeano et al_2015_The Mahalanobis Distance for Functional Data With Applications to Classification_supp.pdf;/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Technometrics/2015/Galeano et al_2015_The Mahalanobis Distance for Functional Data With Applications to Classification.pdf}
}

@phdthesis{gauthierListRecommendationsMultiarmed2022,
  type = {These de Doctorat},
  title = {List Recommendations with Multi-Armed Bandits},
  author = {Gauthier, Camille-Sovanneary},
  year = {2022},
  month = mar,
  urldate = {2023-10-24},
  abstract = {Nous {\'e}tudions le probl{\`e}me d'apprentissage de l'ordonnancement en ligne de L items pour K positions pr{\'e}d{\'e}finies sur une page web. Pour cela, nous nous int{\'e}ressons aux algorithmes de bandits manchots qui apprennent les param{\`e}tres de mod{\`e}les de clics identifi{\'e}s, tel que le mod{\`e}le bas{\'e} sur les positions (PBM). Les algorithmes de l'{\'e}tat-de-l'art s'attaquent rarement au PBM complet, o{\`u} tous les param{\`e}tres sont inconnus. De plus, l'{\'e}tat de l'art contient peu d'algorithmes bas{\'e}s sur Thompson Sampling ou sur les bandits unimodaux, malgr{\'e} leurs performances empiriques reconnues. Nos deux premi{\`e}res contributions s'appuient sur les bandits unimodaux : GRAB est sp{\'e}cialis{\'e} pour un PBM complet et UniRank, traite des mod{\`e}les de clics divers. Ces deux contributions, tr{\`e}s efficaces, ont une borne sup{\'e}rieure de regret th{\'e}orique. La troisi{\`e}me contribution fournit une famille de bandits adressant le probl{\`e}me PBM complet en couplant l'algorithme Thompson Sampling avec des m{\'e}thodes d'{\'e}chantillonnage par   cha{\^i}nes de Markov Monte-Carlo (MCMC). Deux m{\'e}thodes MCMC sont utilis{\'e}es : par descente de gradient par Langevin,  donnant des r{\'e}sultats empiriques semblables {\`a} l'{\'e}tat de l'art avec un temps de calcul bas et stable, et par Metropolis Hasting, qui offre le regret empirique le plus bas pour ce probl{\`e}me pour un PBM complet.},
  collaborator = {Fromont, {\'E}lisa and Gaudel, Romaric},
  copyright = {Licence Etalab},
  school = {Rennes 1},
  keywords = {bandits-theory,markov-process,online-learning,recommendation-systems},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Rennes 1/2022/Gauthier_2022_List recommendations with multi-armed bandits.pdf}
}

@phdthesis{genetayQuelquesProblematiquesAutour2022,
  type = {These de Doctorat},
  title = {Quelques Probl{\'e}matiques Autour Du Clustering : Robustesse, Grande Dimension et D{\'e}tection d'intrusion},
  shorttitle = {Quelques Probl{\'e}matiques Autour Du Clustering},
  author = {Genetay, Edouard},
  year = {2022},
  month = may,
  urldate = {2023-10-24},
  abstract = {Le clustering vise {\`a} regrouper les donn{\'e}es observ{\'e}es en diff{\'e}rents sous-ensembles partageant des propri{\'e}t{\'e}s similaires. Le plus souvent ce regroupement se fait via l'optimisation d'un crit{\`e}re choisi {\`a} l'avance. Dans cette th{\`e}se CIFRE, nous avons {\'e}tudi{\'e} le clustering sous trois aspects diff{\'e}rents.Dans une premi{\`e}re partie, nous proposons une m{\'e}thode d'estimation robuste de K centro{\"i}des bas{\'e} sur le crit{\`e}re, dit des {\guillemotleft} K-means {\guillemotright}. Nous proposons {\'e}galement une m{\'e}thode d'initialisation robuste de la proc{\'e}dure. D'une part, la robustesse des proc{\'e}dures propos{\'e}es a {\'e}t{\'e} test{\'e}e par de nombreuses simulations num{\'e}riques. D'autre part, nous avons montr{\'e} un th{\'e}or{\`e}me donnant la vitesse de convergence d'un estimateur id{\'e}alis{\'e} en pr{\'e}sence d'outliers ainsi qu'un th{\'e}or{\`e}me donnant le breakdown point de la m{\'e}thode. Dans une seconde partie nous nous pla{\c c}ons dans le cadre d'un m{\'e}lange {\'e}quilibr{\'e} de deux gaussiennes isotropes, centr{\'e} en l'origine, afin de fournir la premi{\`e}re analyse th{\'e}orique d'un estimateur de clustering bas{\'e} sur un crit{\`e}re d'entropie conditionnelle. Nous montrons que le crit{\`e}re est localement convexe, offrant d'une part des vitesses d'apprentissage rapide et d'autre part une in{\'e}galit{\'e} oracle en grande dimension, lorsque le vecteur moyen de s{\'e}paration est sparse.Dans une troisi{\`e}me partie, plus pratique et consacr{\'e}e {\`a} des graphes en cybers{\'e}curit{\'e}, nous regardons si l'{\'e}volution du nombre de clusters obtenus par une m{\'e}thode d'optimisation de modularit{\'e} peut r{\'e}v{\'e}ler des anomalies caus{\'e}es par une intrusion dans un syst{\`e}me informatique.},
  collaborator = {Saumard, Adrien and Patilea, Valentin},
  copyright = {Licence Etalab},
  school = {Rennes, {\'E}cole Nationale de la Statistique et de l'Analyse de l'Information},
  keywords = {clustering},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Rennes, École Nationale de la Statistique et de l'Analyse de l'Information/2022/Genetay_2022_Quelques problématiques autour du clustering.pdf}
}

@article{gertheissLongitudinalScalaronfunctionsRegression2013,
  title = {Longitudinal Scalar-on-Functions Regression with Application to Tractography Data},
  author = {Gertheiss, Jan and Goldsmith, Jeff and Crainiceanu, Ciprian and Greven, Sonja},
  year = {2013},
  month = jul,
  journal = {Biostatistics},
  volume = {14},
  number = {3},
  pages = {447--461},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxs051},
  urldate = {2023-09-19},
  abstract = {We propose a class of estimation techniques for scalar-on-function regression where both outcomes and functional predictors may be observed at multiple visits. Our methods are motivated by a longitudinal brain diffusion tensor imaging tractography study. One of the study's primary goals is to evaluate the contemporaneous association between human function and brain imaging over time. The complexity of the study requires the development of methods that can simultaneously incorporate: (1) multiple functional (and scalar) regressors; (2) longitudinal outcome and predictor measurements per patient; (3) Gaussian or non-Gaussian outcomes; and (4) missing values within functional predictors. We propose two versions of a new method, longitudinal functional principal components regression (PCR). These methods extend the well-known functional PCR and allow for different effects of subject-specific trends in curves and of visit-specific deviations from that trend. The new methods are compared with existing approaches, and the most promising techniques are used for analyzing the tractography data.},
  keywords = {functional-principal-components,functional-regression,longitudinal-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biostatistics/2013/Gertheiss et al_2013_Longitudinal scalar-on-functions regression with application to tractography.pdf;/Users/steven/Zotero/storage/37R2937Y/259826.html}
}

@misc{giddensDPpackPackageDifferentially2023,
  title = {{{DPpack}}: {{An R Package}} for {{Differentially Private Statistical Analysis}} and {{Machine Learning}}},
  shorttitle = {{{DPpack}}},
  author = {Giddens, Spencer and Liu, Fang},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10965},
  eprint = {2309.10965},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.10965},
  urldate = {2023-10-25},
  abstract = {Differential privacy (DP) is the state-of-the-art framework for guaranteeing privacy for individuals when releasing aggregated statistics or building statistical/machine learning models from data. We develop the open-source R package DPpack that provides a large toolkit of differentially private analysis. The current version of DPpack implements three popular mechanisms for ensuring DP: Laplace, Gaussian, and exponential. Beyond that, DPpack provides a large toolkit of easily accessible privacy-preserving descriptive statistics functions. These include mean, variance, covariance, and quantiles, as well as histograms and contingency tables. Finally, DPpack provides user-friendly implementation of privacy-preserving versions of logistic regression, SVM, and linear regression, as well as differentially private hyperparameter tuning for each of these models. This extensive collection of implemented differentially private statistics and models permits hassle-free utilization of differential privacy principles in commonly performed statistical analysis. We plan to continue developing DPpack and make it more comprehensive by including more differentially private machine learning techniques, statistical modeling and inference in the future.},
  archiveprefix = {arxiv},
  keywords = {differential-privacy,r-software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Giddens_Liu_2023_DPpack.pdf;/Users/steven/Zotero/storage/QHIUMWU2/2309.html}
}

@phdthesis{goffinetClusteringMultiblocsVisualisation2021,
  type = {These de Doctorat},
  title = {Clustering Multi-Blocs et Visualisation Analytique de Donn{\'e}es S{\'e}quentielles Massives Issues de Simulation Du V{\'e}hicule Autonome},
  author = {Goffinet, {\'E}tienne},
  year = {2021},
  month = dec,
  urldate = {2023-10-25},
  abstract = {La validation des syst{\`e}mes avanc{\'e}s d'aide {\`a} la conduite reste l'un des plus grands d{\'e}fis que les constructeurs automobiles doivent relever pour fournir des voitures autonomes s{\^u}res. La validation fiable de ces syst{\`e}mes n{\'e}cessite d'{\'e}valuer la qualit{\'e} et la coh{\'e}rence de leur r{\'e}action dans un large {\'e}ventail de sc{\'e}narios de conduite. Dans ce contexte, les syst{\`e}mes de simulation {\`a} grande {\'e}chelle contournent les limites de la validation physique et produisent d'importantes quantit{\'e}s de s{\'e}ries temporelles en haute dimension. Le d{\'e}fi est de trouver des informations utiles dans ces ensembles de donn{\'e}es multivari{\'e}es non {\'e}tiquet{\'e}es qui peuvent contenir des variables bruit{\'e}es, parfois corr{\'e}l{\'e}es ou non informatives. Cette th{\`e}se propose plusieurs outils bas{\'e}s sur des mod{\`e}les probabilistes pour le regroupement non-supervis{\'e} de s{\'e}ries temporelles univari{\'e}es et multivari{\'e}es, bas{\'e}s sur une approche Dictionnaire ou dans un cadre bay{\'e}sien non param{\'e}trique. L'objectif est de trouver automatiquement des groupes pertinents et naturels de comportements de conduite et, dans le cas multivari{\'e}, d'effectuer une s{\'e}lection de mod{\`e}les et une r{\'e}duction de la dimension des s{\'e}ries temporelles multivari{\'e}es. Les m{\'e}thodes sont exp{\'e}riment{\'e}es sur des jeux de donn{\'e}es simul{\'e}s et appliqu{\'e}es {\`a} des cas d'usage industriels du Groupe Renault.},
  collaborator = {Lebbah, Mustapha and Azzag, Hanane},
  copyright = {Licence Etalab},
  school = {Paris 13},
  keywords = {automotive-data,clustering,multivariate-time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Paris 13/2021/Goffinet_2021_Clustering multi-blocs et visualisation analytique de données séquentielles.pdf}
}

@article{goldsmithCorrectedConfidenceBands2013,
  title = {Corrected {{Confidence Bands}} for {{Functional Data Using Principal Components}}},
  author = {Goldsmith, J. and Greven, S. and Crainiceanu, C.},
  year = {2013},
  journal = {Biometrics},
  volume = {69},
  number = {1},
  pages = {41--51},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2012.01808.x},
  urldate = {2024-01-10},
  abstract = {Functional principal components (FPC) analysis is widely used to decompose and express functional observations. Curve estimates implicitly condition on basis functions and other quantities derived from FPC decompositions; however these objects are unknown in practice. In this article, we propose a method for obtaining correct curve estimates by accounting for uncertainty in FPC decompositions. Additionally, pointwise and simultaneous confidence intervals that account for both model- and decomposition-based variability are constructed. Standard mixed model representations of functional expansions are used to construct curve estimates and variances conditional on a specific decomposition. Iterated expectation and variance formulas combine model-based conditional estimates across the distribution of decompositions. A bootstrap procedure is implemented to understand the uncertainty in principal component decomposition quantities. Our method compares favorably to competing approaches in simulation studies that include both densely and sparsely observed functions. We apply our method to sparse observations of CD4 cell counts and to dense white-matter tract profiles. Code for the analyses and simulations is publicly available, and our method is implemented in the R package refund on CRAN.},
  copyright = {Copyright {\copyright} 2013, The International Biometric Society},
  langid = {english},
  keywords = {Bootstrap,Functional principal components analysis,Iterated expectation and variance,Simultaneous bands},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/2013/Goldsmith et al_2013_Corrected Confidence Bands for Functional Data Using Principal Components.pdf}
}

@misc{goldsmithRefundRegressionFunctional2023,
  title = {Refund: {{Regression}} with {{Functional Data}}},
  shorttitle = {Refund},
  author = {Goldsmith, Jeff and Scheipl, Fabian and Huang, Lei and Wrobel, Julia and Di, Chongzhi and Gellar, Jonathan and Harezlak, Jaroslaw and McLean, Mathew W. and Swihart, Bruce and Xiao, Luo and Crainiceanu, Ciprian and Reiss, Philip T. and Chen, Yakuan and Greven, Sonja and Huo, Lan and Kundu, Madan Gopal and Park, So Young and Miller, David L. and Staicu, Ana-Maria and Cui, Erjia and Li, Ruonan and Li, Zheyuan},
  year = {2023},
  month = dec,
  urldate = {2024-01-10},
  abstract = {Methods for regression for functional data, including function-on-scalar, scalar-on-function, and function-on-function regression. Some of the functions are applicable to image data.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2023/Goldsmith et al_2023_refund.pdf}
}

@misc{golovkineAdaptiveEstimationIrregular2023,
  title = {Adaptive Estimation of Irregular Mean and Covariance Functions},
  author = {Golovkine, Steven and Klutchnikoff, Nicolas and Patilea, Valentin},
  year = {2023},
  month = jul,
  number = {arXiv:2108.06507},
  eprint = {2108.06507},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.06507},
  urldate = {2023-09-09},
  abstract = {Nonparametric estimators for the mean and the covariance functions of functional data are proposed. The setup covers a wide range of practical situations. The random trajectories are, not necessarily differentiable, have unknown regularity, and are measured with error at discrete design points. The measurement error could be heteroscedastic. The design points could be either randomly drawn or common for all curves. The estimators depend on the local regularity of the stochastic process generating the functional data. We consider a simple estimator of this local regularity which exploits the replication and regularization features of functional data. Next, we use the ``smoothing first, then estimate'' approach for the mean and the covariance functions. They can be applied with both sparsely or densely sampled curves, are easy to calculate and to update, and perform well in simulations. Simulations built upon an example of real data set, illustrate the effectiveness of the new approach.},
  archiveprefix = {arxiv},
  keywords = {covariance-operator-estimation,functional-data-analysis,mean-function-estimation,smoothness},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Golovkine et al_2023_Adaptive estimation of irregular mean and covariance functions.pdf;/Users/steven/Zotero/storage/M8RCD7IV/2108.html}
}

@article{golovkineClusteringMultivariateFunctional2022,
  title = {Clustering Multivariate Functional Data Using Unsupervised Binary Trees},
  author = {Golovkine, Steven and Klutchnikoff, Nicolas and Patilea, Valentin},
  year = {2022},
  month = apr,
  journal = {Computational Statistics \& Data Analysis},
  volume = {168},
  pages = {107376},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2021.107376},
  urldate = {2023-09-01},
  abstract = {A model-based clustering algorithm is proposed for a general class of functional data for which the components could be curves or images. The random functional data realizations could be measured with errors at discrete, and possibly random, points in the definition domain. The idea is to build a set of binary trees by recursive splitting of the observations. The number of groups are determined in a data-driven way. The new algorithm provides easily interpretable results and fast predictions for online data sets. Results on simulated datasets reveal good performance in various complex settings. The methodology is applied to the analysis of vehicle trajectories on a German roundabout.},
  keywords = {clustering,functional-principal-components,gaussian-mixture-model,model-based-clustering,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2022/Golovkine et al_2022_Clustering multivariate functional data using unsupervised binary trees.pdf;/Users/steven/Zotero/storage/RFZMW5EA/S0167947321002103.html}
}

@misc{golovkineEstimationNumberComponents2023,
  title = {On the Estimation of the Number of Components in Multivariate Functional Principal Component Analysis},
  author = {Golovkine, Steven and Gunning, Edward and Simpkin, Andrew J. and Bargary, Norma},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04540},
  eprint = {2311.04540},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2311.04540},
  urldate = {2024-01-15},
  abstract = {Happ and Greven (2018) developed a methodology for principal components analysis of multivariate functional data for data observed on different dimensional domains. Their approach relies on an estimation of univariate functional principal components for each univariate functional feature. In this paper, we present extensive simulations to investigate choosing the number of principal components to retain. We show empirically that the conventional approach of using a percentage of variance explained threshold for each univariate functional feature may be unreliable when aiming to explain an overall percentage of variance in the multivariate functional data, and thus we advise practitioners to be careful when using it.},
  archiveprefix = {arxiv},
  keywords = {62R10,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Golovkine et al_2023_On the estimation of the number of components in multivariate functional.pdf;/Users/steven/Zotero/storage/N4TQGQBQ/2311.html}
}

@misc{golovkineFDApyPythonPackage2021,
  title = {{{FDApy}}: A {{Python}} Package for Functional Data},
  shorttitle = {{{FDApy}}},
  author = {Golovkine, Steven},
  year = {2021},
  month = jan,
  number = {arXiv:2101.11003},
  eprint = {2101.11003},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.11003},
  urldate = {2023-09-09},
  abstract = {We introduce the Python package, FDApy, as an implementation of functional data. This package provide modules for the analysis of such data. It includes classes for different dimensional data as well as irregularly sampled functional data. A simulation toolbox is also provided. It might be used to simulate different clusters of functional data. Some methodologies to handle these data are implemented, such as dimension reduction and clustering. New methods can be easily added. The package is publicly available on the Python Package Index and Github.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,multivariate-functional-data,python-software,software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2021/Golovkine_2021_FDApy.pdf;/Users/steven/Zotero/storage/MXCQQVFS/2101.html}
}

@article{golovkineLearningSmoothnessNoisy2022,
  title = {Learning the Smoothness of Noisy Curves with Application to Online Curve Estimation},
  author = {Golovkine, Steven and Klutchnikoff, Nicolas and Patilea, Valentin},
  year = {2022},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {16},
  number = {1},
  pages = {1485--1560},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/22-EJS1997},
  urldate = {2023-09-01},
  abstract = {Combining information both within and across trajectories, we propose a simple estimator for the local regularity of the trajectories of a stochastic process. Independent trajectories are measured with errors at randomly sampled time points. The proposed approach is model-free and applies to a large class of stochastic processes. Non-asymptotic bounds for the concentration of the estimator are derived. Given the estimate of the local regularity, we build a nearly optimal local polynomial smoother from the curves from a new, possibly very large sample of noisy trajectories. We derive non-asymptotic pointwise risk bounds uniformly over the new set of curves. Our estimates perform well in simulations, in both cases of differentiable or non-differentiable trajectories. Real data sets illustrate the effectiveness of the new approaches.},
  keywords = {adaptive-optimal-smoothing,functional-data-analysis,smoothness,traffic-flow},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Electronic Journal of Statistics/2022/Golovkine et al_2022_Learning the smoothness of noisy curves with application to online curve.pdf}
}

@misc{golovkineUseGramMatrix2023,
  title = {On the Use of the {{Gram}} Matrix for Multivariate Functional Principal Components Analysis},
  author = {Golovkine, Steven and Gunning, Edward and Simpkin, Andrew J. and Bargary, Norma},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12949},
  eprint = {2306.12949},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.12949},
  urldate = {2023-09-09},
  abstract = {Dimension reduction is crucial in functional data analysis (FDA). The key tool to reduce the dimension of the data is functional principal component analysis. Existing approaches for functional principal component analysis usually involve the diagonalization of the covariance operator. With the increasing size and complexity of functional datasets, estimating the covariance operator has become more challenging. Therefore, there is a growing need for efficient methodologies to estimate the eigencomponents. Using the duality of the space of observations and the space of functional features, we propose to use the inner-product between the curves to estimate the eigenelements of multivariate and multidimensional functional datasets. The relationship between the eigenelements of the covariance operator and those of the inner-product matrix is established. We explore the application of these methodologies in several FDA settings and provide general guidance on their usability.},
  archiveprefix = {arxiv},
  keywords = {dimension-reduction,functional-data-analysis,functional-principal-components,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Golovkine et al_2023_On the use of the Gram matrix for multivariate functional principal components.pdf;/Users/steven/Zotero/storage/ENF6DK3X/2306.html}
}

@phdthesis{gonzalezhernandezRepresentingFunctionalData2010,
  type = {{{doctoralThesis}}},
  title = {Representing Functional Data in Reproducing {{Kernel Hilbert}} Spaces with Applications to Clustering, Classification and Time Series Problems},
  author = {Gonz{\'a}lez Hern{\'a}ndez, Javier},
  year = {2010},
  month = may,
  urldate = {2023-10-24},
  abstract = {In modern data analysis areas such as Image Analysis, Chemometrics or Information Retrieval the raw data are often complex and their representation in Euclidean spaces is not straightforward. However most statistical data analysis techniques are designed to deal with points in Euclidean spaces and hence a representation of the data in some Euclidean coordinate system is always required as a previous step to apply multivariate analysis techniques. This process is crucial to guarantee the success of the data analysis methodologies and will be a core contribution of this thesis. In this work we will develop general data representation techniques in the framework of Functional Data Analysis (FDA) for classification and clustering problems. In Chapter 1 we motivate the problems to solve, describe the roadmap of the contributions and set up the notation of this work. In Chapter 2 we review some aspects concerning Reproducing Kernel Hilbert Spaces (RKHSs), Regularization Theory Integral Operators, Support Vector Machines and Kernel Combinations. In Chapter 3 we propose a new methodology to obtain finite-dimensional representations of functional data. The key idea is to consider each functional curve as a point in a general function space and then project these points onto a Reproducing Kernel Hilbert Space (RKHS) with the aid of Regularization theory. We will describe the projection methods, analyze its theoretical properties and develop an strategy to select appropriate RKHSs to represent the functional data. Following the functional data analysis approach, we develop in Chapter 4 a new procedure to deal with proximity (similarity or distance) matrices in classification problems by studying the connection between proximity measures and a certain class of integral operators. The idea is to come up with a methodology able to estimate an integral operator whose associated kernel function, evaluated at the sample, approximates the sample proximity matrix of the problem. To show the broad scope of application of the methodology,we will apply it to three cases: (1) classification problems where the only available information about the data is an asymmetric similarity matrix (2) partially labeled classification problems and (3) classification problems where several sources of information are available and can be combined to obtain the discrimination function. In Chapter 5 we propose an spectral framework for information fusion when the sources of information are given by a set of proximity matrices. Our approach is based on the simultaneous diagonalization of the original matrices of the problem and it represents a natural way to manage the redundant information involved in the fusion process. In particular, we define a new metric for proximity matrices and we propose a method that automatically eliminates the redundant information among a set of matrices when they are combined. We conclude the contributions of the thesis in Chapter 6 with a battery of simulated and real examples devoted to compare the performance of the proposed methodologies with the state of the art in representation methods. Finally, in Chapter 7 we include a discussion regarding the topics described above and we propose some future lines of research we believe are the natural extensions to the work developed in this thesis. ------------------------------------------------------------------------------------------------------------------------------------------------},
  copyright = {Atribuci{\'o}n-NoComercial-SinDerivadas 3.0 Espa{\~n}a},
  langid = {english},
  keywords = {classification,clustering,functional-data-analysis,reproducing-kernel-hilbert-space},
  annotation = {Accepted: 2010-10-13T08:11:00Z},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2010/González Hernández_2010_Representing functional data in reproducing Kernel Hilbert spaces with.pdf}
}

@article{gonzalezRepresentingFunctionalData2010,
  title = {Representing Functional Data in Reproducing {{Kernel Hilbert Spaces}} with Applications to Clustering and Classification},
  author = {Gonz{\'a}lez, Javier and Mu{\~n}oz, Alberto},
  year = {2010},
  month = may,
  journal = {DES - Working Papers. Statistics and Econometrics. WS},
  number = {ws102713},
  publisher = {{Universidad Carlos III de Madrid. Departamento de Estad{\'i}stica}},
  urldate = {2023-07-08},
  abstract = {Functional data are difficult to manage for many traditional statistical techniques given their very high (or intrinsically infinite) dimensionality. The reason is that functional data are essentially functions and most algorithms are designed to work with (low) finite-dimensional vectors. Within this context we propose techniques to obtain finitedimensional representations of functional data. The key idea is to consider each functional curve as a point in a general function space and then project these points onto a Reproducing Kernel Hilbert Space with the aid of Regularization theory. In this work we describe the projection method, analyze its theoretical properties and propose a model selection procedure to select appropriate Reproducing Kernel Hilbert spaces to project the functional data.},
  langid = {english},
  keywords = {classification,clustering,functional-data-analysis,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/DES - Working Papers. Statistics and Econometrics. WS/2010/González_Muñoz_2010_Representing functional data in reproducing Kernel Hilbert Spaces with.pdf;/Users/steven/Zotero/storage/U9SIH6GN/ws102713.html}
}

@phdthesis{grailContributionLectureAutomatique2021,
  type = {These de Doctorat},
  title = {Contribution {\`a} La Lecture Automatique {\`a} l'aide de R{\'e}seaux Neuronaux Profonds},
  author = {Grail, Quentin},
  year = {2021},
  month = nov,
  urldate = {2023-10-25},
  abstract = {La compr{\'e}hension automatique du langage naturel est un d{\'e}fi important de l'intelligence artificielle.Dans cette dissertation, nous d{\'e}crivons l'ensemble de nos contributions apport{\'e}es {\`a} ce domaine.Nous pr{\'e}sentons plusieurs directions que nous pensons cruciales {\`a} la construction de meilleurs syst{\`e}mes de traitement automatique du langage naturel.La premi{\`e}re partie de cette dissertation couvre certains concepts essentiels notamment en proposant un historique rapide de la repr{\'e}sentation vectorielle de mots ainsi que des t{\^a}ches de lecture et de r{\'e}sum{\'e} automatique de texte.Cette partie d{\'e}crit certains des principaux objectifs qui ont guid{\'e}s la recherche durant ces derni{\`e}res ann{\'e}es jusqu'{\`a} la r{\'e}cente r{\'e}volution de l'apprentissage profond appliqu{\'e}e au traitement du langage naturel.Le premier th{\`e}me d{\'e}velopp{\'e} dans cette th{\`e}se concerne la compr{\'e}hension automatique de texte au travers de la t{\^a}che de question-r{\'e}ponse.Nos contributions dans ce domaine sont li{\'e}es {\`a} trois aspects principaux : les donn{\'e}es d'{\'e}valuation, les algorithmes d'apprentissage, la construction de nouveaux mod{\`e}les.Dans ce premier th{\`e}me, nous proposons un jeu de donn{\'e}es de question-r{\'e}ponse permettant d'{\'e}valuer les comp{\'e}tences de raisonnement relationnel du syst{\`e}me de lecture.Ensuite, nous proposons un protocole d'apprentissage adversarial ayant pour but de g{\'e}n{\'e}rer automatiquement des exemples bruit{\'e}s afin d'am{\'e}liorer les performances du mod{\`e}le de lecture.Finalement, nous d{\'e}crivons nos travaux propos{\'e}s dans le cadre de question-r{\'e}ponse multi-hop. La t{\^a}che de question-r{\'e}ponse est assez g{\'e}n{\'e}rale et de nouveaux types de questions ont {\'e}merg{\'e}s ces derni{\`e}res ann{\'e}es dans le but d'{\'e}valuer diff{\'e}rentes comp{\'e}tences des mod{\`e}les de lecture.Les questions multi-hop font partie de ces nouvelles directions et n{\'e}cessite au lecteur de collecter de l'information dans plusieurs parties de documents afin de r{\'e}pondre correctement {\`a} une question.Nous pensons que cette t{\^a}che est un pas de plus vers la construction de meilleurs mod{\`e}les de compr{\'e}hension du langage et proposons notre contribution au travers d'un mod{\`e}le de lecture efficace et interpr{\'e}table.L'explosion de l'apprentissage profond associ{\'e} {\`a} l'augmentation de la puissance de calcul des machines modernes {\`a} conduit {\`a} des progr{\`e}s remarquables dans le domaine du traitement du langage naturel.Cependant, les r{\'e}centes architectures d{\'e}velopp{\'e}es ont tendance {\`a} {\^e}tre {\'e}valu{\'e}es sur des t{\^a}ches n{\'e}cessitant de lire uniquement des textes de taille relativement mod{\'e}r{\'e}e.Le deuxi{\`e}me th{\`e}me couvert dans cette th{\`e}se concerne l'apprentissage de repr{\'e}sentations de textes longs en utilisant diff{\'e}rentes architectures d'apprentissage profond {\'e}tat de l'art.Nous d{\'e}crivons notre proposition ayant pour but d'am{\'e}liorer les r{\'e}centes approches propos{\'e}es, en les adaptant pour des t{\^a}ches n{\'e}cessitant le traitement de documents longs.Nous avons {\'e}valu{\'e} cette proposition sur une t{\^a}che de r{\'e}sum{\'e} extractif de textes scientifiques et pr{\'e}sentons des r{\'e}sultats encourageants ne n{\'e}cessitant qu'une adaptation minimale des architectures existantes.},
  collaborator = {Gaussier, {\'E}ric},
  copyright = {Licence Etalab},
  school = {Universit{\'e} Grenoble Alpes},
  keywords = {adversarial-learning,deep-learning,natural-language-processing},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Université Grenoble Alpes/2021/Grail_2021_Contribution à la lecture automatique à l’aide de réseaux neuronaux profonds.pdf}
}

@article{grevenLongitudinalFunctionalPrincipal2010,
  title = {Longitudinal Functional Principal Component Analysis},
  author = {Greven, Sonja and Crainiceanu, Ciprian and Caffo, Brian and Reich, Daniel},
  year = {2010},
  journal = {Electronic journal of statistics},
  volume = {4},
  pages = {1022--1054},
  issn = {1935-7524},
  doi = {10.1214/10-EJS575},
  urldate = {2024-01-28},
  abstract = {We introduce models for the analysis of functional data observed at multiple time points. The dynamic behavior of functional data is decomposed into a time-dependent population average, baseline (or static) subject-specific variability, longitudinal (or dynamic) subject-specific variability, subject-visit-specific variability and measurement error. The model can be viewed as the functional analog of the classical longitudinal mixed effects model where random effects are replaced by random processes. Methods have wide applicability and are computationally feasible for moderate and large data sets. Computational feasibility is assured by using principal component bases for the functional processes. The methodology is motivated by and applied to a diffusion tensor imaging (DTI) study designed to analyze differences and changes in brain connectivity in healthy volunteers and multiple sclerosis (MS) patients. An R implementation is provided., 87},
  pmcid = {PMC3131008},
  pmid = {21743825},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Electronic journal of statistics/2010/Greven et al_2010_Longitudinal functional principal component analysis.pdf}
}

@article{grithFunctionalPrincipalComponent2018,
  title = {Functional {{Principal Component Analysis}} for {{Derivatives}} of {{Multivariate Curves}}},
  author = {Grith, Maria and Wagner, Heiko and H{\"a}rdle, Wolfgang K. and Kneip, Alois},
  year = {2018},
  journal = {Statistica Sinica},
  volume = {28},
  number = {4},
  eprint = {26511222},
  eprinttype = {jstor},
  pages = {2469--2496},
  publisher = {{Institute of Statistical Science, Academia Sinica}},
  issn = {1017-0405},
  urldate = {2023-11-20},
  abstract = {We propose two methods based on the functional principal component analysis (FPCA) to estimate smooth derivatives for a sample of observed curves with a multidimensional domain. We apply the eigendecomposition to a) the dual covariance matrix of the derivatives; b) the dual covariance matrix of the observed curves, and take derivatives of their eigenfunctions. To handle noisy and discrete observations, we rely on local polynomial regression. We show that if the curves are contained in a finite-dimensional function space, the second method performs better asymptotically. We apply our methodology in simulations and an empirical study of option implied state price density surfaces. Using call data for the DAX 30 stock index between 2002 and 2011, we identify three components that are interpreted as volatility, skewness and tail factors, and we find evidence of term structure variation.},
  keywords = {derivatives,functional-principal-components,multidimensional-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistica Sinica/2018/Grith et al_2018_Functional Principal Component Analysis for Derivatives of Multivariate Curves.pdf}
}

@misc{grossmannCanPhysicsInformedNeural2023,
  title = {Can {{Physics-Informed Neural Networks}} Beat the {{Finite Element Method}}?},
  author = {Grossmann, Tamara G. and Komorowska, Urszula Julia and Latz, Jonas and Sch{\"o}nlieb, Carola-Bibiane},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04107},
  eprint = {2302.04107},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04107},
  urldate = {2023-10-26},
  abstract = {Partial differential equations play a fundamental role in the mathematical modelling of many processes and systems in physical, biological and other sciences. To simulate such processes and systems, the solutions of PDEs often need to be approximated numerically. The finite element method, for instance, is a usual standard methodology to do so. The recent success of deep neural networks at various approximation tasks has motivated their use in the numerical solution of PDEs. These so-called physics-informed neural networks and their variants have shown to be able to successfully approximate a large range of partial differential equations. So far, physics-informed neural networks and the finite element method have mainly been studied in isolation of each other. In this work, we compare the methodologies in a systematic computational study. Indeed, we employ both methods to numerically solve various linear and nonlinear partial differential equations: Poisson in 1D, 2D, and 3D, Allen-Cahn in 1D, semilinear Schr{\textbackslash}"odinger in 1D and 2D. We then compare computational costs and approximation accuracies. In terms of solution time and accuracy, physics-informed neural networks have not been able to outperform the finite element method in our study. In some experiments, they were faster at evaluating the solved PDE.},
  archiveprefix = {arxiv},
  keywords = {differential-equation,finite-element-method,physics-informed-model},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Grossmann et al_2023_Can Physics-Informed Neural Networks beat the Finite Element Method.pdf;/Users/steven/Zotero/storage/AXQ4EVTS/2302.html}
}

@phdthesis{gunningStatisticalModellingSecondgeneration2023,
  type = {Thesis},
  title = {Statistical Modelling of Second-Generation Functional Data with Application in Biomechanics and Human Movement Research},
  author = {Gunning, Edward},
  year = {2023},
  month = dec,
  doi = {10.34961/researchrepository-ul.24948066.v1},
  urldate = {2024-01-28},
  abstract = {In recent years, the data being collected in human movement biomechanics have increased in size and complexity. Improvements in data acquisition and processing mean that more data can be recorded and extracted for each individual, producing datasets of increased volume that are multivariate in nature and exhibit complex dependence structures due to repeated measurements.Functional data analysis is a branch of statistics that is well suited to the analysis of continuous biomechanical data, e.g., kinematics or kinetics that are recorded throughout a movement, because it treats the measured data streams as smooth, time-varying functions. Traditional functional data analysis tools have been developed for independent, univariate samples of functional data, so they are not flexible enough to fully leverage the richness of modern biomechanical datasets. There is a growing need to adapt and extend these tools to modern data structures and characteristics. To address this challenge, this thesis develops new statistical approaches for structured functional data, as motivated by a large study on running-related injuries, where kinematic data from recreational runners were captured during a treadmill run with the goal of understanding running technique and its links to injury. The motivating biomechanical dataset is richin size and structure as it contains repeated measurements of multiple kinematic variables from a large number of individuals. Thus, it presents methodological and computational challenges and facilitates the development of novel statistical methodology that is generally applicable in a variety of fields.The first challenge addressed in this thesis is the development of multivariate functional mixed effects modelling techniques, to allow the kinematic data from multiple body parts (e.g., hip and knee) to be analysed simultaneously. The mixed effects modelling framework readily incorporates covariate information and facilitates appropriate modelling of the complex dependence structure between observations from each individual (i.e., each individual has multiple repetitions of the movement/strides, measurements are made on both sides of the body). This work is divided into two sequential contributions. Firstly, a multivariate functional mixed effects model is developed for a subset of the motivating dataset, which contains average hip angle and knee angle curves for each individual on either side of the body. As the second contribution, the modelling framework is extended to handle the full collection of strides in the motivating dataset, resulting in a multivariate multilevel longitudinal functional model which uses a longitudinal functional data analysis approach to capture temporal dependence among the adjacent strides.The second challenge addressed in this thesis is the estimation of continuous?time ordinary differential equations from functional data. A re-formulation of Principal Differential Analysis, a classical technique for estimating differential equations from functional data, is developed. The new formulation is based on a generative statistical model, providing a better understanding of the technique from a statistical perspective and broadening its applicability. The methodology is demonstrated on simulated data from ordinary differential equation models and on a subset of kinematic data from the motivating dataset for this thesis.},
  langid = {english},
  school = {University of Limerick},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/University of Limerick/2023/Gunning_2023_Statistical modelling of second-generation functional data with application in.pdf}
}

@article{habermanAnalysisResidualsCrossClassified1973,
  title = {The {{Analysis}} of {{Residuals}} in {{Cross-Classified Tables}}},
  author = {Haberman, Shelby J.},
  year = {1973},
  journal = {Biometrics},
  volume = {29},
  number = {1},
  eprint = {2529686},
  eprinttype = {jstor},
  pages = {205--220},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2529686},
  urldate = {2023-10-24},
  abstract = {Techniques are proposed for analysis of residuals associated with log-linear models for frequency tables. Results are applied to two-way tables and logit models.},
  keywords = {residuals-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/1973/Haberman_1973_The Analysis of Residuals in Cross-Classified Tables.pdf}
}

@misc{haghbinRegularizedMultivariateFunctional2023,
  title = {Regularized {{Multivariate Functional Principal Component Analysis}}},
  author = {Haghbin, Hossein and Zhao, Yue and Maadooliat, Mehdi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.13980},
  eprint = {2306.13980},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.13980},
  urldate = {2023-10-25},
  abstract = {Multivariate Functional Principal Component Analysis (MFPCA) is a valuable tool for exploring relationships and identifying shared patterns of variation in multivariate functional data. However, controlling the roughness of the extracted Principal Components (PCs) can be challenging. This paper introduces a novel approach called regularized MFPCA (ReMFPCA) to address this issue and enhance the smoothness and interpretability of the multivariate functional PCs. ReMFPCA incorporates a roughness penalty within a penalized framework, using a parameter vector to regulate the smoothness of each functional variable. The proposed method generates smoothed multivariate functional PCs, providing a concise and interpretable representation of the data. Extensive simulations and real data examples demonstrate the effectiveness of ReMFPCA and its superiority over alternative methods. The proposed approach opens new avenues for analyzing and uncovering relationships in complex multivariate functional datasets.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,functional-principal-components,hilbert-space-theory,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Haghbin et al_2023_Regularized Multivariate Functional Principal Component Analysis.pdf;/Users/steven/Zotero/storage/IWHDHN73/2306.html}
}

@article{hallAsymptoticallyOptimalDifferenceBased1990,
  title = {Asymptotically {{Optimal Difference-Based Estimation}} of {{Variance}} in {{Nonparametric Regression}}},
  author = {Hall, Peter and Kay, J. W. and Titterington, D. M.},
  year = {1990},
  journal = {Biometrika},
  volume = {77},
  number = {3},
  eprint = {2336990},
  eprinttype = {jstor},
  pages = {521--528},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2336990},
  urldate = {2023-11-15},
  abstract = {We define and compute asymptotically optimal difference sequences for estimating error variance in homoscedastic nonparametric regression. Our optimal difference sequences do not depend on unknowns, such as the mean function, and provide substantial improvements over the suboptimal sequences commonly used in practice. For example, in the case of normal data the usual variance estimator based on symmetric second-order differences is only 64\% efficient relative to the estimator based on optimal second-order differences. The efficiency of an optimal mth-order difference estimator relative to the error sample variance is 2m/(2m + 1). Again this is for normal data, and increases as the tails of the error distribution become heavier.},
  keywords = {noise-reduction,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrika/1990/Hall et al_1990_Asymptotically Optimal Difference-Based Estimation of Variance in Nonparametric.pdf}
}

@misc{happ-kurzMFPCAMultivariateFunctional2022,
  title = {{{MFPCA}}: {{Multivariate Functional Principal Component Analysis}} for {{Data Observed}} on {{Different Dimensional Domains}}},
  shorttitle = {{{MFPCA}}},
  author = {{Happ-Kurz}, Clara},
  year = {2022},
  month = sep,
  urldate = {2024-01-10},
  abstract = {Calculate a multivariate functional principal component analysis for data observed on different dimensional domains. The estimation algorithm relies on univariate basis expansions for each element of the multivariate functional data (Happ \& Greven, 2018) {$<$}doi:10.1080/01621459.2016.1273115{$>$}. Multivariate and univariate functional data objects are represented by S4 classes for this type of data implemented in the package 'funData'. For more details on the general concepts of both packages and a case study, see Happ-Kurz (2020) {$<$}doi:10.18637/jss.v093.i05{$>$}.},
  copyright = {GPL-2},
  keywords = {FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2022/Happ-Kurz_2022_MFPCA.pdf}
}

@article{happ-kurzObjectOrientedSoftwareFunctional2020,
  title = {Object-{{Oriented Software}} for {{Functional Data}}},
  author = {{Happ-Kurz}, Clara},
  year = {2020},
  month = apr,
  journal = {Journal of Statistical Software},
  volume = {93},
  pages = {1--38},
  issn = {1548-7660},
  doi = {10.18637/jss.v093.i05},
  urldate = {2023-07-31},
  abstract = {This paper introduces the funData R package as an object-oriented implementation of functional data. It implements a unified framework for dense univariate and multivariate functional data on one- and higher dimensional domains as well as for irregular functional data. The aim of this package is to provide a user-friendly, self-contained core toolbox for functional data, including important functionalities for creating, accessing and modifying functional data objects, that can serve as a basis for other packages. The package further contains a full simulation toolbox, which is a useful feature when implementing and testing new methodological developments. Based on the theory of object-oriented data analysis, it is shown why it is natural to implement functional data in an object-oriented manner. The classes and methods provided by funData are illustrated in many examples using two freely available datasets. The MFPCA package, which implements multivariate functional principal component analysis, is presented as an example for an advanced methodological package that uses the funData package as a basis, including a case study with real data. Both packages are publicly available on GitHub and the Comprehensive R Archive Network.},
  copyright = {Copyright (c) 2020 Clara Happ-Kurz},
  langid = {english},
  keywords = {functional-data-analysis,functional-principal-components,multivariate-functional-data,r-software,software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Statistical Software/2020/Happ-Kurz_2020_Object-Oriented Software for Functional Data.pdf}
}

@article{happMultivariateFunctionalPrincipal2018,
  title = {Multivariate {{Functional Principal Component Analysis}} for {{Data Observed}} on {{Different}} ({{Dimensional}}) {{Domains}}},
  author = {Happ, Clara and Greven, Sonja},
  year = {2018},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {113},
  number = {522},
  pages = {649--659},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1273115},
  urldate = {2023-07-08},
  abstract = {Existing approaches for multivariate functional principal component analysis are restricted to data on the same one-dimensional interval. The presented approach focuses on multivariate functional data on different domains that may differ in dimension, such as functions and images. The theoretical basis for multivariate functional principal component analysis is given in terms of a Karhunen{\textendash}Lo{\`e}ve Theorem. For the practically relevant case of a finite Karhunen{\textendash}Lo{\`e}ve representation, a relationship between univariate and multivariate functional principal component analysis is established. This offers an estimation strategy to calculate multivariate functional principal components and scores based on their univariate counterparts. For the resulting estimators, asymptotic results are derived. The approach can be extended to finite univariate expansions in general, not necessarily orthonormal bases. It is also applicable for sparse functional data or data with measurement error. A flexible R implementation is available on CRAN. The new method is shown to be competitive to existing approaches for data observed on a common one-dimensional domain. The motivating application is a neuroimaging study, where the goal is to explore how longitudinal trajectories of a neuropsychological test score covary with FDG-PET brain scans at baseline. Supplementary material, including detailed proofs, additional simulation results, and software is available online.},
  keywords = {dimension-reduction,functional-data-analysis,image-analysis,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2018/Happ and Greven - 2018 - Multivariate Functional Principal Component Analys_supp.pdf;/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2018/Happ and Greven - 2018 - Multivariate Functional Principal Component Analys.pdf}
}

@book{hardleAppliedMultivariateStatistical2019,
  title = {Applied {{Multivariate Statistical Analysis}}},
  author = {H{\"a}rdle, Wolfgang Karl and Simar, L{\'e}opold},
  year = {2019},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-26006-4},
  urldate = {2024-01-18},
  isbn = {978-3-030-26005-7 978-3-030-26006-4},
  langid = {english},
  keywords = {Applications in Finance,Big Data Analysis,Cluster Analysis,Clustering,Computationally Intensive Techniques,Conjoint Measurement Analysis,Dimension Reduction Techniques,Discriminant Analysis,Hypothesis Testing,Lasso and Elastic Net,Multivariate Classification,Multivariate Data Analysis,Projection Persuit,quantitative finance,Sliced Inverse Regression,Variable Selection},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer International Publishing/2019/Härdle_Simar_2019_Applied Multivariate Statistical Analysis2.pdf}
}

@book{hastieElementsStatisticalLearning2009,
  title = {The {{Elements}} of {{Statistical Learning}}},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  year = {2009},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-84858-7},
  urldate = {2023-07-07},
  isbn = {978-0-387-84857-0 978-0-387-84858-7},
  keywords = {statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2009/Hastie et al_2009_The Elements of Statistical Learning_outline.pdf;/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2009/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf}
}

@article{hillStatisticsMultipleParticle1996,
  title = {Statistics of Multiple Particle Breakage},
  author = {Hill, Priscilla J. and Ng, Ka M.},
  year = {1996},
  journal = {AIChE Journal},
  volume = {42},
  number = {6},
  pages = {1600--1611},
  issn = {1547-5905},
  doi = {10.1002/aic.690420611},
  urldate = {2023-07-08},
  abstract = {A method for generating theoretical breakage distribution functions for multiple particle breakage is presented. It starts with the joint probability function that accounts for all the child particles; it is then reduced to the marginal probability function commonly used in the breakage equation. This method is flexible enough to allow the user to choose the number of child particles and the functional form to be used. The method is demonstrated with both product and summation functions with a power-law form. To facilitate the use of these theoretical functions for statistical analyses, a companion discretized breakage equation is developed. The new equation guarantees the conservation of mass and correct prediction of the total number of particles despite discretization. It is easy to use because it is a set of ordinary differential equations and applicable to both equal-size and geometric-size intervals. Simulation results show that different breakage distribution functions coupled with different breakage rates can produce almost indistinguishable particle-size distributions, signifying the need for further work in this area.},
  copyright = {Copyright {\copyright} 1996 American Institute of Chemical Engineers},
  langid = {english},
  keywords = {breakage-equation,particle-size-distribution,statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/AIChE Journal/1996/Hill and Ng - 1996 - Statistics of multiple particle breakage.pdf;/Users/steven/Zotero/storage/6H7B9A22/aic.html}
}

@article{hormannDynamicFunctionalPrincipal2015,
  title = {Dynamic {{Functional Principal Components}}},
  author = {H{\"o}rmann, Siegfried and Kidzi{\'n}ski, {\L}ukasz and Hallin, Marc},
  year = {2015},
  month = mar,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {77},
  number = {2},
  pages = {319--348},
  issn = {1369-7412},
  doi = {10.1111/rssb.12076},
  urldate = {2023-09-19},
  abstract = {We address the problem of dimension reduction for time series of functional data (Xt:t{$\in$}Z). Such functional time series frequently arise, for example, when a continuous time process is segmented into some smaller natural units, such as days. Then each X ~t represents one intraday curve. We argue that functional principal component analysis, though a key technique in the field and a benchmark for any competitor, does not provide an adequate dimension reduction in a time series setting. Functional principal component analysis indeed is a static procedure which ignores the essential information that is provided by the serial dependence structure of the functional data under study. Therefore, inspired by Brillinger's theory of dynamic principal components, we propose a dynamic version of functional principal component analysis which is based on a frequency domain approach. By means of a simulation study and an empirical illustration, we show the considerable improvement that the dynamic approach entails when compared with the usual static procedure.},
  keywords = {dimension-reduction,functional-principal-components,functional-time-series,spectral-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society Series B Statistical Methodology/2015/Hörmann et al_2015_Dynamic Functional Principal Components.pdf;/Users/steven/Zotero/storage/UTNKMA5X/7040632.html}
}

@article{horvathEstimationMeanFunctional2013,
  title = {Estimation of the {{Mean}} of {{Functional Time Series}} and a {{Two-Sample Problem}}},
  author = {Horv{\'a}th, Lajos and Kokoszka, Piotr and Reeder, Ron},
  year = {2013},
  month = jan,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {75},
  number = {1},
  pages = {103--122},
  issn = {1369-7412},
  doi = {10.1111/j.1467-9868.2012.01032.x},
  urldate = {2023-09-19},
  abstract = {The paper is concerned with inference based on the mean function of a functional time series. We develop a normal approximation for the functional sample mean and then focus on the estimation of the asymptotic variance kernel. Using these results, we develop and asymptotically justify testing procedures for the equality of means in two functional samples exhibiting temporal dependence. Evaluated by means of a simulation study and application to a real data set, these two-sample procedures enjoy good size and power in finite samples.},
  keywords = {functional-time-series,mean-function-estimation,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society Series B Statistical Methodology/2013/Horváth et al_2013_Estimation of the Mean of Functional Time Series and a Two-Sample Problem.pdf;/Users/steven/Zotero/storage/2N9V3I73/7075406.html}
}

@book{horvathInferenceFunctionalData2012a,
  title = {Inference for {{Functional Data}} with {{Applications}}},
  author = {Horv{\'a}th, Lajos and Kokoszka, Piotr},
  year = {2012},
  series = {Springer {{Series}} in {{Statistics}}},
  volume = {200},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-3655-3},
  urldate = {2023-09-19},
  isbn = {978-1-4614-3654-6 978-1-4614-3655-3},
  langid = {english},
  keywords = {functional-data-analysis,functional-time-series,hilbert-space-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2012/Horváth_Kokoszka_2012_Inference for Functional Data with Applications.pdf}
}

@article{horvathTestingIndependenceFunctional2015,
  title = {Testing for Independence between Functional Time Series},
  author = {Horv{\'a}th, Lajos and Rice, Gregory},
  year = {2015},
  month = dec,
  journal = {Journal of Econometrics},
  series = {Frontiers in {{Time Series}} and {{Financial Econometrics}}},
  volume = {189},
  number = {2},
  pages = {371--382},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2015.03.030},
  urldate = {2023-09-19},
  abstract = {Frequently econometricians are interested in verifying a relationship between two or more time series. Such analysis is typically carried out by causality and/or independence tests which have been well studied when the data is univariate or multivariate. Modern data though is increasingly of a high dimensional or functional nature for which finite dimensional methods are not suitable. In the present paper we develop methodology to check the assumption that data obtained from two functional time series are independent. Our procedure is based on the norms of empirical cross covariance operators and is asymptotically validated when the underlying populations are assumed to be in a class of weakly dependent random functions which include the functional ARMA, ARCH and GARCH processes.},
  keywords = {central-limit-theorem,functional-time-series,independence-testing,weak-dependence},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Econometrics/2015/Horváth_Rice_2015_Testing for independence between functional time series.pdf;/Users/steven/Zotero/storage/V9M6YVC4/S0304407615001086.html}
}

@article{horvathTestingStationarityFunctional2014,
  title = {Testing Stationarity of Functional Time Series},
  author = {Horv{\'a}th, Lajos and Kokoszka, Piotr and Rice, Gregory},
  year = {2014},
  month = mar,
  journal = {Journal of Econometrics},
  volume = {179},
  number = {1},
  pages = {66--82},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2013.11.002},
  urldate = {2023-09-19},
  abstract = {Economic and financial data often take the form of a collection of curves observed consecutively over time. Examples include, intraday price curves, yield and term structure curves, and intraday volatility curves. Such curves can be viewed as a time series of functions. A fundamental issue that must be addressed, before an attempt is made to statistically model such data, is whether these curves, perhaps suitably transformed, form a stationary functional time series. This paper formalizes the assumption of stationarity in the context of functional time series and proposes several procedures to test the null hypothesis of stationarity. The tests are nontrivial extensions of the broadly used tests in the KPSS family. The properties of the tests under several alternatives, including change-point and I(1), are studied, and new insights, present only in the functional setting are uncovered. The theory is illustrated by a small simulation study and an application to intraday price curves.},
  keywords = {functional-data-analysis,functional-time-series,stationarity-analysis,time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Econometrics/2014/Horváth et al_2014_Testing stationarity of functional time series.pdf;/Users/steven/Zotero/storage/VKWF5MER/S0304407613002327.html}
}

@book{huberPerfectSimulation2016,
  title = {Perfect {{Simulation}}},
  author = {Huber, Mark L.},
  year = {2016},
  month = feb,
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/b19235},
  abstract = {Exact sampling, specifically coupling from the past (CFTP), allows users to sample exactly from the stationary distribution of a Markov chain. During its nearly 20 years of existence, exact sampling has evolved into perfect simulation, which enables high-dimensional simulation from interacting distributions.Perfect Simulation illustrates the applic},
  isbn = {978-0-429-16526-9},
  keywords = {simulation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2016/Huber_2016_Perfect Simulation.pdf}
}

@article{huFunctionalRegressionDependent2023,
  title = {Functional Regression with Dependent Error and Missing Observation in Reproducing Kernel {{Hilbert}} Spaces},
  author = {Hu, Yan-Ping and Liang, Han-Ying},
  year = {2023},
  month = sep,
  journal = {Journal of the Korean Statistical Society},
  volume = {52},
  number = {3},
  pages = {736--764},
  issn = {2005-2863},
  doi = {10.1007/s42952-023-00219-2},
  urldate = {2023-10-25},
  abstract = {In this paper, we focus on the partial functional linear model with missing observation, which allows the responses or part of the covariates or responses and part of the covariates simultaneously missing at random, where the regression error is a linear process deduced by not necessarily independent random variables. Under the reproducing kernel Hilbert space setting, we construct the estimators of the slope parameter and coefficient function in the model based on the inverse probability weighting methods, and establish their asymptotic normality and weak convergence with rates, respectively. Meanwhile, the penalized estimator of the parameter is defined by the SCAD penalty and its oracle property is investigated. In addition, we construct a test statistic to check a linear hypothesis of the nonzero parameters and discuss its asymptotic distribution. Simulation study and real data analysis are conducted to investigate the finite sample performance of the proposed methods.},
  langid = {english},
  keywords = {functional-data-analysis,functional-linear-model,reproducing-kernel-hilbert-space,variable-selection},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Korean Statistical Society/2023/Hu_Liang_2023_Functional regression with dependent error and missing observation in.pdf}
}

@misc{husseyDataNotAvailable2023,
  title = {Data Is Not Available upon Request},
  author = {Hussey, Ian},
  year = {2023},
  month = may,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/jbu9r},
  urldate = {2023-10-26},
  abstract = {Many journals now require data sharing and require articles to include a Data Availability Statement. However, several studies over the past two decades have shown that promissory notes about data sharing are rarely abided by and that data is generally not available upon request. This has negative consequences for many essential aspects of scientific knowledge production, including independent verification of results, efficient secondary use of data, and knowledge synthesis. I assessed the prevalence of data sharing upon request in articles employing the Implicit Relational Assessment Procedure published within the last 5 years. Of 52 articles, 42\% contained a Data Availability Statement, most of which stated that data was available upon request. This rose from 0\% in 2018 to 100\% in 2022, indicating a change in journals' policies. However, only 27\% of articles' authors actually shared data. Among articles stating that data was available upon request, only 17\% shared data upon request. The presence of Data Availability Statements was not associated with higher rates of data sharing (p = .55), indicating a lack of adherence with journals' policies. Results replicate those found elsewhere: data is generally not available upon request, and promissory Data Availability Statements are typically not adhered to. Issues, causes, and implications are considered.},
  langid = {american},
  keywords = {meta-science,research},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/PsyArXiv/2023/Hussey_2023_Data is not available upon request.pdf}
}

@article{hyndmanForecastingFunctionalTime2009,
  title = {Forecasting Functional Time Series},
  author = {Hyndman, Rob J. and Shang, Han Lin},
  year = {2009},
  month = sep,
  journal = {Journal of the Korean Statistical Society},
  volume = {38},
  number = {3},
  pages = {199--211},
  issn = {2005-2863},
  doi = {10.1016/j.jkss.2009.06.002},
  urldate = {2023-09-19},
  abstract = {We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions, assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006, and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts. We also propose two new bootstrap methods to construct prediction intervals, and evaluate and compare their empirical coverage probabilities.},
  langid = {english},
  keywords = {forecasting,functional-data-analysis,functional-partial-least-squares,functional-principal-components,functional-time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Korean Statistical Society/2009/Hyndman_Shang_2009_Forecasting functional time series.pdf}
}

@article{ievaCovariancebasedClusteringMultivariate2016,
  title = {Covariance-Based {{Clustering}} in {{Multivariate}} and {{Functional Data Analysis}}},
  author = {Ieva, Francesca and Paganoni, Anna Maria and Tarabelloni, Nicholas},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {143},
  pages = {1--21},
  issn = {1533-7928},
  urldate = {2023-11-06},
  abstract = {In this paper we propose a new algorithm to perform clustering of multivariate and functional data. We study the case of two populations different in their covariances, rather than in their means. The algorithm relies on a proper quantification of distance between the estimated covariance operators of the populations, and subdivides data in two groups maximising the distance between their induced covariances. The naive implementation of such an algorithm is computationally forbidding, so we propose a heuristic formulation with a much lighter complexity and we study its convergence properties, along with its computational cost. We also propose to use an enhanced estimator for the estimation of discrete covariances of functional data, namely a linear shrinkage estimator, in order to improve the precision of the clustering. We establish the effectiveness of our algorithm through applications to both synthetic data and a real data set coming from a biomedical context, showing also how the use of shrinkage estimation may lead to substantially better results.},
  keywords = {clustering,covariance-operator-estimation,functional-data-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Machine Learning Research/2016/Ieva et al_2016_Covariance-based Clustering in Multivariate and Functional Data Analysis.pdf}
}

@article{ievaMultivariateFunctionalClustering2013,
  title = {Multivariate Functional Clustering for the Morphological Analysis of Electrocardiograph Curves},
  author = {Ieva, Francesca and Paganoni, Anna M. and Pigoli, Davide and Vitelli, Valeria},
  year = {2013},
  journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {62},
  number = {3},
  eprint = {24771812},
  eprinttype = {jstor},
  pages = {401--418},
  publisher = {{[Wiley, Royal Statistical Society]}},
  issn = {0035-9254},
  urldate = {2023-11-21},
  abstract = {Cardiovascular ischaemic diseases are one of the main causes of death all over the world. In this class of pathologies, a quick diagnosis is essential for a good prognosis in reperfusive treatment. In particular, an automatic classification procedure based on statistical analysis of teletransmitted electrocardiograph ('ECG') traces would be very helpful for an early diagnosis. This work presents an analysis of ECG traces, either physiological or pathological, of patients whose 12-lead prehospital ECG has been sent to the 118 Dispatch Center in Milan by life-support personnel. The statistical analysis starts with a preprocessing step, where functional data are reconstructed from noisy observations and biological variability is removed by a non-linear registration procedure. Then, a multivariate functional k-means clustering procedure is carried out on reconstructed and registered ECGs and their first derivatives. Hence, a new semi-automatic diagnostic procedure, based solely on the ECG morphology, is proposed to classify ECG traces; finally, the performance of this classification method is evaluated.},
  keywords = {clustering,electrocardiogram,functional-data-analysis,multivariate-functional-data,registration},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society. Series C (Applied Statistics)/2013/Ieva et al_2013_Multivariate functional clustering for the morphological analysis of.pdf}
}

@article{ioannidisThousandsScientistsPublish2018,
  title = {Thousands of Scientists Publish a Paper Every Five Days},
  author = {Ioannidis, John P. A. and Klavans, Richard and Boyack, Kevin W.},
  year = {2018},
  month = sep,
  journal = {Nature},
  volume = {561},
  number = {7722},
  pages = {167--169},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-018-06185-8},
  urldate = {2023-10-24},
  abstract = {To highlight uncertain norms in authorship, John P. A. Ioannidis, Richard Klavans and Kevin W. Boyack identified the most prolific scientists of recent years.},
  copyright = {2021 Nature},
  langid = {english},
  keywords = {publishing,research},
  annotation = {Bandiera\_abtest: a Cg\_type: Comment Subject\_term: Authorship, Publishing},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Nature/2018/Ioannidis et al_2018_Thousands of scientists publish a paper every five days.pdf;/Users/steven/Zotero/storage/IVWETEBS/d41586-018-06185-8.html}
}

@article{jacquesModelbasedClusteringMultivariate2014,
  title = {Model-Based Clustering for Multivariate Functional Data},
  author = {Jacques, Julien and Preda, Cristian},
  year = {2014},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  volume = {71},
  pages = {92--106},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2012.12.004},
  urldate = {2023-07-08},
  abstract = {The first model-based clustering algorithm for multivariate functional data is proposed. After introducing multivariate functional principal components analysis (MFPCA), a parametric mixture model, based on the assumption of normality of the principal component scores, is defined and estimated by an EM-like algorithm. The main advantage of the proposed model is its ability to take into account the dependence among curves. Results on simulated and real datasets show the efficiency of the proposed method.},
  langid = {english},
  keywords = {functional-principal-components,model-based-clustering,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2014/Jacques_Preda_2014_Model-based clustering for multivariate functional data.pdf;/Users/steven/Zotero/storage/8GDK5ZI4/S0167947312004380.html}
}

@misc{jaffardReviewUnivariateMultivariate2022,
  title = {A Review of Univariate and Multivariate Multifractal Analysis Illustrated by the Analysis of Marathon Runners Physiological Data},
  author = {Jaffard, St{\'e}phane and Sa{\"e}s, Guillaume and Nasr, Wejdene Ben and Palacin, Florent and Billat, V{\'e}ronique},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14612},
  eprint = {2209.14612},
  primaryclass = {eess, math, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14612},
  urldate = {2023-10-26},
  abstract = {We review the central results concerning wavelet methods in multifractal analysis, which consists in analysis of the pointwise singularities of a signal, and we describe its recent extension to multivariate multifractal analysis, which deals with the joint analysis of several signals; we focus on the mathematical questions that this new techniques motivate. We illustrate these methods by an application to data recorded on marathon runners.},
  archiveprefix = {arxiv},
  keywords = {physiological-data,smoothness,sport-science,wavelet-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2022/Jaffard et al_2022_A review of univariate and multivariate multifractal analysis illustrated by.pdf;/Users/steven/Zotero/storage/ZD7PCBIZ/2209.html}
}

@article{juTreebasedBoostingFunctional2023,
  title = {Tree-Based Boosting with Functional Data},
  author = {Ju, Xiaomeng and {Salibi{\'a}n-Barrera}, Mat{\'i}as},
  year = {2023},
  month = may,
  journal = {Computational Statistics},
  issn = {1613-9658},
  doi = {10.1007/s00180-023-01364-2},
  urldate = {2023-10-24},
  abstract = {In this article we propose a boosting algorithm for regression with functional explanatory variables and scalar responses. The algorithm uses decision trees constructed with multiple projections as the ``base-learners'', which we call ``functional multi-index trees''. We establish identifiability conditions for these trees and introduce two algorithms to compute them. We use numerical experiments to investigate the performance of our method and compare it with several linear and nonlinear regression estimators, including recently proposed nonparametric and semiparametric functional additive estimators. Simulation studies show that the proposed method is consistently among the top performers, whereas the performance of existing alternatives can vary substantially across different settings. In a real example, we apply our method to predict electricity demand using price curves and show that our estimator provides better predictions compared to its competitors, especially when one adjusts for seasonality.},
  langid = {english},
  keywords = {boosting-algorithm,functional-data-analysis,functional-regression},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics/2023/Ju_Salibián-Barrera_2023_Tree-based boosting with functional data.pdf}
}

@book{kallianpurStochasticFilteringTheory1980,
  title = {Stochastic {{Filtering Theory}}},
  author = {Kallianpur, Gopinath},
  editor = {Balakrishnan, A. V.},
  year = {1980},
  series = {Stochastic {{Modelling}} and {{Applied Probability}}},
  volume = {13},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4757-6592-2},
  urldate = {2023-10-31},
  isbn = {978-1-4419-2810-8 978-1-4757-6592-2},
  keywords = {probability-theory,stochastic-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/1980/Kallianpur_1980_Stochastic Filtering Theory2.pdf}
}

@book{karhunenUeberLineareMethoden1947,
  title = {{{\"U}ber lineare Methoden in der Wahrscheinlichkeitsrechnung}},
  author = {Karhunen, Kari},
  year = {1947},
  series = {{Suomalaisen Tiedeakatemian toimituksia}},
  number = {ARRAY(0x55c9032fd4e0)},
  publisher = {{Zugl.: Helsinki, Univ., Diss., 1947}},
  address = {{Helsinki}},
  langid = {german},
  keywords = {hilbert-space-theory}
}

@misc{kassiLearningRegularityMultivariate2023,
  title = {Learning the Regularity of Multivariate Functional Data},
  author = {Kassi, Omar and Klutchnikoff, Nicolas and Patilea, Valentin},
  year = {2023},
  month = oct,
  number = {arXiv:2307.14163},
  eprint = {2307.14163},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.14163},
  urldate = {2023-10-27},
  abstract = {Combining information both within and between sample realizations, we propose a simple estimator for the local regularity of surfaces in the functional data framework. The independently generated surfaces are measured with errors at possibly random discrete times. Non-asymptotic exponential bounds for the concentration of the regularity estimators are derived. An indicator for anisotropy is proposed and an exponential bound of its risk is derived. Two applications are proposed. We first consider the class of multi-fractional, bi-dimensional, Brownian sheets with domain deformation, and study the nonparametric estimation of the deformation. As a second application, we build minimax optimal, bivariate kernel estimators for the reconstruction of the surfaces.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,minimax-theory,multidimensional-functional-data,smoothness},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Kassi et al_2023_Learning the regularity of multivariate functional data.pdf;/Users/steven/Zotero/storage/AJVLJX9Y/2307.html}
}

@article{kastnerBayesianParameterEstimation2013,
  title = {Bayesian Parameter Estimation for a Jet-Milling Model Using {{Metropolis}}{\textendash}{{Hastings}} and {{Wang}}{\textendash}{{Landau}} Sampling},
  author = {Kastner, Catharine A. and Braumann, Andreas and Man, Peter L. W. and Mosbach, Sebastian and Brownbridge, George P. E. and Akroyd, Jethro and Kraft, Markus and Himawan, Chrismono},
  year = {2013},
  month = feb,
  journal = {Chemical Engineering Science},
  volume = {89},
  pages = {244--257},
  issn = {0009-2509},
  doi = {10.1016/j.ces.2012.11.027},
  urldate = {2023-07-08},
  abstract = {Bayesian parameter estimates for a computationally expensive multi-response jet-milling model are computed using the Metropolis{\textendash}Hastings and Wang{\textendash}Landau Markov Chain Monte Carlo sampling algorithms. The model is accompanied by data obtained from 74 experiments at different process settings which is used to estimate the model parameters. The experimentally measured quantities are the 10th, 50th and 90th quantiles of the resulting particle size distributions. Parameter estimation is performed on a population balance jet-milling model composed of three subprocesses: jet expansion, milling and classification. The model contains eight parameters requiring estimation and can compute the same quantities that are determined in the experiments. As the model is computationally expensive to solve, the sampling algorithms are applied to a surrogate model to establish algorithm specific parameters and to obtain model parameter estimates. The resulting parameter estimates are given with a discussion of their reliability and the observed behaviour of the two sampling algorithms. Comparison of the autocorrelation function between samples generated by the two algorithms shows that the Wang{\textendash}Landau algorithm exhibits more rapid decay. Trace plots of the parameter samples from the two algorithms appear to be analogous and encourage the supposition that the Markov Chains have converged to the distribution of interest. One- and two-dimensional density plots indicate a unimodal distribution for all parameters, which suggests that the obtained estimates are unique. The two-dimensional density plots also suggest correlation between at least two of the model parameters. The realised distribution generated by both algorithms produced consistent results and demonstrated similar behaviour. For the application considered in this work, the Wang{\textendash}Landau algorithm is found to exhibit superior performance with respect to the correlation and equivalent performance in all other respects.},
  langid = {english},
  keywords = {bayesian-analysis,metropolis-hastings-algorithm,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chemical Engineering Science/2013/Kastner et al. - 2013 - Bayesian parameter estimation for a jet-milling mo.pdf;/Users/steven/Zotero/storage/HHMPLUFA/S000925091200677X.html}
}

@article{kimFunctionalClusteringSphere2023,
  title = {Functional Clustering on a Sphere via {{Riemannian}} Functional Principal Components},
  author = {Kim, Hyunsung and Lim, Yaeji},
  year = {2023},
  journal = {Stat},
  volume = {12},
  number = {1},
  pages = {e557},
  issn = {2049-1573},
  doi = {10.1002/sta4.557},
  urldate = {2023-10-25},
  abstract = {We propose the functional clustering algorithm applicable to the sphere-valued random curves, called k-centres Riemannian functional clustering (kCRFC). It is based on Riemannian functional principal component scores and k-centres functional clustering algorithm; thus, we can obtain accurate clustering results by reflecting the geometry of the sphere. Our method shows better clustering performances than existing multivariate functional clustering methods in various simulation settings. We apply the proposed method to the migration trajectories of Egyptian Vultures in the Middle East and East Africa and fruit fly behaviours, containing the curves lied on two-dimensional and three-dimensional sphere, respectively.},
  copyright = {{\copyright} 2023 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {clustering,functional-data-analysis,functional-principal-components,sphere-valued-functional-data},
  file = {/Users/steven/Zotero/storage/T38R85WA/Kim and Lim - 2023 - Functional clustering on a sphere via Riemannian f.pdf;/Users/steven/Zotero/storage/FIIJLMUG/sta4.html}
}

@article{kleinDistributionalRegressionData2024,
  title = {Distributional {{Regression}} for {{Data Analysis}}},
  author = {Klein, Nadja},
  year = {2024},
  journal = {Annual Review of Statistics and Its Application},
  volume = {11},
  number = {1},
  pages = {null},
  doi = {10.1146/annurev-statistics-040722-053607},
  urldate = {2023-10-27},
  abstract = {Flexible modeling of how an entire distribution changes with covariates is an important yet challenging generalization of mean-based regression that has seen growing interest over the past decades in both the statistics and machine learning literature. This review outlines selected state-of-the-art statistical approaches to distributional regression, complemented with alternatives from machine learning. Topics covered include the similarities and differences between these approaches, extensions, properties and limitations, estimation procedures, and the availability of software. In view of the increasing complexity and availability of large-scale data, this review also discusses the scalability of traditional estimation methods, current trends, and open challenges. Illustrations are provided using data on childhood malnutrition in Nigeria and Australian electricity prices. Expected final online publication date for the Annual Review of Statistics and Its Application, Volume 11 is March 2024. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
  keywords = {distributional-regression,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annual Review of Statistics and Its Application/2024/Klein_2024_Distributional Regression for Data Analysis.pdf}
}

@article{kneafseyAssessingInfluenceNeutral2018,
  title = {Assessing the Influence of Neutral Grounds on Match Outcomes},
  author = {Kneafsey, Liam and M{\"u}ller, Stefan},
  year = {2018},
  month = nov,
  journal = {International Journal of Performance Analysis in Sport},
  volume = {18},
  number = {6},
  pages = {892--905},
  publisher = {{Routledge}},
  issn = {2474-8668},
  doi = {10.1080/24748668.2018.1525678},
  urldate = {2023-10-24},
  abstract = {The home advantage in various sports has been well documented. So far, we lack knowledge whether playing in neutral venues indeed removes many, if not all, theoretically assumed advantages of playing at home. Analysing over 3,500 senior men's Gaelic football and hurling matches {\textendash} field games with the highest participation rates in Ireland {\textendash} between 2009 and 2018, we test the potential moderating influence of neutral venues. In hurling and Gaelic football, a considerable share of matches is played at neutral venues. We test the influence of neutral venues based on descriptive statistics, and multilevel logistic and multinomial regressions controlling for team strength, the importance of the match, the year, and the sport. With predicted probabilities ranging between 0.8 and 0.9, the favourite team is very likely to win home matches. The predicted probability drops below 0.6 for away matches. At neutral venues, the favourite team has a predicted probability of winning of 0.7. A Coarsened Exact Matching (CEM) approach also reveals very substantive and significant effects for the ``treatment'' of neutral venues. Overall, neutral venues appear to be an under-utilised option for creating fairer and less predictable competition, especially in single-game knock-out matches.},
  keywords = {gaelic-football,home-advantage,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/International Journal of Performance Analysis in Sport/2018/Kneafsey_Müller_2018_Assessing the influence of neutral grounds on match outcomes.pdf}
}

@article{kneipInferenceDensityFamilies2001,
  title = {Inference for {{Density Families Using Functional Principal Component Analysis}}},
  author = {Kneip, Alois and Utikal, Klaus J.},
  year = {2001},
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {454},
  eprint = {2670290},
  eprinttype = {jstor},
  pages = {519--532},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  urldate = {2023-10-02},
  abstract = {We consider t = 1, ..., T samples of iid observations \{X1t,..., Xnt t\} from unknown population densities \{ft\}. To characterize differences and similarities of \{ft\}, we assume their expansions into the first L principal components. From the given observations \{Xit\}, we study inference on the components and on their required number L. A detailed asymptotic theory is presented. Our method is applied in the analysis of yearly cross-sectional samples of British households. Interpretation of the estimated principal components and their scores provides new insights into the evolution and interplay of household income and age distributions from 1968-1988. From estimating their required numbers L, we draw conclusions on the dimensionality of mixture models for describing the densities.},
  keywords = {density-estimation,forecasting,functional-principal-components,kernel-smoothing,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2001/Kneip_Utikal_2001_Inference for Density Families Using Functional Principal Component Analysis.pdf}
}

@article{kokoszkaAsymptoticNormalityPrincipal2013,
  title = {Asymptotic Normality of the Principal Components of Functional Time Series},
  author = {Kokoszka, Piotr and Reimherr, Matthew},
  year = {2013},
  month = may,
  journal = {Stochastic Processes and their Applications},
  volume = {123},
  number = {5},
  pages = {1546--1562},
  issn = {0304-4149},
  doi = {10.1016/j.spa.2012.12.011},
  urldate = {2023-09-19},
  abstract = {We establish the asymptotic normality of the sample principal components of functional stochastic processes under nonrestrictive assumptions which admit nonlinear functional time series models. We show that the aforementioned asymptotic depends only on the asymptotic normality of the sample covariance operator, and that the latter condition holds for weakly dependent functional time series which admit expansions as Bernoulli shifts. The weak dependence is quantified by the condition of L4-m-approximability which includes all functional time series models in practical use. We also demonstrate convergence of the cross covariance operators of the sample functional principal components to their counterparts in the normal limit.},
  keywords = {asymptotic-theory,functional-principal-components,functional-time-series,weak-dependence},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Stochastic Processes and their Applications/2013/Kokoszka_Reimherr_2013_Asymptotic normality of the principal components of functional time series.pdf;/Users/steven/Zotero/storage/2FDR28DA/S0304414912002724.html}
}

@book{kokoszkaIntroductionFunctionalData2017,
  title = {Introduction to {{Functional Data Analysis}}},
  author = {Kokoszka, Piotr and Reimherr, Matthew},
  year = {2017},
  month = sep,
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/9781315117416},
  abstract = {Introduction to Functional Data Analysis provides a concise textbook introduction to the field. It explains how to analyze functional data, both at exploratory and inferential levels. It also provides a systematic and accessible exposition of the methodology and the required mathematical framework. The book can be used as textbook for a semester-long course on FDA for advanced undergraduate or MS statistics majors, as well as for MS and PhD students in other disciplines, including applied mathematics, environmental science, public health, medical research, geophysical sciences and economics. It can also be used for self-study and as a reference for researchers in those fields who wish to acquire solid understanding of FDA methodology and practical guidance for its implementation. Each chapter contains plentiful examples of relevant R code and theoretical and data analytic problems. The material of the book can be roughly divided into four parts of approximately equal length: 1) basic concepts and techniques of FDA, 2) functional regression models, 3) sparse and dependent functional data, and 4) introduction to the Hilbert space framework of FDA. The book assumes advanced undergraduate background in calculus, linear algebra, distributional probability theory, foundations of statistical inference, and some familiarity with R programming. Other required statistics background is provided in scalar settings before the related functional concepts are developed. Most chapters end with references to more advanced research for those who wish to gain a more in-depth understanding of a specific topic.},
  isbn = {978-1-315-11741-6},
  keywords = {functional-data-analysis,functional-regression,functional-time-series,hilbert-space-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2017/Kokoszka_Reimherr_2017_Introduction to Functional Data Analysis.pdf}
}

@article{komarekClusteringMultivariateContinuous2013,
  title = {Clustering for Multivariate Continuous and Discrete Longitudinal Data},
  author = {Kom{\'a}rek, Arno{\v s}t and Kom{\'a}rkov{\'a}, Lenka},
  year = {2013},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {7},
  number = {1},
  pages = {177--200},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/12-AOAS580},
  urldate = {2023-11-21},
  abstract = {Multiple outcomes, both continuous and discrete, are routinely gathered on subjects in longitudinal studies and during routine clinical follow-up in general. To motivate our work, we consider a longitudinal study on patients with primary biliary cirrhosis (PBC) with a continuous bilirubin level, a discrete platelet count and a dichotomous indication of blood vessel malformations as examples of such longitudinal outcomes. An apparent requirement is to use all the outcome values to classify the subjects into groups (e.g., groups of subjects with a similar prognosis in a clinical setting). In recent years, numerous approaches have been suggested for classification based on longitudinal (or otherwise correlated) outcomes, targeting not only traditional areas like biostatistics, but also rapidly evolving bioinformatics and many others. However, most available approaches consider only continuous outcomes as a basis for classification, or if noncontinuous outcomes are considered, then not in combination with other outcomes of a different nature. Here, we propose a statistical method for clustering (classification) of subjects into a prespecified number of groups with a priori unknown characteristics on the basis of repeated measurements of several longitudinal outcomes of a different nature. This method relies on a multivariate extension of the classical generalized linear mixed model where a mixture distribution is additionally assumed for random effects. We base the inference on a Bayesian specification of the model and simulation-based Markov chain Monte Carlo methodology. To apply the method in practice, we have prepared ready-to-use software for use in R (http://www.R-project.org). We also discuss evaluation of uncertainty in the classification and also discuss usage of a recently proposed methodology for model comparison{\textemdash}the selection of a number of clusters in our case{\textemdash}based on the penalized posterior deviance proposed by Plummer [Biostatistics 9 (2008) 523{\textendash}539].},
  keywords = {bayesian-analysis,clustering,functional-data-analysis,longitudinal-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Applied Statistics/2013/Komárek_Komárková_2013_Clustering for multivariate continuous and discrete longitudinal data.pdf}
}

@misc{konerPowerSampleSize2023,
  title = {Power and Sample Size Calculation of Two-Sample Projection-Based Testing for Sparsely Observed Functional Data},
  author = {Koner, Salil and Luo, Sheng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06252},
  eprint = {2310.06252},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06252},
  urldate = {2023-10-27},
  abstract = {Projection-based testing for mean trajectory differences in two groups of irregularly and sparsely observed functional data has garnered significant attention in the literature because it accommodates a wide spectrum of group differences and (non-stationary) covariance structures. This article presents the derivation of the theoretical power function and the introduction of a comprehensive power and sample size (PASS) calculation toolkit tailored to the projection-based testing method developed by Wang (2021). Our approach accommodates a wide spectrum of group difference scenarios and a broad class of covariance structures governing the underlying processes. Through extensive numerical simulation, we demonstrate the robustness of this testing method by showcasing that its statistical power remains nearly unaffected even when a certain percentage of observations are missing, rendering it 'missing-immune'. Furthermore, we illustrate the practical utility of this test through analysis of two randomized controlled trials of Parkinson's disease. To facilitate implementation, we provide a user-friendly R package fPASS, complete with a detailed vignette to guide users through its practical application. We anticipate that this article will significantly enhance the usability of this potent statistical tool across a range of biostatistical applications, with a particular focus on its relevance in the design of clinical trials.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,sparse-functional-data,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Koner_Luo_2023_Power and sample size calculation of two-sample projection-based testing for.pdf;/Users/steven/Zotero/storage/GKUDFRDA/2310.html}
}

@article{konerSecondGenerationFunctionalData2023,
  title = {Second-{{Generation Functional Data}}},
  author = {Koner, Salil and Staicu, Ana-Maria},
  year = {2023},
  journal = {Annual Review of Statistics and Its Application},
  volume = {10},
  number = {1},
  pages = {547--572},
  doi = {10.1146/annurev-statistics-032921-033726},
  urldate = {2023-09-20},
  abstract = {Modern studies from a variety of fields record multiple functional observations according to either multivariate, longitudinal, spatial, or time series designs. We refer to such data as second-generation functional data because their analysis{\textemdash}unlike typical functional data analysis, which assumes independence of the functions{\textemdash}accounts for the complex dependence between the functional observations and requires more advanced methods. In this article, we provide an overview of the techniques for analyzing second-generation functional data with a focus on highlighting the key methodological intricacies that stem from the need for modeling complex dependence, compared with independent functional data. For each of the four types of second-generation functional data presented{\textemdash}multivariate functional data, longitudinal functional data, functional time series and spatially functional data{\textemdash}we discuss how the widely popular functional principal component analysis can be extended to these settings to define, identify main directions of variation, and describe dependence among the functions. In addition to modeling, we also discuss prediction, statistical inference, and application to clustering. We close by discussing future directions in this area.},
  keywords = {functional-principal-components,functional-time-series,longitudinal-functional-data,multivariate-functional-data,spatial-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annual Review of Statistics and Its Application/2023/Koner_Staicu_2023_Second-Generation Functional Data.pdf}
}

@article{kovalchikPlayerTrackingData2023,
  title = {Player {{Tracking Data}} in {{Sports}}},
  author = {Kovalchik, Stephanie A.},
  year = {2023},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {10},
  number = {1},
  pages = {677--697},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-033021-110117},
  urldate = {2023-10-24},
  abstract = {There has been rapid growth in the collection of player tracking data in recent years. These data, providing spatiotemporal locations of players and ball at high resolution, have spurred methodological developments in a range of sports. There have been impacts in the development of player performance measurement (e.g., distance traveled) and in the attribution of value to specific plays (e.g., expected points from a given position) or even specific actions within a play. This review highlights key methodological contributions via statistical and machine learning approaches. The studies and outcomes discussed show how sports can be a playground for extending analytical techniques in a range of areas. The review also describes the ongoing methodological challenges associated with the use of tracking data.},
  langid = {english},
  keywords = {machine-learning,spatiotemporal-data,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annual Review of Statistics and Its Application/2023/Kovalchik_2023_Player Tracking Data in Sports.pdf}
}

@phdthesis{lacroixDecompositionsTensoriellesPour2020,
  type = {These de Doctorat},
  title = {D{\'e}compositions Tensorielles Pour La Compl{\'e}tion de Bases de Connaissance},
  author = {Lacroix, Timoth{\'e}e},
  year = {2020},
  month = jul,
  urldate = {2023-10-25},
  abstract = {Dans cette th{\`e}se, nous abordons le probl{\`e}me de pr{\'e}diction de liens dans des tenseurs binaires d'ordre trois et quatre contenant des observations positives uniquement. Ce type de tenseur appara{\^i}t dans les probl{\`e}mes de recommandations sur le web, en bio-informatique pour compl{\'e}ter des bases d'interactions entre prot{\'e}ines, ou plus g{\'e}n{\'e}ralement pour la compl{\'e}tion bases de connaissances. Ces derni{\`e}res nous permettent d'{\'e}valuer nos m{\'e}thodes de compl{\'e}tion {\`a} grande {\'e}chelle et sur des types de graphes relationnels vari{\'e}s.Notre approche est parall{\`e}le {\`a} celle de la compl{\'e}tion de matrice. Nous r{\'e}solvons de mani{\`e}re non-convexe un probl{\`e}me de minimisation empirique r{\'e}gularis{\'e} sur des tenseurs de faible rangs. Dans un premier temps, nous validons empiriquement notre approche en obtenant des performances sup{\'e}rieures {\`a} l'{\'e}tat de l'art sur de nombreux jeux de donn{\'e}es.Ces performances ne peuvent {\^e}tre atteintes que pour des rangs trop {\'e}lev{\'e}s pour que cette m{\'e}thode soit applicable {\`a} l'{\'e}chelle de bases de connaissances compl{\`e}tes. Nous nous int{\'e}ressons dans un second temps {\`a} la d{\'e}composition Tucker, plus expressive que la d{\'e}composition Canonique, mais plus difficile {\`a} optimiser. En corrigeant l'algorithme adaptatif Adagrad, nous arrivons {\`a} optimiser efficacement des d{\'e}compositions Tucker dont le c{\oe}ur est al{\'e}atoire et fix{\'e}. Ces m{\'e}thodes nous permettent d'am{\'e}liorer les performances en compl{\'e}tion pour une quantit{\'e} faible de param{\`e}tres par entit{\'e}s.Finalement, nous {\'e}tudions le cas de base de connaissances temporelles, dans lesquels les pr{\'e}dicats ne sont valides que sur certains intervalles de temps. Nous proposons une formulation faible rang et une r{\'e}gularisation adapt{\'e}e {\`a} la structure du probl{\`e}me, qui nous permet d'obtenir des performances sup{\'e}rieures {\`a} l'{\'e}tat de l'art.},
  collaborator = {Marlet, Renaud},
  copyright = {Licence Etalab},
  school = {Paris Est},
  keywords = {machine-learning,tensor-decomposition},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Paris Est/2020/Lacroix_2020_Décompositions tensorielles pour la complétion de bases de connaissance.pdf}
}

@article{ledoitWellconditionedEstimatorLargedimensional2004,
  title = {A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices},
  author = {Ledoit, Olivier and Wolf, Michael},
  year = {2004},
  month = feb,
  journal = {Journal of Multivariate Analysis},
  volume = {88},
  number = {2},
  pages = {365--411},
  issn = {0047-259X},
  doi = {10.1016/S0047-259X(03)00096-4},
  urldate = {2023-11-07},
  abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For large-dimensional covariance matrices, the usual estimator{\textemdash}the sample covariance matrix{\textemdash}is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to infinity together. Extensive Monte Carlo confirm that the asymptotic results tend to hold well in finite sample.},
  keywords = {covariance-operator-estimation,shrinkage},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Multivariate Analysis/2004/Ledoit_Wolf_2004_A well-conditioned estimator for large-dimensional covariance matrices.pdf;/Users/steven/Zotero/storage/DP6D83TC/S0047259X03000964.html}
}

@article{liddellAnalyzingOrdinalData2018,
  title = {Analyzing Ordinal Data with Metric Models: {{What}} Could Possibly Go Wrong?},
  shorttitle = {Analyzing Ordinal Data with Metric Models},
  author = {Liddell, Torrin M. and Kruschke, John K.},
  year = {2018},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  volume = {79},
  pages = {328--348},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2018.08.009},
  urldate = {2023-10-24},
  abstract = {We surveyed all articles in the Journal of Personality and Social Psychology (JPSP), Psychological Science (PS), and the Journal of Experimental Psychology: General (JEP:G) that mentioned the term ``Likert,'' and found that 100\% of the articles that analyzed ordinal data did so using a metric model. We present novel evidence that analyzing ordinal data as if they were metric can systematically lead to errors. We demonstrate false alarms (i.e., detecting an effect where none exists, Type I errors) and failures to detect effects (i.e., loss of power, Type II errors). We demonstrate systematic inversions of effects, for which treating ordinal data as metric indicates the opposite ordering of means than the true ordering of means. We show the same problems {\textemdash} false alarms, misses, and inversions {\textemdash} for interactions in factorial designs and for trend analyses in regression. We demonstrate that averaging across multiple ordinal measurements does not solve or even ameliorate these problems. A central contribution is a graphical explanation of how and when the misrepresentations occur. Moreover, we point out that there is no sure-fire way to detect these problems by treating the ordinal values as metric, and instead we advocate use of ordered-probit models (or similar) because they will better describe the data. Finally, although frequentist approaches to some ordered-probit models are available, we use Bayesian methods because of their flexibility in specifying models and their richness and accuracy in providing parameter estimates. An R script is provided for running an analysis that compares ordered-probit and metric models.},
  keywords = {bayesian-analysis,ordinal-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Experimental Social Psychology/2018/Liddell_Kruschke_2018_Analyzing ordinal data with metric models.pdf;/Users/steven/Zotero/storage/J2PZ9EBC/S0022103117307746.html}
}

@article{lieblInferenceSparseDense2019,
  title = {Inference for Sparse and Dense Functional Data with Covariate Adjustments},
  author = {Liebl, Dominik},
  year = {2019},
  month = mar,
  journal = {Journal of Multivariate Analysis},
  series = {Special {{Issue}} on {{Functional Data Analysis}} and {{Related Topics}}},
  volume = {170},
  pages = {315--335},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2018.04.006},
  urldate = {2023-10-24},
  abstract = {We consider inference for the mean and covariance functions of covariate adjusted functional data using Local Linear Kernel (LLK) estimators. By means of a double asymptotic, we differentiate between sparse and dense covariate adjusted functional data {\textemdash}~depending on the relative order of m (the discretization points per function) and n (the number of functions). Our simulation results demonstrate that the existing asymptotic normality results can lead to severely misleading inferences in finite samples. We explain this phenomenon based on our theoretical results and propose finite-sample corrections which provide practically useful approximations for inference in sparse and dense data scenarios. The relevance of our theoretical results is showcased using a real-data application.},
  keywords = {asymptotic-theory,functional-data-analysis,local-polynomial-smoothing},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Multivariate Analysis/2019/Liebl_2019_Inference for sparse and dense functional data with covariate adjustments2.pdf;/Users/steven/Zotero/storage/DIZ5QYZ5/S0047259X18301957.html}
}

@article{lilaSmoothPrincipalComponent2016,
  title = {Smooth {{Principal Component Analysis}} over Two-Dimensional Manifolds with an Application to Neuroimaging},
  author = {Lila, Eardi and Aston, John A. D. and Sangalli, Laura M.},
  year = {2016},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {10},
  number = {4},
  pages = {1854--1879},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/16-AOAS975},
  urldate = {2023-10-27},
  abstract = {Motivated by the analysis of high-dimensional neuroimaging signals located over the cortical surface, we introduce a novel Principal Component Analysis technique that can handle functional data located over a two-dimensional manifold. For this purpose a regularization approach is adopted, introducing a smoothing penalty coherent with the geodesic distance over the manifold. The model introduced can be applied to any manifold topology, and can naturally handle missing data and functional samples evaluated in different grids of points. We approach the discretization task by means of finite element analysis, and propose an efficient iterative algorithm for its resolution. We compare the performances of the proposed algorithm with other approaches classically adopted in literature. We finally apply the proposed method to resting state functional magnetic resonance imaging data from the Human Connectome Project, where the method shows substantial differential variations between brain regions that were not apparent with other approaches.},
  keywords = {differential-equation,functional-data-analysis,functional-magnetic-resonance-imaging,functional-principal-components},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Applied Statistics/2016/Lila et al_2016_Smooth Principal Component Analysis over two-dimensional manifolds with an.pdf}
}

@article{liuEstimatingDerivativesSamples2009,
  title = {Estimating {{Derivatives}} for {{Samples}} of {{Sparsely Observed Functions}}, {{With Application}} to {{Online Auction Dynamics}}},
  author = {Liu, Bitao and M{\"u}ller, Hans-Georg},
  year = {2009},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {104},
  number = {486},
  pages = {704--717},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/jasa.2009.0115},
  urldate = {2023-11-27},
  abstract = {It is often of interest to recover derivatives of a sample of random functions from sparse and noise-contaminated measurements, especially when the dynamics of underlying processes is of interest. We propose a novel approach based on estimating derivatives of eigenfunctions and expansions of random functions into their eigenfunctions to obtain a representation for derivatives. In combination with estimates for functional principal component scores for sparse data, this leads to a viable solution of the challenging problem to recover derivatives for sparsely observed functions. We establish consistency results and demonstrate in simulations that the method is superior to alternative approaches (derivative estimation with random effects models based on B-spline bases, kernel smoothing, smoothing splines, or P-splines). Our study is motivated by an analysis of bidding histories for eBay auctions, for which bids are typically very sparse in the middle and somewhat more frequent near the beginning and end of an auction. We demonstrate the estimation of derivatives of price curves for individual auctions from the sparsely observed bidding histories and also derive a model-free first-order differential equation that applies in the case of Gaussian processes. This provides a data-driven dynamic model that we use to elucidate auction dynamics.},
  keywords = {derivatives,functional-data-analysis,functional-principal-components,smoothness,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2009/Liu_Müller_2009_Estimating Derivatives for Samples of Sparsely Observed Functions, With.pdf}
}

@book{loeveFonctionsAleatoiresSecond1965,
  title = {{Fonctions al{\'e}atoires du second ordre}},
  author = {Lo{\`e}ve, Michel},
  year = {1965},
  googlebooks = {Was5XwAACAAJ},
  langid = {french},
  keywords = {hilbert-space-theory}
}

@article{longEstimatingReactionParameters2022,
  title = {Estimating Reaction Parameters in Mechanism-Enabled Population Balance Models of Nanoparticle Size Distributions: {{A Bayesian}} Inverse Problem Approach},
  shorttitle = {Estimating Reaction Parameters in Mechanism-Enabled Population Balance Models of Nanoparticle Size Distributions},
  author = {Long, Danny K. and Bangerth, Wolfgang and Handwerk, Derek R. and Whitehead, Christopher B. and Shipman, Patrick D. and Finke, Richard G.},
  year = {2022},
  journal = {Journal of Computational Chemistry},
  volume = {43},
  number = {1},
  pages = {43--56},
  issn = {1096-987X},
  doi = {10.1002/jcc.26770},
  urldate = {2023-07-08},
  abstract = {In order to quantitatively predict nano- as well as other particle-size distributions, one needs to have both a mathematical model and estimates of the parameters that appear in these models. Here, we show how one can use Bayesian inversion to obtain statistical estimates for the parameters that appear in recently derived mechanism-enabled population balance models (ME-PBM) of nanoparticle growth. The Bayesian approach addresses the question of ``how well do we know our parameters, along with their uncertainties?.'' The results reveal that Bayesian inversion statistical analysis on an example, prototype nanoparticle formation system allows one to estimate not just the most likely rate constants and other parameter values, but also their SDs, confidence intervals, and other statistical information. Moreover, knowing the reliability of the mechanistic model's parameters in turn helps inform one about the reliability of the proposed mechanism, as well as the reliability of its predictions. The paper can also be seen as a tutorial with the additional goal of achieving a ``Gold Standard'' Bayesian inversion ME-PBM benchmark that others can use as a control to check their own use of this methodology for other systems of interest throughout nature. Overall, the results provide strong support for the hypothesis that there is substantial value in using a Bayesian inversion methodology for parameter estimation in particle formation systems.},
  copyright = {{\copyright} 2021 Wiley Periodicals LLC.},
  langid = {english},
  keywords = {bayesian-analysis,growth,nucleation,particle-size-distribution,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Computational Chemistry/2022/Long et al. - 2022 - Estimating reaction parameters in mechanism-enable.pdf}
}

@misc{loningSktimeUnifiedInterfaceb,
  title = {Sktime: {{A Unified Interface}} for {{Machine Learning}} with {{Time Series}}},
  author = {L{\"o}ning, Markus and Bagnall, Anthony and Ganesh, Sajaysurya and Kazakov, Viktor},
  abstract = {We present sktime {\textendash} a new scikit-learn compatible Python library with a unified interface for machine learning with time series. Time series data gives rise to various distinct but closely related learning tasks, such as forecasting and time series classification, many of which can be solved by reducing them to related simpler tasks. We discuss the main rationale for creating a unified interface, including reduction, as well as the design of sktime's core API, supported by a clear overview of common time series tasks and reduction approaches.},
  file = {/Users/steven/Zotero/storage/PS5DIM6V/Löning et al. - sktime A Uniﬁed Interface for Machine Learning wi.pdf}
}

@article{lopezHowOftenDoes2018,
  title = {How Often Does the Best Team Win? {{A}} Unified Approach to Understanding Randomness in {{North American}} Sport},
  shorttitle = {How Often Does the Best Team Win?},
  author = {Lopez, Michael J. and Matthews, Gregory J. and Baumer, Benjamin S.},
  year = {2018},
  month = dec,
  journal = {The Annals of Applied Statistics},
  volume = {12},
  number = {4},
  pages = {2483--2516},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/18-AOAS1165},
  urldate = {2023-10-26},
  abstract = {Statistical applications in sports have long centered on how to best separate signal (e.g., team talent) from random noise. However, most of this work has concentrated on a single sport, and the development of meaningful cross-sport comparisons has been impeded by the difficulty of translating luck from one sport to another. In this manuscript we develop Bayesian state-space models using betting market data that can be uniformly applied across sporting organizations to better understand the role of randomness in game outcomes. These models can be used to extract estimates of team strength, the between-season, within-season and game-to-game variability of team strengths, as well each team's home advantage. We implement our approach across a decade of play in each of the National Football League (NFL), National Hockey League (NHL), National Basketball Association (NBA) and Major League Baseball (MLB), finding that the NBA demonstrates both the largest dispersion in talent and the largest home advantage, while the NHL and MLB stand out for their relative randomness in game outcomes. We conclude by proposing new metrics for judging competitiveness across sports leagues, both within the regular season and using traditional postseason tournament formats. Although we focus on sports, we discuss a number of other situations in which our generalizable models might be usefully applied.},
  keywords = {bayesian-analysis,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Applied Statistics/2018/Lopez et al_2018_How often does the best team win.pdf}
}

@book{lototskyStochasticPartialDifferential2017,
  title = {Stochastic {{Partial Differential Equations}}},
  author = {Lototsky, Sergey V. and Rozovsky, Boris L.},
  year = {2017},
  series = {Universitext},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-58647-2},
  urldate = {2023-10-25},
  isbn = {978-3-319-58645-8 978-3-319-58647-2},
  keywords = {differential-equation,stochastic-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer International Publishing/2017/Lototsky_Rozovsky_2017_Stochastic Partial Differential Equations2.pdf}
}

@article{luzlopezgarciaKmeansAlgorithmsFunctional2015,
  title = {K-Means Algorithms for Functional Data},
  author = {Luz L{\'o}pez Garc{\'i}a, Mar{\'i}a and {Garc{\'i}a-R{\'o}denas}, Ricardo and Gonz{\'a}lez G{\'o}mez, Antonia},
  year = {2015},
  month = mar,
  journal = {Neurocomputing},
  volume = {151},
  pages = {231--245},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2014.09.048},
  urldate = {2023-10-27},
  abstract = {Cluster analysis of functional data considers that the objects on which you want to perform a taxonomy are functions f:X{$\subset$}Rp{$\mapsto$}R and the available information about each object is a sample in a finite set of points fn=\{(xi,yi){$\in$}X{\texttimes}R\}i=1n. The aim is to infer the meaningful groups by working explicitly with its infinite-dimensional nature. In this paper the use of K-means algorithms to solve this problem is analysed. A comparative study of three K-means algorithms has been conducted. The K-means algorithm for raw data, a kernel K-means algorithm for raw data and a K-means algorithm using two distances for functional data are tested. These distances, called dVn and d{$\phi$}, are based on projections onto Reproducing Kernel Hilbert Spaces (RKHS) and Tikhonov regularization theory. Although it is shown that both distances are equivalent, they lead to two different strategies to reduce the dimensionality of the data. In the case of dVn distance the most suitable strategy is Johnson{\textendash}Lindenstrauss random projections. The dimensionality reduction for d{$\phi$} is based on spectral methods. A key aspect that has been analysed is the effect of the sampling \{xi\}i=1n on the K-means algorithm performance. In the numerical study an ex professo example is given to show that if the sampling is not uniform in X, then a K-means algorithm that ignores the functional nature of the data can reduce its performance. It is numerically shown that the original K-means algorithm and that suggested here lead to similar performance in the examples when X is uniformly sampled, but the computational cost when working with the original set of observations is higher than the K-means algorithms based on d{$\phi$} or dVn, as they use strategies to reduce the dimensionality of the data. The numerical tests are completed with a case study to analyse what kind of problem the K-means algorithm for functional data must face.},
  keywords = {clustering,dimension-reduction,functional-data-analysis,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Neurocomputing/2015/Luz López García et al_2015_K-means algorithms for functional data.pdf;/Users/steven/Zotero/storage/652VHEA6/S0925231214012521.html}
}

@article{maddoxBayesianEstimationIngame2022,
  title = {Bayesian Estimation of In-Game Home Team Win Probability for College Basketball},
  author = {Maddox, Jason T. and Sides, Ryan and Harvill, Jane L.},
  year = {2022},
  month = sep,
  journal = {Journal of Quantitative Analysis in Sports},
  volume = {18},
  number = {3},
  pages = {201--213},
  publisher = {{De Gruyter}},
  issn = {1559-0410},
  doi = {10.1515/jqas-2021-0086},
  urldate = {2023-10-24},
  abstract = {Two new Bayesian methods for estimating and predicting in-game home team win probabilities in Division I NCAA men's college basketball are proposed. The first method has a prior that adjusts as a function of lead differential and time elapsed. The second is an adjusted version of the first, where the adjustment is a linear combination of the Bayesian estimator with a time-weighted pregame win probability. The proposed methods are compared to existing methods, showing the new methods are competitive with or outperform existing methods for both estimation and prediction. The utility is illustrated via an application to the 2012/2013 through the 2019/2020 NCAA Division I Men's Basketball seasons.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {basketball,bayesian-analysis,home-advantage,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Quantitative Analysis in Sports/2022/Maddox et al_2022_Bayesian estimation of in-game home team win probability for college basketball.pdf}
}

@article{maFunctionalMixedModel2021,
  title = {A Functional Mixed Model for Scalar on Function Regression with Application to a Functional {{MRI}} Study},
  author = {Ma, Wanying and Xiao, Luo and Liu, Bowen and Lindquist, Martin A},
  year = {2021},
  month = jul,
  journal = {Biostatistics},
  volume = {22},
  number = {3},
  pages = {439--454},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxz046},
  urldate = {2023-09-19},
  abstract = {Motivated by a functional magnetic resonance imaging (fMRI) study, we propose a new functional mixed model for scalar on function regression. The model extends the standard scalar on function regression for repeated outcomes by incorporating subject-specific random functional effects. Using functional principal component analysis, the new model can be reformulated as a mixed effects model and thus easily fit. A test is also proposed to assess the existence of the subject-specific random functional effects. We evaluate the performance of the model and test via a simulation study, as well as on data from the motivating fMRI study of thermal pain. The data application indicates significant subject-specific effects of the human brain hemodynamics related to pain and provides insights on how the effects might differ across subjects.},
  keywords = {functional-data-analysis,functional-magnetic-resonance-imaging,functional-mixed-model,functional-principal-components,functional-regression},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biostatistics/2021/Ma et al_2021_A functional mixed model for scalar on function regression with application to.pdf;/Users/steven/Zotero/storage/94NC2TYF/5601315.html}
}

@article{mancoFaultDetectionExplanation2017,
  title = {Fault Detection and Explanation through Big Data Analysis on Sensor Streams},
  author = {Manco, Giuseppe and Ritacco, Ettore and Rullo, Pasquale and Gallucci, Lorenzo and Astill, Will and Kimber, Dianne and Antonelli, Marco},
  year = {2017},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {87},
  pages = {141--156},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2017.05.079},
  urldate = {2023-10-02},
  abstract = {Fault prediction is an important topic for the industry as, by providing effective methods for predictive maintenance, allows companies to perform important time and cost savings. In this paper we describe an application developed to predict and explain door failures on metro trains. To this end, the aim was twofold: first, devising prediction techniques capable of early detecting door failures from diagnostic data; second, describing failures in terms of properties distinguishing them from normal behavior. Data pre-processing was a complex task aimed at overcoming a number of issues with the dataset, like size, sparsity, bias, burst effect and trust. Since failure premonitory signals did not share common patterns, but were only characterized as non-normal device signals, fault prediction was performed by using outlier detection. Fault explanation was finally achieved by exhibiting device features showing abnormal values. An experimental evaluation was performed to assess the quality of the proposed approach. Results show that high-degree outliers are effective indicators of incipient failures. Also, explanation in terms of abnormal feature values (responsible for outlierness) seems to be quite expressive.There are some aspects in the proposed approach that deserve particular attention. We introduce a general framework for the failure detection problem based on an abstract model of diagnostic data, along with a formal problem statement. They both provide the basis for the definition of an effective data pre-processing technique where the behavior of a device, in a given time frame, is summarized through a number of suitable statistics. This approach strongly mitigates the issues related to data errors/noise, thus enabling to perform an effective outlier detection. All this, in our view, provides the grounds of a general methodology for advanced prognostic systems.},
  keywords = {anomaly-detection,outliers-detection,sensor-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Expert Systems with Applications/2017/Manco et al_2017_Fault detection and explanation through big data analysis on sensor streams.pdf;/Users/steven/Zotero/storage/ASENML5N/S0957417417304074.html}
}

@article{martinezrodriguezParticleAgglomerationFlows2022,
  title = {Particle Agglomeration in Flows: {{Fast}} Data-Driven Spatial Decomposition Algorithm for {{CFD}} Simulations},
  shorttitle = {Particle Agglomeration in Flows},
  author = {Mart{\'i}nez Rodr{\'i}guez, Kerlyns and Bossy, Mireille and Henry, Christophe},
  year = {2022},
  month = apr,
  journal = {International Journal of Multiphase Flow},
  volume = {149},
  pages = {103962},
  issn = {0301-9322},
  doi = {10.1016/j.ijmultiphaseflow.2021.103962},
  urldate = {2023-07-08},
  abstract = {Computational fluid dynamics simulations in practical industrial/environmental cases often involve non-homogeneous concentrations of particles. In Euler{\textendash}Lagrange simulations, this can induce the propagation of numerical error when the number of collision/agglomeration events is computed using mean-field approaches. In fact, mean-field statistical collision models allow to sample the number of collision events using a priori information on the frequency of collisions (the collision kernel). Yet, since such methods often rely on the mesh used for the Eulerian simulation of the fluid phase, the particle number concentration within a given cell might not be homogeneous, leading to numerical errors. In this article, we apply the data-driven spatial decomposition (D2SD) algorithm to control such error in simulations of particle agglomeration. Significant improvements are made to design a fast D2SD version, minimising the additional computational cost by developing re-meshing criteria. Through the application to some practical simulation cases, we show the importance of splitting the domain when computing agglomeration events in Euler/Lagrange simulations, so that within each elementary cell there is a spatially uniform distribution of particles.},
  langid = {english},
  keywords = {agglomeration,computational-fluid-dynamics,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/International Journal of Multiphase Flow/2022/Martínez Rodríguez et al_2022_Particle agglomeration in flows.pdf;/Users/steven/Zotero/storage/LTFAYIT3/S0301932221003578.html}
}

@article{martinoKmeansProcedureBased2019,
  title = {A K-Means Procedure Based on a {{Mahalanobis}} Type Distance for Clustering Multivariate Functional Data},
  author = {Martino, Andrea and Ghiglietti, Andrea and Ieva, Francesca and Paganoni, Anna Maria},
  year = {2019},
  month = jun,
  journal = {Statistical Methods \& Applications},
  volume = {28},
  number = {2},
  pages = {301--322},
  issn = {1613-981X},
  doi = {10.1007/s10260-018-00446-6},
  urldate = {2023-11-21},
  abstract = {This paper proposes a clustering procedure for samples of multivariate functions in \$\$(L\^2(I))\^\{J\}\$\$, with \$\$J{\textbackslash}ge 1\$\$. This method is based on a k-means algorithm in which the distance between the curves is measured with a metric that generalizes the Mahalanobis distance in Hilbert spaces, considering the correlation and the variability along all the components of the functional data. The proposed procedure has been studied in simulation and compared with the k-means based on other distances typically adopted for clustering multivariate functional data. In these simulations, it is shown that the k-means algorithm with the generalized Mahalanobis distance provides the best clustering performances, both in terms of mean and standard deviation of the number of misclassified curves. Finally, the proposed method has been applied to two case studies, concerning ECG signals and growth curves, where the results obtained in simulation are confirmed and strengthened.},
  langid = {english},
  keywords = {clustering,electrocardiogram,functional-data-analysis,mahalanobis-distance,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistical Methods & Applications/2019/Martino et al_2019_A k-means procedure based on a Mahalanobis type distance for clustering.pdf}
}

@article{masakSeparableExpansionsCovariance2023,
  title = {Separable Expansions for Covariance Estimation via the Partial Inner Product},
  author = {Masak, T and Sarkar, S and Panaretos, V M},
  year = {2023},
  month = mar,
  journal = {Biometrika},
  volume = {110},
  number = {1},
  pages = {225--247},
  issn = {1464-3510},
  doi = {10.1093/biomet/asac035},
  urldate = {2023-10-27},
  abstract = {The nonparametric estimation of covariance lies at the heart of functional data analysis, whether for curve or surface-valued data. The case of a two-dimensional domain poses both statistical and computational challenges, which are typically alleviated by assuming separability. However, separability is often questionable, sometimes even demonstrably inadequate. We propose a framework for the analysis of covariance operators of random surfaces that generalizes separability while retaining its major advantages. Our approach is based on the expansion of the covariance into a series of separable terms. The expansion is valid for any covariance over a two-dimensional domain. Leveraging the key notion of the partial inner product, we generalize the power iteration method to general Hilbert spaces, and show how the aforementioned expansion can be efficiently constructed in practice at the level of the surface observations. Truncation of the expansion and retention of the leading terms automatically induces a nonparametric estimator of the covariance, whose parsimony is dictated by the truncation level. The resulting estimator can be calculated, stored and manipulated with little computational overhead relative to separability. Consistency and rates of convergence are derived under mild regularity assumptions, illustrating the trade-off between bias and variance regulated by the truncation level. The merits and practical performance of the proposed methodology are demonstrated in a comprehensive simulation study.},
  keywords = {covariance-operator-estimation,functional-data-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrika/2023/Masak et al_2023_Separable expansions for covariance estimation via the partial inner product.pdf;/Users/steven/Zotero/storage/FA26ISIY/6609757.html}
}

@book{mcelreathStatisticalRethinkingBayesian2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{STAN}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/9780429029608},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work. The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding. The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses. Features Integrates working code into the main text Illustrates concepts through worked data analysis examples Emphasizes understanding assumptions and how assumptions are reflected in code Offers more detailed explanations of the mathematics in optional sections Presents examples of using the dagitty R package to analyze causal graphs Provides the rethinking R package on the author's website and on GitHub},
  isbn = {978-0-429-02960-8},
  keywords = {bayesian-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2020/McElreath - 2020 - Statistical Rethinking A Bayesian Course with Exa.pdf}
}

@misc{mcgowanCausalInferenceNot2023,
  title = {Causal Inference Is Not Just a Statistics Problem},
  author = {McGowan, Lucy D'Agostino and Gerke, Travis and Barrett, Malcolm},
  year = {2023},
  month = jul,
  number = {arXiv:2304.02683},
  eprint = {2304.02683},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.02683},
  urldate = {2023-10-27},
  abstract = {This paper introduces a collection of four data sets, similar to Anscombe's Quartet, that aim to highlight the challenges involved when estimating causal effects. Each of the four data sets is generated based on a distinct causal mechanism: the first involves a collider, the second involves a confounder, the third involves a mediator, and the fourth involves the induction of M-Bias by an included factor. The paper includes a mathematical summary of each data set, as well as directed acyclic graphs that depict the relationships between the variables. Despite the fact that the statistical summaries and visualizations for each data set are identical, the true causal effect differs, and estimating it correctly requires knowledge of the data-generating mechanism. These example data sets can help practitioners gain a better understanding of the assumptions underlying causal inference methods and emphasize the importance of gathering more information beyond what can be obtained from statistical tools alone. The paper also includes R code for reproducing all figures and provides access to the data sets themselves through an R package named quartets.},
  archiveprefix = {arxiv},
  keywords = {causality},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/McGowan et al_2023_Causal inference is not just a statistics problem.pdf;/Users/steven/Zotero/storage/A334XV8M/2304.html}
}

@book{mendelsonIntroductionTopologyThird2012,
  title = {Introduction to {{Topology}}: {{Third Edition}}},
  shorttitle = {Introduction to {{Topology}}},
  author = {Mendelson, Bert},
  year = {2012},
  month = apr,
  publisher = {{Courier Corporation}},
  abstract = {Highly regarded for its exceptional clarity, imaginative and instructive exercises, and fine writing style, this concise book offers an ideal introduction to the fundamentals of topology. Originally conceived as a text for a one-semester course, it is directed to undergraduate students whose studies of calculus sequence have included definitions and proofs of theorems. The book\&\#39;s principal aim is to provide a simple, thorough survey of elementary topics in the study of collections of objects, or sets, that possess a mathematical structure.The author begins with an informal discussion of set theory in Chapter 1, reserving coverage of countability for Chapter 5, where it appears in the context of compactness. In the second chapter Professor Mendelson discusses metric spaces, paying particular attention to various distance functions which may be defined on Euclidean n-space and which lead to the ordinary topology. Chapter 3 takes up the concept of topological space, presenting it as a generalization of the concept of a metric space. Chapters 4 and 5 are devoted to a discussion of the two most important topological properties: connectedness and compactness. Throughout the text, Dr. Mendelson, a former Professor of Mathematics at Smith College, has included many challenging and stimulating exercises to help students develop a solid grasp of the material presented.},
  googlebooks = {FWFmoEUJSwkC},
  isbn = {978-0-486-13509-0},
  langid = {english},
  keywords = {topology},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Courier Corporation/2012/Mendelson_2012_Introduction to Topology.pdf}
}

@phdthesis{menschApprentissageRepresentationsImagerie2018,
  type = {These de Doctorat},
  title = {Apprentissage de Repr{\'e}sentations En Imagerie Fonctionnelle},
  author = {Mensch, Arthur},
  year = {2018},
  month = sep,
  urldate = {2023-10-25},
  abstract = {Gr{\^a}ce aux avanc{\'e}es technologiques dans le domaine de l'imagerie fonctionnelle c{\'e}r{\'e}brale, les neurosciences cognitives accumulent une grande quantit{\'e} de cartes spatiales d{\'e}crivant de mani{\`e}re quantitative l'activit{\'e} neuronale suscit{\'e}e dans le cerveau humain en r{\'e}ponse {\`a} des t{\^a}ches ou des stimuli sp{\'e}cifiques, ou de mani{\`e}re spontan{\'e}e. Dans cette th{\`e}se, nous nous int{\'e}ressons particuli{\`e}rement aux donn{\'e}es issues de l'imagerie par r{\'e}sonance magn{\'e}tique fonctionnelle (IRMf), que nous {\'e}tudions dans un cadre d'apprentissage statistique. Notre objectif est d'apprendre des mod{\`e}les d'activit{\'e} c{\'e}r{\'e}brale {\`a} partir des donn{\'e}es. Nous proposons diff{\'e}rentes nouvelles mani{\`e}res de profiter de la grande quantit{\'e} de donn{\'e}es IRMf disponible. Tout d'abord, nous consid{\'e}rons les donn{\'e}es d'IRMf de repos, que nous traitons gr{\^a}ce {\`a} des m{\'e}thodes de factorisation de matrices. Nous pr{\'e}sentons de nouvelles m{\'e}thodes pour calculer en un temps raisonnable une factorisation parcimonieuse de matrices constitu{\'e}es de centaines d'enregistrements d'IRMf. Cela nous permet d'extraire des r{\'e}seaux fonctionnels {\`a} partir de donn{\'e}es d'une envergure in{\'e}dite. Notre m{\'e}thode principale introduit une r{\'e}duction al{\'e}atoire de la dimension des donn{\'e}es dans une boucle d'apprentissage en ligne. L'algorithme propos{\'e} converge plus de 10 fois plus vite que les meilleures m{\'e}thodes existantes, pour diff{\'e}rentes configurations et sur plusieurs jeux de donn{\'e}es. Nous effectuons une vaste validation exp{\'e}rimentale de notre approche de sous-{\'e}chantillonnage al{\'e}atoire. Nous proposons une {\'e}tude th{\'e}orique des propri{\'e}t{\'e}s de convergence de notre algorithme. Dans un second temps, nous nous int{\'e}ressons aux donn{\'e}es d'IRMf d'activation. Nous d{\'e}montrons comment agr{\'e}ger diff{\'e}rents {\'e}tudes acquises suivant des protocoles distincts afin d'apprendre des mod{\`e}les joints de d{\'e}codage plus justes et interpr{\'e}tables. Notre mod{\`e}le multi-{\'e}tudes apprend {\`a} r{\'e}duire la dimension des images c{\'e}r{\'e}brales en entr{\'e}e en m{\^e}me temps qu'il apprend {\`a} les classifier, pour chacune des {\'e}tudes, {\`a} partir de leurs repr{\'e}sentations r{\'e}duites. Cela suscite un transfert d'information entre les {\'e}tudes. En cons{\'e}quence, notre mod{\`e}le multi-{\'e}tude est plus performant que les mod{\`e}les de d{\'e}codage appris sur chaque {\'e}tude s{\'e}par{\'e}ment. Notre approche identifie une repr{\'e}sentation universellement pertinente de l'activit{\'e} c{\'e}r{\'e}brale, support{\'e}e par un petit nombre de r{\'e}seaux optimis{\'e}s pour l'identification de t{\^a}ches.},
  collaborator = {Thirion, Bertrand},
  copyright = {Licence Etalab},
  school = {Universit{\'e} Paris-Saclay (ComUE)},
  keywords = {deep-learning,functional-magnetic-resonance-imaging,machine-learning,optimisation,tensor-decomposition},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Université Paris-Saclay (ComUE)/2018/Mensch_2018_Apprentissage de représentations en imagerie fonctionnelle.pdf}
}

@techreport{mohajerComparisonGapStatistic2010,
  type = {{{workingPaper}}},
  title = {A Comparison of {{Gap}} Statistic Definitions with and without Logarithm Function},
  author = {Mohajer, Mojgan and Englmeier, Karl-Hans and Schmid, Volker J.},
  year = {2010},
  month = dec,
  volume = {96},
  doi = {10.5282/ubm/epub.11920},
  urldate = {2023-07-08},
  abstract = {The Gap statistic is a standard method for determining the number of clusters in a set of data. The Gap statistic standardizes the graph of \${\textbackslash}log(W\_\{k\})\$, where \$W\_\{k\}\$ is the within-cluster dispersion, by comparing it to its expectation under an appropriate null reference distribution of the data. We suggest to use \$W\_\{k\}\$ instead of \${\textbackslash}log(W\_\{k\})\$, and to compare it to the expectation of \$W\_\{k\}\$ under a null reference distribution. In fact, whenever a number fulfills the original Gap statistic inequality, this number also fulfills the inequality of a Gap statistic using \$W\_\{k\}\$, but not {\textbackslash}textit\{vice versa\}. The two definitions of the Gap function are evaluated on several simulated data set and on a real data of DCE-MR images.},
  langid = {english},
  keywords = {clustering},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2010/Mohajer et al_2010_A comparison of Gap statistic definitions with and without logarithm function.pdf;/Users/steven/Zotero/storage/Y3IPPS59/11920.html}
}

@article{mohammadiFunctionalDataAnalysis2023,
  title = {Functional Data Analysis with Rough Sample Paths?},
  author = {Mohammadi, Neda and Panaretos, Victor M.},
  year = {2023},
  journal = {Journal of Nonparametric Statistics},
  volume = {0},
  number = {0},
  pages = {1--19},
  publisher = {{Taylor \& Francis}},
  issn = {1048-5252},
  doi = {10.1080/10485252.2023.2215347},
  urldate = {2023-10-24},
  abstract = {Functional data are typically modeled as sample paths of smooth stochastic processes in order to mitigate the fact that they are often observed discretely and noisily, occasionally irregularly and sparsely. The smoothness assumption is imposed to allow for the use of smoothing techniques that annihilate the noise. At the same time, imposing the smoothness assumption excludes a considerable range of stochastic processes, most notably diffusion processes. Under perfect observation of the sample paths, such processes would not need to be excluded from the realm of functional data analysis. In this paper, we introduce a careful modification of existing methods, dubbed the `reflected triangle estimator', and show that this allows for the functional data analysis of processes with nowhere differentiable sample paths, even when these are discretely and noisily observed, including under irregular and sparse designs. Our estimator matches the established rates of convergence for processes with smooth paths, and furthermore attains the same optimal rates as one would get under perfect observation. Thus, with reflected triangle estimation, the scope of applicability of much of the methodology developed for discretely/irregularly/noisily/sparsely sampled functional data is considerably extended. By way of simulation it is shown that the advantages furnished are reflected in practice, hinting at potential closer links with the field of diffusion inference.},
  keywords = {covariance-operator-estimation,functional-data-analysis,local-polynomial-smoothing,smoothness},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Nonparametric Statistics/2023/Mohammadi_Panaretos_2023_Functional data analysis with rough sample paths.pdf}
}

@misc{moindjieRegressionModelsRepeated2023,
  title = {Regression Models with Repeated Functional Data},
  author = {Moindji{\'e}, Issam-Ali and Preda, Cristian},
  year = {2023},
  month = aug,
  number = {arXiv:2308.01747},
  eprint = {2308.01747},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.01747},
  urldate = {2023-10-27},
  abstract = {Linear regression and classification models with repeated functional data are considered. For each statistical unit in the sample, a real-valued parameter is observed over time under different conditions. Two regression models based on fusion penalties are presented. The first one is a generalization of the variable fusion model based on the 1-nearest neighbor. The second one, called group fusion lasso, assumes some grouping structure of conditions and allows for homogeneity among the regression coefficient functions within groups. A finite sample numerical simulation and an application on EEG data are presented.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,functional-regression,multivariate-functional-data,repeated-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Moindjié_Preda_2023_Regression models with repeated functional data.pdf;/Users/steven/Zotero/storage/QZK2ARH7/2308.html}
}

@phdthesis{momalNetworkInferenceIncomplete2020,
  type = {These de Doctorat},
  title = {Network Inference from Incomplete Abundance Data},
  author = {Momal, Rapha{\"e}lle},
  year = {2020},
  month = nov,
  urldate = {2023-10-25},
  abstract = {Les r{\'e}seaux sont utilis{\'e}s comme outils en microbiologie et en {\'e}cologie pour repr{\'e}senter des relations entre esp{\`e}ces. Les mod{\`e}les graphiques gaussiens sont le cadre math{\'e}matique d{\'e}di{\'e} {\`a} l'inf{\'e}rence des r{\'e}seaux de d{\'e}pendances conditionnelles, qui permettent une s{\'e}paration claires des effets directs et indirects. Cependant, les donn{\'e}es observ{\'e}es sont souvent des comptages discr{\`e}ts qui ne permettent pas l'utilisation de ce mod{\`e}le. Cette th{\`e}se d{\'e}veloppe une m{\'e}thodologie pour l'inf{\'e}rence de r{\'e}seaux {\`a} partir de donn{\'e}es d'abondance d'esp{\`e}ces. La m{\'e}thode repose sur une exploration efficace et exhaustive de l'espace des arbres couvrants dans un espace latent des comptages observ{\'e}s, rendue possible par les propri{\'e}t{\'e}s alg{\'e}briques de ces structures.Par ailleurs,  il est probable que les comptages observ{\'e}s d{\'e}pendent d'acteurs non mesur{\'e}s (esp{\`e}ces ou covariable).  Ce ph{\'e}nom{\`e}ne produit des ar{\^e}tes suppl{\'e}mentaires dans le r{\'e}seau marginal entre les esp{\`e}ces li{\'e}es {\`a} l'acteur manquant dans le r{\'e}seau complet, ce qui fausse la suite des analyses. Le second objectif de ce travail est de prendre en compte les acteurs manquants lors de l'inf{\'e}rence de r{\'e}seau. Les param{\`e}tres du mod{\`e}le propos{\'e} sont estim{\'e}s par une approche variationnelle, qui fournit des {\'e}l{\'e}ments d'information pertinents {\`a} propos des donn{\'e}es non observ{\'e}es.},
  collaborator = {Robin, St{\'e}phane and Ambroise, Christophe},
  copyright = {Licence Etalab},
  school = {universit{\'e} Paris-Saclay},
  keywords = {graphical-model,network-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/université Paris-Saclay/2020/Momal_2020_Network inference from incomplete abundance data.pdf}
}

@phdthesis{montariolModelsDiachronicSemantic2021,
  type = {These de Doctorat},
  title = {Models of Diachronic Semantic Change Using Word Embeddings},
  author = {Montariol, Syrielle},
  year = {2021},
  month = feb,
  urldate = {2023-10-25},
  abstract = {Dans cette th{\`e}se, nous {\'e}tudions les changements lexico-s{\'e}mantiques : les variations temporelles dans l'usage et la signification des mots, {\'e}galement appel{\'e} 	extit\{diachronie\}. Ces changements refl{\`e}tent l'{\'e}volution de divers aspects de la soci{\'e}t{\'e} tels que l'environnement technologique et culturel.Nous explorons et {\'e}valuons des m{\'e}thodes de construction de plongements lexicaux variant dans le temps afin d'analyser l'{\'e}volution du language. Nous utilisont notamment des plongements contextualis{\'e}s {\`a} partir de mod{\`e}les de langue pr{\'e}-entra{\^i}n{\'e}s tels que BERT.Nous proposons plusieurs approches pour extraire et agr{\'e}ger les repr{\'e}sentations contextualis{\'e}es des mots dans le temps, et quantifier leur degr{\'e} de changement s{\'e}mantique. En particulier, nous abordons l'aspect pratique de ces syst{\`e}mes: le passage {\`a} l'{\'e}chelle de nos approches, en vue de les appliquer {\`a} de grands corpus ou de larges vocabulaire; leur interpr{\'e}tabilit{\'e}, en d{\'e}sambigu{\"i}sant les diff{\'e}rents usages d'un mot au cours du temps; et leur applicabilit{\'e} {\`a} des probl{\'e}matiques concr{\`e}tes, pour des documents li{\'e}s au COVID19 et des corpus du domaine financier. Nous {\'e}valuons l'efficacit{\'e} de ces m{\'e}thodes de mani{\`e}re quantitative, en utilisant plusieurs corpus annot{\'e}s, et de mani{\`e}re qualitative, en liant les variations d{\'e}tect{\'e}es dans des corpus avec  des {\'e}v{\'e}nements de la vie r{\'e}elle et des donn{\'e}es num{\'e}riques.Enfin, nous {\'e}tendons la t{\^a}che de d{\'e}tection de changements s{\'e}mantiques au-del{\`a} de la dimension temporelle. Nous l'adaptons {\`a} un cadre bilingue, pour {\'e}tudier l'{\'e}volution conjointe d'un mot et sa traduction dans deux corpus de langues diff{\'e}rentes; et {\`a} un cadre synchronique, pour d{\'e}tecter des variations s{\'e}mantiques entre diff{\'e}rentes sources ou communaut{\'e}s en plus de la variation temporelle.},
  collaborator = {Allauzen, Alexandre and Janvier, Jean-Baptiste},
  copyright = {Licence Etalab},
  school = {universit{\'e} Paris-Saclay},
  keywords = {natural-language-processing,neural-network,word-embeddings},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/université Paris-Saclay/2021/Montariol_2021_Models of diachronic semantic change using word embeddings.pdf}
}

@article{mouraEstimationParametersSelection2022,
  title = {Estimation {{Of Parameters And Selection Of Models Applied To Population Balance Dynamics Via Approximate Bayesian Computational}}},
  author = {Moura, Carlos Henrique and Viegas, Bruno Marques and Tavares, Maria and Macedo, Emanuel and Estumano, Diego Cardoso},
  year = {2022},
  month = may,
  journal = {Journal of Heat and Mass Transfer Research},
  volume = {9},
  number = {1},
  pages = {53--64},
  publisher = {{Semnan University}},
  issn = {2345-508X},
  doi = {10.22075/jhmtr.2022.25186.1361},
  urldate = {2023-07-08},
  abstract = {Population balance models mathematically describe the particle size distribution based on modeling physical phenomena that influence the distribution, such as aggregation, growth, and breakage. Due to the wide range of mechanisms present, several models are presented in the literature since several hypotheses are considered. In the current work, the Approximate Bayesian Computational statistical technique was used to select four different models of population balance and estimate their parameters. Three strategies were applied to the drawing of parameters, evaluating the correlation between the parameters of the models. An adaptive tolerance in each population and a stopping criterion, based on Morozov's uncertainty principle, were used for the algorithm. The technique obtained reasonable estimates for the phenomenological rates of the models. The algorithm correctly selected the model used for generating measurements, and the three draw strategies demonstrated good applicability. The results obtained showed that the algorithm presented accuracy and precision in estimating the parameters and properly selected the models analyzed.},
  keywords = {bayesian-analysis,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Heat and Mass Transfer Research/2022/Moura et al_2022_Estimation Of Parameters And Selection Of Models Applied To Population Balance.pdf}
}

@article{mouraParameterEstimationPopulation2021,
  title = {Parameter {{Estimation}} in {{Population Balance}} through {{Bayesian}} ‎{{Technique Markov Chain Monte Carlo}}},
  author = {Moura, Carlos H. R. and Viegas, Bruno M. and Tavares, Maria R. M. and Mac{\^e}do, Emanuel N. and Estumano, Diego C. and Quaresma, Jo{\~a}o N. N.},
  year = {2021},
  month = apr,
  journal = {Journal of Applied and Computational Mechanics},
  volume = {7},
  number = {2},
  pages = {890--901},
  publisher = {{Shahid Chamran University of Ahvaz}},
  issn = {2383-4536},
  doi = {10.22055/jacm.2021.35741.2725},
  urldate = {2023-07-08},
  abstract = {In this work, the Markov Chain Monte Carlo is applied to estimate parameters that represent mechanisms that describe particles' dynamics in particulate systems from the literature's proposed models. Initially, the reduced sensitivity coefficient is evaluated to verify which parameters could be estimated simultaneously. The technique is then applied to estimate the models' parameters in different numerical scenarios to determine the rates that influence population dynamics. After the analyzes are performed, the estimates show good precision, accuracy, and a good fit between the measured and estimated state variables. The results show that the Markov chain Monte Carlo can determine the rates of population balance phenomenon.},
  keywords = {bayesian-analysis,markov-process,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Applied and Computational Mechanics/2021/Moura et al_2021_Parameter Estimation in Population Balance through Bayesian Technique Markov.pdf}
}

@article{ntzoufrasBayesianModelsPrediction2021,
  title = {Bayesian Models for Prediction of the Set-Difference in Volleyball},
  author = {Ntzoufras, Ioannis and Palaskas, Vasilis and Drikos, Sotiris},
  year = {2021},
  month = oct,
  journal = {IMA Journal of Management Mathematics},
  volume = {32},
  number = {4},
  pages = {491--518},
  issn = {1471-678X},
  doi = {10.1093/imaman/dpab007},
  urldate = {2023-10-24},
  abstract = {We study and develop Bayesian models for the analysis of volleyball match outcomes as recorded by the set-difference. Due to the peculiarity of the outcome variable (set-difference) which takes discrete values from \$-3\$ to \$3\$, we cannot consider standard models based on the usual Poisson or binomial assumptions used for other sports such as football/soccer. Hence, the first and foremost challenge was to build models appropriate for the set-difference of each volleyball match. Here we consider two major approaches: (a) an ordered multinomial logistic regression model and (b) a model based on a truncated version of the Skellam distribution. For the first model, we consider the set-difference as an ordinal response variable within the framework of multinomial logistic regression models. Concerning the second model, we adjust the Skellam distribution to account for the volleyball rules. We fit and compare both models with the same covariate structure as in Karlis \&amp; Ntzoufras (2003). Both models are fitted, illustrated and compared within Bayesian framework using data from both the regular season and the play-offs of the season 2016/17 of the Greek national men's volleyball league A1.},
  keywords = {bayesian-analysis,sport-science,volleyball},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/IMA Journal of Management Mathematics/2021/Ntzoufras et al_2021_Bayesian models for prediction of the set-difference in volleyball.pdf;/Users/steven/Zotero/storage/UF3DGZ32/6220350.html}
}

@misc{oguamalamMinimumRegularizedCovariance2023,
  title = {Minimum Regularized Covariance Trace Estimator and Outlier Detection for Functional Data},
  author = {Oguamalam, Jeremy and Radoji{\v c}i{\'c}, Una and Filzmoser, Peter},
  year = {2023},
  month = jul,
  number = {arXiv:2307.13509},
  eprint = {2307.13509},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.13509},
  urldate = {2023-10-27},
  abstract = {In this paper, we propose the Minimum Regularized Covariance Trace (MRCT) estimator, a novel method for robust covariance estimation and functional outlier detection. The MRCT estimator employs a subset-based approach that prioritizes subsets exhibiting greater centrality based on the generalization of the Mahalanobis distance, resulting in a fast-MCD type algorithm. Notably, the MRCT estimator handles high-dimensional data sets without the need for preprocessing or dimension reduction techniques, due to the internal smoothening whose amount is determined by the regularization parameter \${\textbackslash}alpha {$>$} 0\$. The selection of the regularization parameter \${\textbackslash}alpha\$ is automated. The proposed method adapts seamlessly to sparsely observed data by working directly with the finite matrix of basis coefficients. An extensive simulation study demonstrates the efficacy of the MRCT estimator in terms of robust covariance estimation and automated outlier detection, emphasizing the balance between noise exclusion and signal preservation achieved through appropriate selection of \${\textbackslash}alpha\$. The method converges fast in practice and performs favorably when compared to other functional outlier detection methods.},
  archiveprefix = {arxiv},
  keywords = {covariance-operator-estimation,functional-data-analysis,mahalanobis-distance,outliers-detection},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Oguamalam et al_2023_Minimum regularized covariance trace estimator and outlier detection for.pdf;/Users/steven/Zotero/storage/GTG5SUI3/2307.html}
}

@article{oseiBayesianLinearModels2022,
  title = {Bayesian Linear Models for Cardinal Paired Comparison Data},
  author = {Osei, Prince P. and Davidov, Ori},
  year = {2022},
  month = aug,
  journal = {Computational Statistics \& Data Analysis},
  volume = {172},
  pages = {107481},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2022.107481},
  urldate = {2023-10-24},
  abstract = {This paper develops a methodology for Bayesian updating in normal linear models in situations where the parameter of interest is restricted to a linear subspace. The methodology is motivated by and applied to the calculation of posterior distributions for the merit parameters and ranks arising in paired comparison data. The Bayesian paradigm is found to be ideal for assessing and quantifying the uncertainty in ranking procedures. The methodology is illustrated using simulated data and applied to two data sets: a network meta{\textendash}analysis example and to the ranking of teams in the National Basketball Association (NBA).},
  keywords = {bayesian-analysis,ordinal-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2022/Osei_Davidov_2022_Bayesian linear models for cardinal paired comparison data.pdf;/Users/steven/Zotero/storage/U4FAQ8M9/S0167947322000615.html}
}

@book{pagesMultipleFactorAnalysis2014,
  title = {Multiple {{Factor Analysis}} by {{Example Using R}}},
  author = {Pag{\`e}s, J{\'e}r{\^o}me},
  year = {2014},
  month = nov,
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/b17700},
  abstract = {Multiple factor analysis (MFA) enables users to analyze tables of individuals and variables in which the variables are structured into quantitative, qualitative, or mixed groups. Written by the co-developer of this methodology, Multiple Factor Analysis by Example Using R brings together the theoretical and methodological aspects of MFA. It also inc},
  isbn = {978-0-429-17108-6},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2014/Pagès_2014_Multiple Factor Analysis by Example Using R.pdf}
}

@article{panaretosFourierAnalysisStationary2013,
  title = {Fourier Analysis of Stationary Time Series in Function Space},
  author = {Panaretos, Victor M. and Tavakoli, Shahin},
  year = {2013},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {41},
  number = {2},
  pages = {568--603},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/13-AOS1086},
  urldate = {2023-09-19},
  abstract = {We develop the basic building blocks of a frequency domain framework for drawing statistical inferences on the second-order structure of a stationary sequence of functional data. The key element in such a context is the spectral density operator, which generalises the notion of a spectral density matrix to the functional setting, and characterises the second-order dynamics of the process. Our main tool is the functional Discrete Fourier Transform (fDFT). We derive an asymptotic Gaussian representation of the fDFT, thus allowing the transformation of the original collection of dependent random functions into a collection of approximately independent complex-valued Gaussian random functions. Our results are then employed in order to construct estimators of the spectral density operator based on smoothed versions of the periodogram kernel, the functional generalisation of the periodogram matrix. The consistency and asymptotic law of these estimators are studied in detail. As immediate consequences, we obtain central limit theorems for the mean and the long-run covariance operator of a stationary functional time series. Our results do not depend on structural modelling assumptions, but only functional versions of classical cumulant mixing conditions, and are shown to be stable under discrete observation of the individual curves.},
  keywords = {functional-data-analysis,functional-time-series,spectral-analysis,weak-dependence},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2013/Panaretos_Tavakoli_2013_Fourier analysis of stationary time series in function space.pdf}
}

@article{paparoditisBootstrapPredictionBands2023,
  title = {Bootstrap {{Prediction Bands}} for {{Functional Time Series}}},
  author = {Paparoditis, Efstathios and Shang, Han Lin},
  year = {2023},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {118},
  number = {542},
  pages = {972--986},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2021.1963262},
  urldate = {2023-09-19},
  abstract = {A bootstrap procedure for constructing prediction bands for a stationary functional time series is proposed. The procedure exploits a general vector autoregressive representation of the time-reversed series of Fourier coefficients appearing in the Karhunen{\textendash}Lo{\`e}ve representation of the functional process. It generates backward-in-time functional replicates that adequately mimic the dependence structure of the underlying process in a model-free way and have the same conditionally fixed curves at the end of each functional pseudo-time series. The bootstrap prediction error distribution is then calculated as the difference between the model-free, bootstrap-generated future functional observations and the functional forecasts obtained from the model used for prediction. This allows the estimated prediction error distribution to account for the innovation and estimation errors associated with prediction and the possible errors due to model misspecification. We establish the asymptotic validity of the bootstrap procedure in estimating the conditional prediction error distribution of interest, and we also show that the procedure enables the construction of prediction bands that achieve (asymptotically) the desired coverage. Prediction bands based on a consistent estimation of the conditional distribution of the studentized prediction error process also are introduced. Such bands allow for taking more appropriately into account the local uncertainty of the prediction. Through a simulation study and the analysis of two datasets, we demonstrate the capabilities and the good finite-sample performance of the proposed method.},
  keywords = {forecasting,functional-principal-components,functional-time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2023/Paparoditis_Shang_2023_Bootstrap Prediction Bands for Functional Time Series.pdf}
}

@article{parkClusteringMultivariateFunctional2017,
  title = {Clustering Multivariate Functional Data with Phase Variation},
  author = {Park, Juhyun and Ahn, Jeongyoun},
  year = {2017},
  journal = {Biometrics},
  volume = {73},
  number = {1},
  pages = {324--333},
  issn = {1541-0420},
  doi = {10.1111/biom.12546},
  urldate = {2023-11-21},
  abstract = {When functional data come as multiple curves per subject, characterizing the source of variations is not a trivial problem. The complexity of the problem goes deeper when there is phase variation in addition to amplitude variation. We consider clustering problem with multivariate functional data that have phase variations among the functional variables. We propose a conditional subject-specific warping framework in order to extract relevant features for clustering. Using multivariate growth curves of various parts of the body as a motivating example, we demonstrate the effectiveness of the proposed approach. The found clusters have individuals who show different relative growth patterns among different parts of the body.},
  copyright = {{\copyright} 2016, The International Biometric Society},
  langid = {english},
  keywords = {clustering,functional-data-analysis,growth,multivariate-functional-data,registration},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/2017/Park_Ahn_2017_Clustering multivariate functional data with phase variation.pdf;/Users/steven/Zotero/storage/3ZNMZEFJ/biom.html}
}

@article{pattersonStochasticWeightedParticle2011,
  title = {Stochastic Weighted Particle Methods for Population Balance Equations},
  author = {Patterson, Robert I. A. and Wagner, Wolfgang and Kraft, Markus},
  year = {2011},
  month = aug,
  journal = {Journal of Computational Physics},
  volume = {230},
  number = {19},
  pages = {7456--7472},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2011.06.011},
  urldate = {2023-07-08},
  abstract = {A class of coagulation weight transfer functions is constructed, each member of which leads to a stochastic particle algorithm for the numerical treatment of population balance equations. These algorithms are based on systems of weighted computational particles and the weight transfer functions are constructed such that the number of computational particles does not change during coagulation events. The algorithms also facilitate the simulation of physical processes that change single particles, such as growth, or other surface reactions. Four members of the algorithm family have been numerically validated by comparison to analytic solutions to simple problems. Numerical experiments have been performed for complex laminar premixed flame systems in which members of the class of stochastic weighted particle methods were compared to each other and to a direct simulation algorithm. Two of the weighted algorithms have been shown to offer performance advantages over the direct simulation algorithm in situations where interest is focused on the larger particles in a system. The extent of this advantage depends on the particular system and on the quantities of interest.},
  langid = {english},
  keywords = {bayesian-analysis,coagulation,markov-process,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Computational Physics/2011/Patterson et al. - 2011 - Stochastic weighted particle methods for populatio.pdf;/Users/steven/Zotero/storage/FCX6W3CG/S0021999111003603.html}
}

@misc{paulContinuoustimeMultivariateAnalysis2023,
  title = {Continuous-Time Multivariate Analysis},
  author = {Paul, Biplab and Reiss, Philip T. and Cui, Erjia},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09404},
  eprint = {2307.09404},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09404},
  urldate = {2023-10-27},
  abstract = {The starting point for much of multivariate analysis (MVA) is an \$n{\textbackslash}times p\$ data matrix whose \$n\$ rows represent observations and whose \$p\$ columns represent variables. Some multivariate data sets, however, may be best conceptualized not as \$n\$ discrete \$p\$-variate observations, but as \$p\$ curves or functions defined on a common time interval. We introduce a framework for extending techniques of multivariate analysis to such settings. The proposed framework rests on the assumption that the curves can be represented as linear combinations of basis functions such as B-splines. This is formally identical to the Ramsay-Silverman representation of functional data; but whereas functional data analysis extends MVA to the case of observations that are curves rather than vectors -- heuristically, \$n{\textbackslash}times p\$ data with \$p\$ infinite -- we are instead concerned with what happens when \$n\$ is infinite. We describe how to translate the classical MVA methods of covariance and correlation estimation, principal component analysis, Fisher's linear discriminant analysis, and \$k\$-means clustering to the continuous-time setting. We illustrate the methods with a novel perspective on a well-known Canadian weather data set, and with applications to neurobiological and environmetric data. The methods are implemented in the publicly available R package {\textbackslash}texttt\{ctmva\}.},
  archiveprefix = {arxiv},
  keywords = {functional-data-analysis,functional-principal-components,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Paul et al_2023_Continuous-time multivariate analysis.pdf;/Users/steven/Zotero/storage/NZPB7GXN/2307.html}
}

@article{perperoglouReviewSplineFunction2019,
  title = {A Review of Spline Function Procedures in {{R}}},
  author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
  year = {2019},
  month = mar,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {46},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0666-3},
  urldate = {2023-10-27},
  abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
  keywords = {r-software,review,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/BMC Medical Research Methodology/2019/Perperoglou et al_2019_A review of spline function procedures in R.pdf;/Users/steven/Zotero/storage/39Z32YUB/s12874-019-0666-3.html}
}

@misc{perrichonMultivariateFunctionalData2023,
  title = {A {{Multivariate Functional Data Analysis}} of {{Aircraft Trajectories}}},
  author = {Perrichon, Remi and Gendre, Xavier and Klein, Thierry},
  year = {2023},
  month = jun,
  urldate = {2023-10-25},
  abstract = {While advanced methods for functional data analysis have recently been developed in the literature, applications to aircraft trajectories have remained scarce, despite operational relevance. One reason is the practical difficulties affiliated with the multivariate nature of trajectories and associated physical constraints. Indeed, an aircraft trajectory usually involves three dimensions in space (longitude, latitude, altitude) but also weather values (say wind speed and direction), each dimension having its specificities. To name a few, smoothing altitude values requires to ensure both non-negativity and boundary constraints. Wind directions have support on the unit circle. Additional to constrained smoothing challenges, phase variations are to be taken into account as flights are never of the same duration. To tackle these issues, two smoothing methods respectively based on constrained splines and asymmetric kernels are implemented on real data. For each approach, two strategies to handle the circular nature of wind directions are compared. Registration is performed. A joint pointwise test is proposed to demonstrate that delayed flights have experienced less favorable wind conditions.},
  langid = {english},
  keywords = {aircraft-data,curve-registration,functional-data-analysis,multivariate-functional-data,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2023/Perrichon et al_2023_A Multivariate Functional Data Analysis of Aircraft Trajectories.pdf}
}

@misc{pfortnerPhysicsInformedGaussianProcess2023,
  title = {Physics-{{Informed Gaussian Process Regression Generalizes Linear PDE Solvers}}},
  author = {Pf{\"o}rtner, Marvin and Steinwart, Ingo and Hennig, Philipp and Wenger, Jonathan},
  year = {2023},
  month = apr,
  number = {arXiv:2212.12474},
  eprint = {2212.12474},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.12474},
  urldate = {2023-10-24},
  abstract = {Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bounded linear operator. Crucially, this probabilistic viewpoint allows to (1) quantify the inherent discretization error; (2) propagate uncertainty about the model parameters to the solution; and (3) condition on noisy measurements. Demonstrating the strength of this formulation, we prove that it strictly generalizes methods of weighted residuals, a central class of PDE solvers including collocation, finite volume, pseudospectral, and (generalized) Galerkin methods such as finite element and spectral methods. This class can thus be directly equipped with a structured error estimate. In summary, our results enable the seamless integration of mechanistic models as modular building blocks into probabilistic models by blurring the boundaries between numerical analysis and Bayesian inference.},
  archiveprefix = {arxiv},
  keywords = {differential-equation,gaussian-process,operator-theory,physics-informed-model},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Pförtner et al_2023_Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers.pdf;/Users/steven/Zotero/storage/7WFXG8ZV/2212.html}
}

@article{porterWirelessSensorNetworks2005,
  title = {Wireless {{Sensor Networks}} for {{Ecology}}},
  author = {Porter, John and Arzberger, Peter and Braun, Hans-Werner and Bryant, Pablo and Gage, Stuart and Hansen, Todd and Hanson, Paul and Lin, Chau-Chin and Lin, Fang-Pang and Kratz, Timothy and Michener, William and Shapiro, Sedra and Williams, Thomas},
  year = {2005},
  month = jul,
  journal = {BioScience},
  volume = {55},
  number = {7},
  pages = {561--572},
  issn = {0006-3568},
  doi = {10.1641/0006-3568(2005)055[0561:WSNFE]2.0.CO;2},
  urldate = {2023-10-02},
  abstract = {Field biologists and ecologists are starting to open new avenues of inquiry at greater spatial and temporal resolution, allowing them to ``observe the unobservable'' through the use of wireless sensor networks. Sensor networks facilitate the collection of diverse types of data (from temperature to imagery and sound) at frequent intervals{\textemdash}even multiple times per second{\textemdash}over large areas, allowing ecologists and field biologists to engage in intensive and expansive sampling and to unobtrusively collect new types of data. Moreover, real-time data flows allow researchers to react rapidly to events, thus extending the laboratory to the field. We review some existing uses of wireless sensor networks, identify possible areas of application, and review the underlying technologies in the hope of stimulating additional use of this promising technology to address the grand challenges of environmental science.},
  keywords = {ecology,sensor-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/BioScience/2005/Porter et al_2005_Wireless Sensor Networks for Ecology.pdf;/Users/steven/Zotero/storage/PLWKHNYC/306750.html}
}

@article{prochazkaDiscriminationCyclingPatterns2021,
  title = {Discrimination of Cycling Patterns Using Accelerometric Data and Deep Learning Techniques},
  author = {Proch{\'a}zka, Ale{\v s} and Charv{\'a}tov{\'a}, Hana and Vy{\v s}ata, Old{\v r}ich and Jarchi, Delaram and Sanei, Saeid},
  year = {2021},
  month = jul,
  journal = {Neural Computing and Applications},
  volume = {33},
  number = {13},
  pages = {7603--7613},
  issn = {1433-3058},
  doi = {10.1007/s00521-020-05504-3},
  urldate = {2023-10-24},
  abstract = {The monitoring of physical activities and recognition of motion disorders belong to important diagnostical tools in neurology and rehabilitation. The goal of the present paper is in the contribution to this topic by (1) analysis of accelerometric signals recorded by wearable sensors located at specific body positions and by (2) implementation of deep learning methods to classify signal features. This paper uses the general methodology to analysis of accelerometric signals acquired during cycling at different routes followed by the global positioning system. The experimental dataset includes 850 observations that were recorded by a mobile device in the spine area (L3 verterbra) for cycling routes with the different slope. The proposed methodology includes the use of deep learning convolutional neural networks with five layers applied to signal values transformed into the frequency domain without specification of any signal features. The accuracy of discrimination between different motion patterns for the uphill and downhill cycling and recognition of 4 classes associated with different route slopes was 96.6\% with the loss criterion of 0.275 for sigmoidal activation functions. These results were compared with those evaluated for selected sets of features estimated for each observation and classified by the support vector machine, Bayesian methods, and the two-layer neural network. The best cross-validation error of 0.361 was achieved for the two-layer neural network model with the sigmoidal and softmax transfer functions. Our methodology suggests that deep learning neural networks are efficient in the assessment of motion activities for automated data processing and have a wide range of applications, including rehabilitation, early diagnosis of neurological problems, and possible use in engineering as well.},
  langid = {english},
  keywords = {classification,cycling,deep-learning,machine-learning,neural-network,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Neural Computing and Applications/2021/Procházka et al_2021_Discrimination of cycling patterns using accelerometric data and deep learning.pdf}
}

@misc{pulidoEpigraphHypographIndexes2023,
  title = {The Epigraph and the Hypograph Indexes as Useful Tools for Clustering Multivariate Functional Data},
  author = {Pulido, Bel{\'e}n and {Franco-Pereira}, Alba M. and Lillo, Rosa E.},
  year = {2023},
  month = oct,
  number = {arXiv:2307.16720},
  eprint = {2307.16720},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.16720},
  urldate = {2023-10-25},
  abstract = {The proliferation of data generation has spurred advancements in functional data analysis. With the ability to analyze multiple variables simultaneously, the demand for working with multivariate functional data has increased. This study proposes a novel formulation of the epigraph and hypograph indexes, as well as their generalized expressions, specifically tailored for the multivariate functional context. These definitions take into account the interrelations between components. Furthermore, the proposed indexes are employed to cluster multivariate functional data. In the clustering process, the indexes are applied to both the data and their first and second derivatives. This generates a reduced-dimension dataset from the original multivariate functional data, enabling the application of well-established multivariate clustering techniques which have been extensively studied in the literature. This methodology has been tested through simulated and real datasets, performing comparative analyses against state-of-the-art to assess its performance.},
  archiveprefix = {arxiv},
  keywords = {clustering,functional-data-analysis,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Pulido et al_2023_The epigraph and the hypograph indexes as useful tools for clustering.pdf;/Users/steven/Zotero/storage/MACMUCXX/2307.html}
}

@article{qiuTestsEqualitySeveral2024,
  title = {Tests for Equality of Several Covariance Matrix Functions for Multivariate Functional Data},
  author = {Qiu, Zhiping and Fan, Jiangyuan and Zhang, Jin-Ting and Chen, Jianwei},
  year = {2024},
  month = jan,
  journal = {Journal of Multivariate Analysis},
  volume = {199},
  pages = {105243},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2023.105243},
  urldate = {2023-10-27},
  abstract = {Multivariate functional data are often observed in many scientific fields. This paper considers a multi-sample equal-covariance matrix function testing problem for multivariate functional data. Two new tests are proposed and studied. The asymptotic properties of the two tests under the null hypothesis and a local alternative are investigated. Two methods for approximating the null distributions of the test statistics are described. It is shown that the two tests are root-n consistent. Two simulation studies are conducted to evaluate the finite sample performance of the proposed tests. Finally, the two tests are illustrated via applications to three real multivariate functional data sets.},
  keywords = {functional-data-analysis,multivariate-functional-data,two-sample-problem},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Multivariate Analysis/2024/Qiu et al_2024_Tests for equality of several covariance matrix functions for multivariate.pdf;/Users/steven/Zotero/storage/3DR8TREM/S0047259X23000891.html}
}

@article{ramkrishnaPopulationBalanceModeling2014,
  title = {Population Balance Modeling: Current Status and Future Prospects},
  shorttitle = {Population Balance Modeling},
  author = {Ramkrishna, Doraiswami and Singh, Meenesh R.},
  year = {2014},
  journal = {Annual Review of Chemical and Biomolecular Engineering},
  volume = {5},
  pages = {123--146},
  issn = {1947-5438},
  doi = {10.1146/annurev-chembioeng-060713-040241},
  abstract = {Population balance modeling is undergoing phenomenal growth in its applications, and this growth is accompanied by multifarious reviews. This review aims to fortify the model's fundamental base, as well as point to a variety of new applications, including modeling of crystal morphology, cell growth and differentiation, gene regulatory processes, and transfer of drug resistance. This is accomplished by presenting the many faces of population balance equations that arise in the foregoing applications.},
  langid = {english},
  pmid = {24606333},
  keywords = {population-balance-modelling,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annual Review of Chemical and Biomolecular Engineering/2014/Ramkrishna and Singh - 2014 - Population balance modeling current status and fu.pdf}
}

@misc{ramos-carrenoScikitfdaPythonPackage2023,
  title = {Scikit-Fda: {{A Python Package}} for {{Functional Data Analysis}}},
  shorttitle = {Scikit-Fda},
  author = {{Ramos-Carre{\~n}o}, Carlos and Torrecilla, Jos{\'e} Luis and {Carbajo-Berrocal}, Miguel and Marcos, Pablo and Su{\'a}rez, Alberto},
  year = {2023},
  month = jun,
  number = {arXiv:2211.02566},
  eprint = {2211.02566},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.02566},
  urldate = {2024-01-15},
  abstract = {The library scikit-fda is a Python package for Functional Data Analysis (FDA). It provides a comprehensive set of tools for representation, preprocessing, and exploratory analysis of functional data. The library is built upon and integrated in Python's scientific ecosystem. In particular, it conforms to the scikit-learn application programming interface so as to take advantage of the functionality for machine learning provided by this package: pipelines, model selection, and hyperparameter tuning, among others. The scikit-fda package has been released as free and open-source software under a 3-Clause BSD license and is open to contributions from the FDA community. The library's extensive documentation includes step-by-step tutorials and detailed examples of use.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Ramos-Carreño et al_2023_scikit-fda.pdf;/Users/steven/Zotero/storage/WPIMHU58/2211.html}
}

@misc{ramsayFdaFunctionalData2023,
  title = {Fda: {{Functional Data Analysis}}},
  shorttitle = {Fda},
  author = {Ramsay, James and Hooker, Giles and Graves, Spencer},
  year = {2023},
  month = may,
  urldate = {2024-01-10},
  abstract = {These functions were developed to support functional data analysis as described in Ramsay, J. O. and Silverman, B. W. (2005) Functional Data Analysis. New York: Springer and in Ramsay, J. O., Hooker, Giles, and Graves, Spencer (2009). Functional Data Analysis with R and Matlab (Springer). The package includes data sets and script files working many examples including all but one of the 76 figures in this latter book. Matlab versions are available by ftp from {$<$}https://www.psych.mcgill.ca/misc/fda/downloads/FDAfuns/{$>$}.},
  copyright = {GPL-2 {\textbar} GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2023/Ramsay et al_2023_fda.pdf}
}

@book{ramsayFunctionalDataAnalysis2005,
  title = {Functional {{Data Analysis}}},
  author = {Ramsay, J. O. and Silverman, B. W.},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/b98888},
  urldate = {2023-07-07},
  isbn = {978-0-387-40080-8 978-0-387-22751-1},
  langid = {english},
  keywords = {functional-data-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2005/Ramsay and Silverman - 2005 - Functional Data Analysis.pdf;/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2005/Ramsay_Silverman_2005_Functional Data Analysis_outline.pdf}
}

@book{ramsayFunctionalDataAnalysis2009,
  title = {Functional {{Data Analysis}} with {{R}} and {{MATLAB}}},
  author = {Ramsay, James and Hooker, Giles and Graves, Spencer},
  year = {2009},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-98185-7},
  urldate = {2024-01-10},
  isbn = {978-0-387-98184-0 978-0-387-98185-7},
  langid = {english},
  keywords = {calculus,computer,Curve registration,data analysis,data structures,Derivative estimation,Dynamic systems,functional analysis,Functional linear model,LDA,MATLAB,Regularization},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2009/Ramsay et al_2009_Functional Data Analysis with R and MATLAB_outline.pdf;/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer/2009/Ramsay et al_2009_Functional Data Analysis with R and MATLAB.pdf}
}

@article{ramsayWhenDataAre1982,
  title = {When the Data Are Functions},
  author = {Ramsay, J. O.},
  year = {1982},
  month = dec,
  journal = {Psychometrika},
  volume = {47},
  number = {4},
  pages = {379--396},
  issn = {1860-0980},
  doi = {10.1007/BF02293704},
  urldate = {2023-09-20},
  abstract = {A datum is often a continuous functionx(t) of a variable such as time observed over some interval. One or more such functions are observed for each subject or unit of observation. The extension of classical data analytic techniques designed forp-variate observations to such data is discussed. The essential step is the expression of the classical problem in the language of functional analysis, after which the extension to functions is a straightforward matter. A schematic device called the duality diagram is a very useful tool for describing an analysis and for suggesting new possibilities. Least squares approximation, descriptive statistics, principal components analysis, and canonical correlation analysis are discussed within this broader framework.},
  langid = {english},
  keywords = {duality-diagram,functional-data-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Psychometrika/1982/Ramsay_1982_When the data are functions.pdf}
}

@article{raoModernNonlinearFunctiononfunction2023,
  title = {Modern Non-Linear Function-on-Function Regression},
  author = {Rao, Aniruddha Rajendra and Reimherr, Matthew},
  year = {2023},
  month = oct,
  journal = {Statistics and Computing},
  volume = {33},
  number = {6},
  pages = {130},
  issn = {1573-1375},
  doi = {10.1007/s11222-023-10299-z},
  urldate = {2023-10-27},
  abstract = {We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, function-on-function direct neural networks and function-on-function basis neural networks. Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.},
  langid = {english},
  keywords = {deep-learning,function-on-function-regression,functional-data-analysis,functional-neural-network},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistics and Computing/2023/Rao_Reimherr_2023_Modern non-linear function-on-function regression.pdf}
}

@article{readeSharpnessWeylEstimates1988,
  title = {On the {{Sharpness}} of {{Weyl}}'s {{Estimates}} for {{Eigenvalues}} of {{Smooth Kernels}}},
  author = {Reade, J. B.},
  year = {1988},
  month = may,
  journal = {SIAM Journal on Mathematical Analysis},
  volume = {19},
  number = {3},
  pages = {627--631},
  issn = {0036-1410},
  doi = {10.1137/0519044},
  urldate = {2023-10-24},
  abstract = {The estimate \${\textbackslash}lambda \_n = o(\{1 /n\})\$ obtained by H. Weyl (1912) for the nth largest in modulus eigenvalue \${\textbackslash}lambda \_n \$ of any symmetric Fredholm operator on \$L\^2 [0,1]\^2 \$ with kernel in \$C\^1 [0,1]\^4 \$ is shown to be best possible in the sense that for any increasing sequence \${\textbackslash}alpha \_n {\textbackslash}to {\textbackslash}infty \$ there exist such operators whose nth eigenvalue is not \$o(\{1 / \{n{\textbackslash}alpha \_n \}\})\$. The construction of the counterexample makes use of Rudin{\textendash}Shapiro polynomials. The corresponding result for positive definite operators is proved with a simpler counterexample. The methods generalise to the case \$L\^2 [0,1]\^m (m {\textbackslash}geqq 3)\$ without further difficulty.},
  keywords = {eigenvalues-estimation,operator-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/SIAM Journal on Mathematical Analysis/1988/Reade_1988_On the Sharpness of Weyl’s Estimates for Eigenvalues of Smooth Kernels, II.pdf}
}

@book{rigbyDistributionsModelingLocation2019,
  title = {Distributions for Modeling Location, Scale, and Shape: Using {{GAMLSS}} in {{R}}},
  shorttitle = {Distributions for Modeling Location, Scale, and Shape},
  author = {Rigby, Robert A. and Stasinopoulos, Dimitrios and Heller, Gillian Z. and De Bastiani, Fernanda},
  year = {2019},
  month = oct,
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton, Florida}},
  urldate = {2023-10-25},
  abstract = {This is a book about statistical distributions, their properties, and their application to modelling the dependence of the location, scale, and shape of the distribution of a response variable on explanatory variables. It will be especially useful to applied statisticians and data scientists in a wide range of application areas, and also to those interested in the theoretical properties of distributions. This book follows the earlier book `Flexible Regression and Smoothing: Using GAMLSS in R', [Stasinopoulos et al., 2017], which focused on the GAMLSS model and software. GAMLSS (the Generalized Additive Model for Location, Scale, and Shape, [Rigby and Stasinopoulos, 2005]), is a regression framework in which the response variable can have any parametric distribution and all the distribution parameters can be modelled as linear or smooth functions of explanatory variables. The current book focuses on distributions and their application.},
  isbn = {978-0-367-27884-7},
  langid = {english},
  keywords = {generalized-additive-models,r-software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman & Hall/CRC/2019/Rigby et al_2019_Distributions for modeling location, scale, and shape.pdf;/Users/steven/Zotero/storage/4R6C5CVN/4993.html}
}

@phdthesis{royerOptimaliteStatistiquePartitionnement2018,
  type = {These de Doctorat},
  title = {Optimalit{\'e} Statistique Du Partitionnement Par l'optimisation Convexe},
  author = {Royer, Martin},
  year = {2018},
  month = nov,
  urldate = {2023-10-24},
  abstract = {Ces travaux traitent de la probl{\'e}matique du partitionnement d'un ensemble d'observations ou de variables en groupes d'{\'e}l{\'e}ments similaires. Elle sert de nombreuses applications essentielles comme la classification de g{\`e}nes en biologie ou l'apprentissage automatique en analyse d'image. Les travaux mod{\'e}lisent la notion de similarit{\'e} entre {\'e}l{\'e}ments pour analyser les propri{\'e}t{\'e}s statistiques d'algorithmes de partitionnement, comme l'estimateur des K-moyennes. Ce dernier est {\'e}quivalent au maximum de vraisemblance quand les groupes consid{\'e}r{\'e}s sont homoscedastiques ; dans le cas contraire, on s'aper{\c c}oit que l'estimateur est biais{\'e}, en ce qu'il tend {\`a} s{\'e}parer les groupes ayant une plus grande dispersion. En utilisant une formulation {\'e}quivalente qui fait intervenir l'optimisation semi-d{\'e}finie positive, on propose une correction op{\'e}rationnelle de ce biais. On construit et {\'e}tudie ainsi des algorithmes de complexit{\'e} polynomiale qui sont quasi-minimax pour le partitionnement exact dans les deux contextes {\'e}tudi{\'e}s. Ces r{\'e}sultats s'interpr{\`e}tent dans le cadre de mod{\`e}les standards comme le mod{\`e}le de m{\'e}lange ou le mod{\`e}le {\`a} variables latentes, et s'{\'e}tendent {\`a} de nouveaux mod{\`e}les plus g{\'e}n{\'e}raux et plus robustes, les mod{\`e}les G-block. Les contr{\^o}les peuvent {\^e}tre adapt{\'e}s au nombre intrins{\`e}que de groupes, ainsi qu'{\`a} la dimension effective de l'espace des donn{\'e}es. Ils apportent une meilleure compr{\'e}hension d'estimateurs classiques du partitionnement comme les estimateurs spectraux. Ils sont appuy{\'e}s par des exp{\'e}riences extensives sur donn{\'e}es de synth{\`e}se, ainsi que sur des jeux de donn{\'e}es r{\'e}elles. Enfin lorsqu'on cherche {\`a} am{\'e}liorer l'efficacit{\'e} computationnelle des algorithmes {\'e}tudi{\'e}s, on peut utiliser une connexion forte avec le domaine de l'optimisation convexe et notamment exploiter des techniques de relaxation de faible rang motiv{\'e}es par des probl{\'e}matiques de grande dimension.},
  collaborator = {Giraud, Christophe and Florentina, Bunea},
  copyright = {Licence Etalab},
  school = {Universit{\'e} Paris-Saclay (ComUE)},
  keywords = {clustering,minimax-theory,optimisation,semidefinite-programming},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Université Paris-Saclay (ComUE)/2018/Royer_2018_Optimalité statistique du partitionnement par l'optimisation convexe.pdf}
}

@article{rubinSparselyObservedFunctional2020,
  title = {Sparsely Observed Functional Time Series: Estimation and Prediction},
  shorttitle = {Sparsely Observed Functional Time Series},
  author = {Rub{\'i}n, Tom{\'a}{\v s} and Panaretos, Victor M.},
  year = {2020},
  month = jan,
  journal = {Electronic Journal of Statistics},
  volume = {14},
  number = {1},
  pages = {1137--1210},
  publisher = {{Institute of Mathematical Statistics and Bernoulli Society}},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/20-EJS1690},
  urldate = {2023-09-19},
  abstract = {Functional time series analysis, whether based on time or frequency domain methodology, has traditionally been carried out under the assumption of complete observation of the constituent series of curves, assumed stationary. Nevertheless, as is often the case with independent functional data, it may well happen that the data available to the analyst are not the actual sequence of curves, but relatively few and noisy measurements per curve, potentially at different locations in each curve's domain. Under this sparse sampling regime, neither the established estimators of the time series' dynamics nor their corresponding theoretical analysis will apply. The subject of this paper is to tackle the problem of estimating the dynamics and of recovering the latent process of smooth curves in the sparse regime. Assuming smoothness of the latent curves, we construct a consistent nonparametric estimator of the series' spectral density operator and use it to develop a frequency-domain recovery approach, that predicts the latent curve at a given time by borrowing strength from the (estimated) dynamic correlations in the series across time. This new methodology is seen to comprehensively outperform a naive recovery approach that would ignore temporal dependence and use only methodology employed in the i.i.d. setting and hinging on the lag zero covariance. Further to predicting the latent curves from their noisy point samples, the method fills in gaps in the sequence (curves nowhere sampled), denoises the data, and serves as a basis for forecasting. Means of providing corresponding confidence bands are also investigated. A simulation study interestingly suggests that sparse observation for a longer time period may provide better performance than dense observation for a shorter period, in the presence of smoothness. The methodology is further illustrated by application to an environmental data set on fair-weather atmospheric electricity, which naturally leads to a sparse functional time series.},
  keywords = {functional-data-analysis,functional-time-series,non-parametric-statistics,spectral-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Electronic Journal of Statistics/2020/Rubín_Panaretos_2020_Sparsely observed functional time series.pdf}
}

@article{schmutzClusteringMultivariateFunctional2020,
  title = {Clustering Multivariate Functional Data in Group-Specific Functional Subspaces},
  author = {Schmutz, Amandine and Jacques, Julien and Bouveyron, Charles and Ch{\`e}ze, Laurence and Martin, Pauline},
  year = {2020},
  month = sep,
  journal = {Computational Statistics},
  volume = {35},
  number = {3},
  pages = {1101--1131},
  issn = {1613-9658},
  doi = {10.1007/s00180-020-00958-4},
  urldate = {2023-11-21},
  abstract = {With the emergence of numerical sensors in many aspects of everyday life, there is an increasing need in analyzing multivariate functional data. This work focuses on the clustering of such functional data, in order to ease their modeling and understanding. To this end, a novel clustering technique for multivariate functional data is presented. This method is based on a functional latent mixture model which fits the data into group-specific functional subspaces through a multivariate functional principal component analysis. A family of parsimonious models is obtained by constraining model parameters within and between groups. An Expectation Maximization algorithm is proposed for model inference and the choice of hyper-parameters is addressed through model selection. Numerical experiments on simulated datasets highlight the good performance of the proposed methodology compared to existing works. This algorithm is then applied to the analysis of the pollution in French cities for 1 year.},
  langid = {english},
  keywords = {functional-data-analysis,functional-principal-components,model-based-clustering,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics/2020/Schmutz et al_2020_Clustering multivariate functional data in group-specific functional subspaces.pdf}
}

@article{schuckersEstimationPlayerAging2023,
  title = {Estimation of Player Aging Curves Using Regression and Imputation},
  author = {Schuckers, Michael and Lopez, Michael and Macdonald, Brian},
  year = {2023},
  month = jun,
  journal = {Annals of Operations Research},
  volume = {325},
  number = {1},
  pages = {681--699},
  issn = {1572-9338},
  doi = {10.1007/s10479-022-05127-y},
  urldate = {2023-10-24},
  abstract = {The impact of age on performance is a fundamental component to models of player valuation and prediction across sport. Age effects are typically measured using age curves, which reflect the expected average performance at each age among all players that are eligible to participate. Most age curve methods, however, ignore the reality that age likewise influences which players receive opportunities to perform. In this paper we begin by highlighting how selection bias is linked to the ages in which we observe players perform. Next, using underlying distributions of how players move in and out of sport organizations, we assess the performance of various methods for age curve estimation under the selection bias of player entry and issues of small samples at younger and older ages. We propose several methods for player age curve estimation, introduce a missing data framework, and compare these new methods to more familiar approaches including both parametric and semi-parametric modeling. We then use simulations to compare several approaches for estimating aging curves. Imputation-based methods, as well as models that account for individual player skill, tend to generate lower root mean squared error (RMSE) and age curve shapes that better match the truth. We implement our approach using data from the National Hockey League. All of the data and code for this paper are available in a Github repository.},
  langid = {english},
  keywords = {generalized-additive-models,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annals of Operations Research/2023/Schuckers et al_2023_Estimation of player aging curves using regression and imputation.pdf}
}

@article{seltzSolvingPopulationBalance2021,
  title = {Solving the Population Balance Equation for Non-Inertial Particles Dynamics Using Probability Density Function and Neural Networks: {{Application}} to a Sooting Flame},
  shorttitle = {Solving the Population Balance Equation for Non-Inertial Particles Dynamics Using Probability Density Function and Neural Networks},
  author = {Seltz, Andrea and Domingo, Pascale and Vervisch, Luc},
  year = {2021},
  month = jan,
  journal = {Physics of Fluids},
  volume = {33},
  number = {1},
  pages = {013311},
  issn = {1070-6631},
  doi = {10.1063/5.0031144},
  urldate = {2023-07-08},
  abstract = {Numerical modeling of non-inertial particles dynamics is usually addressed by solving a population balance equation (PBE). In addition to space and time, a discretization is required also in the particle-size space, covering a large range of variation controlled by strongly nonlinear phenomena. A novel approach is presented in which a hybrid stochastic/fixed-sectional method solving the PBE is used to train a combination of an artificial neural network (ANN) with a convolutional neural network (CNN) and recurrent long short-term memory artificial neural layers. The hybrid stochastic/fixed-sectional method decomposes the problem into the total number density and the probability density function of sizes, allowing for an accurate treatment of surface growth/loss. After solving for the transport of species and temperature, the input of the ANN is composed of the thermochemical parameters controlling the particle physics and of the increment in time. The input of the CNN is the shape of the particle size distribution (PSD) discretized in sections of size. From these inputs, in a flow simulation, the ANN{\textendash}CNN returns the PSD shape for the subsequent time step or a source term for the Eulerian transport of the particle size density. The method is evaluated in a canonical laminar premixed sooting flame of the literature, and for a given level of accuracy (i.e., a given discretization of the size space), a significant computing cost reduction is achieved (six times faster compared to a sectional method with ten sections and 30 times faster for 100 sections).},
  keywords = {neural-network,population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Physics of Fluids/2021/Seltz et al_2021_Solving the population balance equation for non-inertial particles dynamics.pdf;/Users/steven/Zotero/storage/ALD7NR4F/Solving-the-population-balance-equation-for-non.html}
}

@article{shangFtsaPackageAnalyzing2013,
  title = {Ftsa: {{An R Package}} for {{Analyzing Functional Time Series}}},
  shorttitle = {Ftsa},
  author = {Shang, Han Lin},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {64--72},
  issn = {2073-4859},
  urldate = {2023-09-28},
  langid = {english},
  keywords = {functional-time-series,r-software,software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The R Journal/2013/Shang_2013_ftsa.pdf;/Users/steven/Zotero/storage/N7SGJN6E/index.html}
}

@article{shangGroupedFunctionalTime2017,
  title = {Grouped {{Functional Time Series Forecasting}}: {{An Application}} to {{Age-Specific Mortality Rates}}},
  shorttitle = {Grouped {{Functional Time Series Forecasting}}},
  author = {Shang, Han Lin and Hyndman, Rob J.},
  year = {2017},
  month = apr,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {2},
  pages = {330--343},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2016.1237877},
  urldate = {2023-09-19},
  abstract = {Age-specific mortality rates are often disaggregated by different attributes, such as sex, state, and ethnicity. Forecasting age-specific mortality rates at the national and sub-national levels plays an important role in developing social policy. However, independent forecasts at the sub-national levels may not add up to the forecasts at the national level. To address this issue, we consider reconciling forecasts of age-specific mortality rates, extending the methods of Hyndman et~al. in 2011 to functional time series, where age is considered as a continuum. The grouped functional time series methods are used to produce point forecasts of mortality rates that are aggregated appropriately across different disaggregation factors. For evaluating forecast uncertainty, we propose a bootstrap method for reconciling interval forecasts. Using the regional age-specific mortality rates in Japan, obtained from the Japanese Mortality Database, we investigate the one- to ten-step-ahead point and interval forecast accuracies between the independent and grouped functional time series forecasting methods. The proposed methods are shown to be useful for reconciling forecasts of age-specific mortality rates at the national and sub-national levels. They also enjoy improved forecast accuracy averaged over different disaggregation factors. Supplementary materials for the article are available online.},
  keywords = {bootstrapping,forecasting,functional-time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Computational and Graphical Statistics/2017/Shang_Hyndman_2017_Grouped Functional Time Series Forecasting.pdf}
}

@article{shiTwoDimensionalFunctionalPrincipal2022,
  title = {Two-{{Dimensional Functional Principal Component Analysis}} for {{Image Feature Extraction}}},
  author = {Shi, Haolun and Yang, Yuping and Wang, Liangliang and Ma, Da and Beg, Mirza Faisal and Pei, Jian and Cao, Jiguo},
  year = {2022},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {31},
  number = {4},
  pages = {1127--1140},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.2022.2035738},
  urldate = {2023-10-24},
  abstract = {Methodologies for functional principal component analysis are well established in the one-dimensional setting. However, for two-dimensional surfaces, for example, images, conducting functional principal component analysis is complicated and challenging, because the conventional eigendecomposition approach would require the estimation of a four-dimensional covariance function, which may incur high cost in terms of time and machine memory. To circumvent such computational difficulties, we propose a novel two-dimensional functional principal component analysis for extracting functional principal components and achieving dimensionality reduction for images. Different from the conventional eigendecomposition approach, our proposed method is based on the direct estimation of the optimal two-dimensional functional principal components via tensor product B-spline, which opens up a new avenue for estimating functional principal components. We present theoretical results that prove the consistency of the proposed approach. Our method is illustrated by analyzing brain images of subjects with the Alzheimer's Disease and the handwritten digits images. The finite sample performance of our method is further assessed with some simulation studies. Supplementary materials for this article are available online.},
  keywords = {functional-data-analysis,functional-principal-components,image-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Computational and Graphical Statistics/2022/Shi et al_2022_Two-Dimensional Functional Principal Component Analysis for Image Feature.pdf}
}

@book{silvermanDensityEstimationStatistics1986,
  title = {Density {{Estimation}} for {{Statistics}} and {{Data Analysis}}},
  author = {Silverman, Bernard W.},
  year = {1986},
  month = apr,
  publisher = {{CRC Press}},
  abstract = {Although there has been a surge of interest in density estimation in recent years, much of the published research has been concerned with purely technical matters with insufficient emphasis given to the technique's practical value. Furthermore, the subject has been rather inaccessible to the general statistician.The account presented in this book places emphasis on topics of methodological importance, in the hope that this will facilitate broader practical application of density estimation and also encourage research into relevant theoretical work. The book also provides an introduction to the subject for those with general interests in statistics. The important role of density estimation as a graphical technique is reflected by the inclusion of more than 50 graphs and figures throughout the text.Several contexts in which density estimation can be used are discussed, including the exploration and presentation of data, nonparametric discriminant analysis, cluster analysis, simulation and the bootstrap, bump hunting, projection pursuit, and the estimation of hazard rates and other quantities that depend on the density. This book includes general survey of methods available for density estimation. The Kernel method, both for univariate and multivariate data, is discussed in detail, with particular emphasis on ways of deciding how much to smooth and on computation aspects. Attention is also given to adaptive methods, which smooth to a greater degree in the tails of the distribution, and to methods based on the idea of penalized likelihood.},
  isbn = {978-0-412-24620-3},
  langid = {english},
  keywords = {Mathematics / Probability & Statistics / General},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/CRC Press/1986/Silverman_1986_Density Estimation for Statistics and Data Analysis.pdf}
}

@article{singhChallengesOpportunitiesConcerning2022,
  title = {Challenges and Opportunities Concerning Numerical Solutions for Population Balances: A Critical Review},
  shorttitle = {Challenges and Opportunities Concerning Numerical Solutions for Population Balances},
  author = {Singh, Mehakpreet and Ranade, Vivek and Shardt, Orest and Matsoukas, Themis},
  year = {2022},
  month = sep,
  journal = {Journal of Physics A: Mathematical and Theoretical},
  volume = {55},
  number = {38},
  pages = {383002},
  publisher = {{IOP Publishing}},
  issn = {1751-8121},
  doi = {10.1088/1751-8121/ac8a42},
  urldate = {2023-07-08},
  abstract = {Population balance models are tools for the study of dispersed systems, such as granular materials, polymers, colloids and aerosols. They are applied with increasing frequency across a wide range of disciplines, including chemical engineering, aerosol physics, astrophysics, polymer science, pharmaceutical sciences, and mathematical biology. Population balance models are used to track particle properties and their changes due to aggregation, fragmentation, nucleation and growth, processes that directly affect the distribution of particle sizes. The population balance equation is an integro-partial differential equation whose domain is the line of positive real numbers. This poses challenges for the stability and accuracy of the numerical methods used to solve for size distribution function and in response to these challenges several different methodologies have been developed in the literature. This review provides a critical presentation of the state of the art in numerical approaches for solving these complex models with emphasis in the algorithmic details that distinguish each methodology. The review covers finite volume methods, Monte Carlo method and sectional methods; the method of moments, another important numerical methodology, is not covered in this review.},
  langid = {english},
  keywords = {population-balance-modelling,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Physics A Mathematical and Theoretical/2022/Singh et al_2022_Challenges and opportunities concerning numerical solutions for population.pdf}
}

@article{songSparseMultivariateFunctional2022,
  title = {Sparse Multivariate Functional Principal Component Analysis},
  author = {Song, Jun and Kim, Kyongwon},
  year = {2022},
  journal = {Stat},
  volume = {11},
  number = {1},
  pages = {e435},
  issn = {2049-1573},
  doi = {10.1002/sta4.435},
  urldate = {2023-09-15},
  abstract = {We introduce a sparse multivariate functional principal component analysis method by incorporating ideas from the group sparse maximum variance method to multivariate functional data. Our method can avoid the ``curse of dimensionality'' from a high-dimensional dataset and enjoy interpretability at the same time. In particular, our unsupervised method can capture important latent factors to explain variability of the dataset, which can induce a clear distinction between important variables in the principal components and unnecessary features based on the sparseness structure. Furthermore, our method can be applied to functional data from a multidimensional domain that hinges on different intervals. In the numerical experiment, we show that our method works well in both low- and high-dimensional multivariate functional data regardless of the number and the type of basis. We further apply our method to stock market data and electroencephalography data in an alcoholism study to demonstrate the theoretical result.},
  copyright = {{\copyright} 2021 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {functional-principal-components,multivariate-functional-data,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Stat/2022/Song_Kim_2022_Sparse multivariate functional principal component analysis.pdf;/Users/steven/Zotero/storage/DACNDSUW/sta4.html}
}

@misc{sortFunctionalGeneralizedCanonical2023,
  title = {Functional {{Generalized Canonical Correlation Analysis}} for Studying Multiple Longitudinal Variables},
  author = {Sort, Lucas and Brusquet, Laurent Le and Tenenhaus, Arthur},
  year = {2023},
  month = oct,
  number = {arXiv:2310.07330},
  eprint = {2310.07330},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.07330},
  urldate = {2023-10-27},
  abstract = {In this paper, we introduce Functional Generalized Canonical Correlation Analysis (FGCCA), a new framework for exploring associations between multiple random processes observed jointly. The framework is based on the multiblock Regularized Generalized Canonical Correlation Analysis (RGCCA) framework. It is robust to sparsely and irregularly observed data, making it applicable in many settings. We establish the monotonic property of the solving procedure and introduce a Bayesian approach for estimating canonical components. We propose an extension of the framework that allows the integration of a univariate or multivariate response into the analysis, paving the way for predictive applications. We evaluate the method's efficiency in simulation studies and present a use case on a longitudinal dataset.},
  archiveprefix = {arxiv},
  keywords = {bayesian-analysis,canonical-correlation-analysis,functional-data-analysis,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Sort et al_2023_Functional Generalized Canonical Correlation Analysis for studying multiple.pdf;/Users/steven/Zotero/storage/WIFNQBGS/2310.html}
}

@article{sotolongo-costaNonextensiveStatisticalModel2015,
  title = {A Non-Extensive Statistical Model for Time-Dependent Multiple Breakage Particle-Size Distribution},
  author = {{Sotolongo-Costa}, O. and {Gaggero-Sager}, L. M. and {Mora-Ramos}, M. E.},
  year = {2015},
  month = nov,
  journal = {Physica A: Statistical Mechanics and its Applications},
  volume = {438},
  pages = {74--80},
  issn = {0378-4371},
  doi = {10.1016/j.physa.2015.06.042},
  urldate = {2023-07-08},
  abstract = {A general formulation for the statistical description of time-dependent multiple particle breakage processes is presented in terms of a purposely constructed dimensionless quantity that contains the main physical magnitudes involved in the problem. The approach combines the Tsallis non-extensive entropy with a kinetic equation with fractionary index for the time evolution of the size/mass of the fragments. The obtained distribution function is tested by fitting some experimental reports. It is found that the better adjustment corresponds, in all cases, to values of the time index equal or below 0.6, whereas the parameter of nonextensivity ranks between 1 and 2, as previously reported in other studies involving some kind of fragmentation. The work could be the first example of a non-extensive maximum-entropy statistical description based on a purposely constructed dimensionless quantity, as well as of the derivation of a fragment size distribution function explicitly dependent on measurable system variables. As a result, the role of quantities such as viscosity, velocity gradient and others becomes explicit in the formulation.},
  langid = {english},
  keywords = {breakage-equation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2015/Sotolongo-Costa et al. - 2015 - A non-extensive statistical model for time-depende.pdf;/Users/steven/Zotero/storage/Q52YSXMH/S0378437115006020.html}
}

@article{staermanFunctionalAnomalyDetection2023,
  title = {Functional Anomaly Detection: A Benchmark Study},
  shorttitle = {Functional Anomaly Detection},
  author = {Staerman, Guillaume and Adjakossa, Eric and Mozharovskyi, Pavlo and Hofer, Vera and Sen Gupta, Jayant and Cl{\'e}men{\c c}on, Stephan},
  year = {2023},
  month = jun,
  journal = {International Journal of Data Science and Analytics},
  volume = {16},
  number = {1},
  pages = {101--117},
  issn = {2364-4168},
  doi = {10.1007/s41060-022-00366-5},
  urldate = {2023-10-24},
  abstract = {The increasing automation in many areas of the Industry expressly demands to design efficient machine learning solutions for the detection of abnormal events. With the ubiquitous deployment of sensors monitoring nearly continuously the health of complex infrastructures, anomaly detection can now rely on measurements sampled at a very high frequency, providing a very rich representation of the phenomenon under surveillance. In order to exploit fully the information thus collected, the observations cannot be treated as multivariate data anymore and a functional analysis approach is required. It is the purpose of this paper to investigate the performance of recent techniques for anomaly detection in the functional setup on real datasets. After an overview of the state of the art and a visual-descriptive study, a variety of anomaly detection methods are compared. While taxonomies of abnormalities (e.g., shape, location) in the functional setup are documented in the literature, assigning a specific type to the identified anomalies appears to be a challenging task. Thus, strengths and weaknesses of the existing approaches are benchmarked in view of these highlighted types in a simulation study. Anomaly detection methods are next evaluated on two datasets, related to the monitoring of helicopters in flight and to the spectrometry of construction materials namely. The benchmark analysis is concluded by a recommendation guidance for practitioners.},
  langid = {english},
  keywords = {depth-statistics,functional-data-analysis,outliers-detection,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/International Journal of Data Science and Analytics/2023/Staerman et al_2023_Functional anomaly detection.pdf}
}

@article{staniswalisNonparametricRegressionAnalysis1998,
  title = {Nonparametric {{Regression Analysis}} of {{Longitudinal Data}}},
  author = {Staniswalis, Joan G. and Lee, J. Jack},
  year = {1998},
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {444},
  eprint = {2670055},
  eprinttype = {jstor},
  pages = {1403--1418},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2670055},
  urldate = {2023-08-24},
  abstract = {Nonparametric methods are developed for estimating the dose effect when a response consists of correlated observations over time measured in a dose-response experiment. The methods can also be applied to data collected from a completely randomized design experiment. Methods are developed for the detection and description of the effects of dose, time, and their interaction. The methods allow for individual variation in the timing and number of observations. A generalization allowing baseline covariates to be incorporated is addressed. These results may be used in an exploratory fashion in the process of building a random-effects model for longitudinal data.},
  keywords = {kernel-smoothing,longitudinal-data,non-parametric-statistics,principal-components},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/1998/Staniswalis_Lee_1998_Nonparametric Regression Analysis of Longitudinal Data.pdf}
}

@book{stasinopoulosFlexibleRegressionSmoothing2017,
  title = {Flexible {{Regression}} and {{Smoothing}}: {{Using GAMLSS}} in {{R}}},
  shorttitle = {Flexible {{Regression}} and {{Smoothing}}},
  author = {Stasinopoulos, Mikis and Rigby, Robert A. and Heller, Gillian Z. and Voudouris, Vlasios and De Bastiani, Fernanda},
  year = {2017},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/b21973},
  abstract = {This book is about learning from data using the Generalized Additive Models for Location, Scale and Shape (GAMLSS). GAMLSS extends the Generalized Linear Models (GLMs) and Generalized Additive Models (GAMs) to accommodate large complex datasets, which are increasingly prevalent. In particular, the GAMLSS statistical framework enables flexible regression and smoothing models to be fitted to the data. The GAMLSS model assumes that the response variable has any parametric (continuous, discrete or mixed) distribution which might be heavy- or light-tailed, and positively or negatively skewed. In addition, all the parameters of the distribution (location, scale, shape) can be modelled as linear or smooth functions of explanatory variables.  Key Features: Provides a broad overview of flexible regression and smoothing techniques to learn from data whilst also focusing on the practical application of methodology using GAMLSS software in R.  Includes a comprehensive collection of real data examples, which reflect the range of problems addressed by GAMLSS models and provide a practical illustration of the process of using flexible GAMLSS models for statistical learning. R code integrated into the text for ease of understanding and replication. Supplemented by a website with code, data and extra materials. This book aims to help readers understand how to learn from data encountered in many fields. It will be useful for practitioners and researchers who wish to understand and use the GAMLSS models to learn from data and also for students who wish to learn GAMLSS through practical examples.},
  isbn = {978-1-315-26987-0},
  keywords = {generalized-additive-models,r-software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2017/Bastiani_2017_Flexible Regression and Smoothing.pdf}
}

@article{stoehrDetectingChangesCovariance2021,
  title = {Detecting Changes in the Covariance Structure of Functional Time Series with Application to {{fMRI}} Data},
  author = {Stoehr, Christina and Aston, John A D and Kirch, Claudia},
  year = {2021},
  month = apr,
  journal = {Econometrics and Statistics},
  volume = {18},
  pages = {44--62},
  issn = {2452-3062},
  doi = {10.1016/j.ecosta.2020.04.004},
  urldate = {2023-09-19},
  abstract = {Functional magnetic resonance imaging (fMRI) data provides information concerning activity in the brain and in particular the interactions between brain regions. Resting state fMRI data is widely used for inferring connectivities in the brain which are not due to external factors. As such analyzes strongly rely on stationarity, change point procedures can be applied in order to detect possible deviations from this crucial assumption. FMRI data is modeled as functional time series and tools for the detection of deviations from covariance stationarity via change point alternatives are developed. A nonparametric procedure which is based on dimension reduction techniques is proposed. However, as the projection of the functional time series on a finite and rather low-dimensional subspace involves the risk of missing changes which are orthogonal to the projection space, two test statistics which take the full functional structure into account are considered. The proposed methods are compared in a simulation study and applied to more than 100 resting state fMRI data sets.},
  keywords = {change-point-analysis,dimension-reduction,functional-data-analysis,functional-magnetic-resonance-imaging,functional-time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Econometrics and Statistics/2021/Stoehr et al_2021_Detecting changes in the covariance structure of functional time series with.pdf;/Users/steven/Zotero/storage/Z9YNCZ8J/S2452306220300460.html}
}

@book{sullivanIntroductionUncertaintyQuantification2015,
  title = {Introduction to {{Uncertainty Quantification}}},
  author = {Sullivan, T.J.},
  year = {2015},
  series = {Texts in {{Applied Mathematics}}},
  volume = {63},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23395-6},
  urldate = {2023-10-25},
  isbn = {978-3-319-23394-9 978-3-319-23395-6},
  langid = {english},
  keywords = {uncertainty-quantification},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer International Publishing/2015/Sullivan_2015_Introduction to Uncertainty Quantification.pdf}
}

@article{szczesnaDatasetsLearningUnknown2023,
  title = {Datasets for Learning of Unknown Characteristics of Dynamical Systems},
  author = {Szcz{\k e}sna, Agnieszka and Augustyn, Dariusz and Har{\k e}{\.z}lak, Katarzyna and Josi{\'n}ski, Henryk and {\'S}wito{\'n}ski, Adam and Kasprowski, Pawe{\l}},
  year = {2023},
  month = feb,
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {79},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-023-01978-7},
  urldate = {2023-10-24},
  abstract = {The ability to uncover characteristics based on empirical measurement is an important step in understanding the underlying system that gives rise to an observed time series. This is especially important for biological signals whose characteristic contributes to the underlying dynamics of the physiological processes. Therefore, by studying such signals, the physiological systems that generate them can be better understood. The datasets presented consist of 33,000 time series of 15 dynamical systems (five chaotic and ten non-chaotic) of the first, second, or third order. Here, the order of a dynamical system means its dimension. The non-chaotic systems were divided into the following classes: periodic, quasi-periodic, and non-periodic. The aim is to propose datasets for machine learning methods, in particular deep learning techniques, to analyze unknown dynamical system characteristics based on obtained time series. In technical validation, three classifications experiments were conducted using two types of neural networks with long short-term memory modules and convolutional layers.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {datasets,time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Scientific Data/2023/Szczęsna et al_2023_Datasets for learning of unknown characteristics of dynamical systems.pdf}
}

@article{tavakoliDetectingLocalizingDifferences2016,
  title = {Detecting and {{Localizing Differences}} in {{Functional Time Series Dynamics}}: {{A Case Study}} in {{Molecular Biophysics}}},
  shorttitle = {Detecting and {{Localizing Differences}} in {{Functional Time Series Dynamics}}},
  author = {Tavakoli, Shahin and Panaretos, Victor M.},
  year = {2016},
  month = jul,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {515},
  pages = {1020--1035},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1147355},
  urldate = {2023-09-19},
  abstract = {Motivated by the problem of inferring the molecular dynamics of DNA in solution, and linking them with its base-pair composition, we consider the problem of comparing the dynamics of functional time series (FTS), and of localizing any inferred differences in frequency and along curvelength. The approach we take is one of Fourier analysis, where the complete second-order structure of the FTS is encoded by its spectral density operator, indexed by frequency and curvelength. The comparison is broken down to a hierarchy of stages: at a global level, we compare the spectral density operators of the two FTS, across frequencies and curvelength, based on a Hilbert{\textendash}Schmidt criterion; then, we localize any differences to specific frequencies; and, finally, we further localize any differences along the length of the random curves, that is, in physical space. A hierarchical multiple testing approach guarantees control of the averaged false discovery rate over the selected frequencies. In this sense, we are able to attribute any differences to distinct dynamic (frequency) and spatial (curvelength) contributions. Our approach is presented and illustrated by means of a case study in molecular biophysics: how can one use molecular dynamics simulations of short strands of DNA to infer their temporal dynamics at the scaling limit, and probe whether these depend on the sequence encoded in these strands? Supplementary materials for this article are available online.},
  keywords = {biophysics,functional-data-analysis,functional-time-series,spectral-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2016/Tavakoli_Panaretos_2016_Detecting and Localizing Differences in Functional Time Series Dynamics.pdf}
}

@article{tavenardTslearnMachineLearning2020,
  title = {Tslearn, {{A Machine Learning Toolkit}} for {{Time Series Data}}},
  author = {Tavenard, Romain and Faouzi, Johann and Vandewiele, Gilles and Divo, Felix and Androz, Guillaume and Holtz, Chester and Payne, Marie and Yurchak, Roman and Ru{\ss}wurm, Marc and Kolar, Kushal and Woods, Eli},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {118},
  pages = {1--6},
  issn = {1533-7928},
  urldate = {2024-01-10},
  abstract = {tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Machine Learning Research/2020/Tavenard et al_2020_Tslearn, A Machine Learning Toolkit for Time Series Data.pdf;/Users/steven/Zotero/storage/7XFUK947/tslearn.html}
}

@phdthesis{thomasEconometricsEnergyDemand2020,
  type = {These de Doctorat},
  title = {The {{Econometrics}} of {{Energy Demand}} : Identification and {{Forecast}}},
  shorttitle = {The {{Econometrics}} of {{Energy Demand}}},
  author = {Thomas, Arthur},
  year = {2020},
  month = dec,
  urldate = {2023-10-25},
  abstract = {La pr{\'e}vention du changement climatique est l'une des priorit{\'e}s de la politique {\'e}nerg{\'e}tique mondiale qui vise {\`a} r{\'e}duire massivement les {\'e}missions de gaz {\`a} effet de serre. Face {\`a} ces d{\'e}fis, il est frappant de constater que notre connaissance de la mod{\'e}lisation de la demande {\'e}nerg{\'e}tique demeure imparfaite car elle repose en grande partie sur des travaux empiriques anciens et des m{\'e}thodologies aujourd'hui d{\'e}pass{\'e}es. L'objectif scientifique de cette th{\`e}se est double : analyser quantitativement les d{\'e}terminants {\'e}conomiques de la demande {\'e}nerg{\'e}tique et d{\'e}velopper de nouveaux mod{\`e}les de pr{\'e}vision. Cette th{\`e}se est structur{\'e}e en quatre chapitres. Le premier chapitre montre que la consommation de gaz naturel en France peut {\^e}tre pr{\'e}dite {\`a} l'aide d'un mod{\`e}le simple utilisant seulement les informations disponibles pour les acteurs du march{\'e}. Ce chapitre prouve l'existence d'une relation {\`a} long terme entre la demande de gaz naturel et les prix des autres {\'e}nergies et il fournit des estimations de leurs impacts marginaux sur les niveaux de demande observ{\'e}s. Le deuxi{\`e}me chapitre {\'e}tudie empiriquement le r{\^o}le de la temp{\'e}rature dans la pr{\'e}vision des prix du gaz aux {\'E}tats-Unis. Il d{\'e}veloppe une m{\'e}thodologie de construction d'un nouvel indice mensuel bas{\'e} sur la temp{\'e}rature. Cet indice capture les variations de la demande r{\'e}siduelle de gaz naturel en temps r{\'e}el. Il est utilis{\'e} comme variable exog{\`e}ne suppl{\'e}mentaire dans des mod{\`e}les structurels VAR afin d'am{\'e}liorer les pr{\'e}visions ; et nous montrons que ces mod{\`e}les pr{\'e}dictifs d{\'e}riv{\'e}s de mod{\`e}les structurels sont am{\'e}lior{\'e}s en s'appuyant sur des donn{\'e}es en temps r{\'e}elles (non sujettes {\`a} r{\'e}vision). Le troisi{\`e}me chapitre propose d'utiliser dans le cas du p{\'e}trole, un mod{\`e}le structurel capturant les anticipations {\`a} l'aide de VAR non causaux et d'identifier correctement les r{\'e}actions des variables cl{\'e}s du p{\'e}trole {\`a} un choc d'actualit{\'e}. Le quatri{\`e}me chapitre r{\'e}examine le pouvoir pr{\'e}dictif de la structure par terme des prix, dite {\guillemotleft} convenience yield {\guillemotright}, du p{\'e}trole et du gaz en int{\'e}grant les anticipations dans une sp{\'e}cification empirique, par le biais d'un VAR non causal bas{\'e} sur la th{\'e}orie du stockage qui fournit des pr{\'e}visions de prix tr{\`e}s comp{\'e}titives dans un cadre bivari{\'e} simple.},
  collaborator = {S{\'e}vi, Beno{\^i}t and Massol, Olivier},
  copyright = {Licence Etalab},
  school = {Nantes},
  keywords = {bayesian-analysis,climate-science,econometrics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Nantes/2020/Thomas_2020_The Econometrics of Energy Demand.pdf}
}

@article{tibshiraniEstimatingNumberClusters2001,
  title = {Estimating the Number of Clusters in a Data Set via the Gap Statistic},
  author = {Tibshirani, Robert and Walther, Guenther and Hastie, Trevor},
  year = {2001},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {63},
  number = {2},
  pages = {411--423},
  issn = {1467-9868},
  doi = {10.1111/1467-9868.00293},
  urldate = {2023-07-08},
  abstract = {We propose a method (the `gap statistic') for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution. Some theory is developed for the proposal and a simulation study shows that the gap statistic usually outperforms other methods that have been proposed in the literature.},
  copyright = {2001 Royal Statistical Society},
  langid = {english},
  keywords = {clustering},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society Series B (Statistical Methodology)/2001/Tibshirani et al_2001_Estimating the number of clusters in a data set via the gap statistic.pdf;/Users/steven/Zotero/storage/KU3F2Z9H/1467-9868.html}
}

@inproceedings{tiwariBanditPAMAlmostLinear2020,
  title = {{{BanditPAM}}: {{Almost Linear Time}} k-{{Medoids Clustering}} via {{Multi-Armed Bandits}}},
  shorttitle = {{{BanditPAM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tiwari, Mo and Zhang, Martin J and Mayclin, James and Thrun, Sebastian and Piech, Chris and Shomorony, Ilan},
  year = {2020},
  volume = {33},
  pages = {10211--10222},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-10-24},
  abstract = {Clustering is a ubiquitous task in data science. Compared to the commonly used k-means clustering, k-medoids clustering requires the cluster centers to be actual data points and supports arbitrary distance metrics, which permits greater interpretability and the clustering of structured objects. Current state-of-the-art k-medoids clustering algorithms, such as Partitioning Around Medoids (PAM), are iterative and are quadratic in the dataset size n for each iteration, being prohibitively expensive for large datasets. We propose BanditPAM, a randomized algorithm inspired by techniques from multi-armed bandits, that reduces the complexity of each PAM iteration from O(n\^2) to O(nlogn) and returns the same results with high probability, under assumptions on the data that often hold in practice. As such, BanditPAM matches state-of-the-art clustering loss while reaching solutions much faster. We empirically validate our results on several large real-world datasets, including a coding exercise submissions dataset from Code.org, the 10x Genomics 68k PBMC single-cell RNA sequencing dataset, and the MNIST handwritten digits dataset. In these experiments, we observe that BanditPAM returns the same results as state-of-the-art PAM-like algorithms up to 4x faster while performing up to 200x fewer distance computations. The improvements demonstrated by BanditPAM enable k-medoids clustering on a wide range of applications, including identifying cell types in large-scale single-cell data and providing scalable feedback for students learning computer science online. We also release highly optimized Python and C++ implementations of our algorithm.},
  keywords = {bandits-theory,clustering,python-software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Curran Associates, Inc./2020/Tiwari et al_2020_BanditPAM.pdf}
}

@misc{togeliusChooseYourWeapon2023,
  title = {Choose {{Your Weapon}}: {{Survival Strategies}} for {{Depressed AI Academics}}},
  shorttitle = {Choose {{Your Weapon}}},
  author = {Togelius, Julian and Yannakakis, Georgios N.},
  year = {2023},
  month = mar,
  number = {arXiv:2304.06035},
  eprint = {2304.06035},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.06035},
  urldate = {2023-10-26},
  abstract = {Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI advancements? Do you feel you have no (or very limited) access to the computational and human resources required for an AI research breakthrough? You are not alone; we feel the same way. A growing number of AI academics can no longer find the means and resources to compete at a global scale. This is a somewhat recent phenomenon, but an accelerating one, with private actors investing enormous compute resources into cutting edge AI research. Here, we discuss what you can do to stay competitive while remaining an academic. We also briefly discuss what universities and the private sector could do improve the situation, if they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of them, but it serves to start a discussion.},
  archiveprefix = {arxiv},
  keywords = {artificial-intelligence,research},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Togelius_Yannakakis_2023_Choose Your Weapon.pdf;/Users/steven/Zotero/storage/LVAD8G5V/2304.html}
}

@book{tsybakovIntroductionNonparametricEstimation2008,
  title = {Introduction to {{Nonparametric Estimation}}},
  author = {Tsybakov, Alexandre B.},
  year = {2008},
  month = oct,
  publisher = {{Springer Series in Statistics}},
  abstract = {Developed from lecture notes and ready to be used for a course on the graduate level, this concise text aims to introduce the fundamental concepts of nonparametric estimation theory while maintaining the exposition suitable for a first approach in the field.},
  googlebooks = {mwB8rUBsbqoC},
  isbn = {978-0-387-79052-7},
  langid = {english},
  keywords = {non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Springer Science & Business Media/2008/Tsybakov - 2008 - Introduction to Nonparametric Estimation.pdf}
}

@article{tuckerElasticFunctionalChangepoint,
  title = {Elastic Functional Changepoint Detection of Climate Impacts from Localized Sources},
  author = {Tucker, J. Derek and Yarger, Drew},
  journal = {Environmetrics},
  volume = {n/a},
  number = {n/a},
  pages = {e2826},
  issn = {1099-095X},
  doi = {10.1002/env.2826},
  urldate = {2023-10-27},
  abstract = {Detecting changepoints in functional data has become an important problem as interest in monitoring of climate phenomenon has increased, where the data is functional in nature. The observed data often contains both amplitude (y{\textbackslash} y {\textbackslash}-axis) and phase (x{\textbackslash} x {\textbackslash}-axis) variability. If not accounted for properly, true changepoints may be undetected, and the estimated underlying mean change functions will be incorrect. In this article, an elastic functional changepoint method is developed which properly accounts for these types of variability. The method can detect amplitude and phase changepoints which current methods in the literature do not, as they focus solely on the amplitude changepoint. This method can easily be implemented using the functions directly or can be computed via functional principal component analysis to ease the computational burden. We apply the method and its nonelastic competitors to both simulated data and observed data to show its efficiency in handling data with phase variation with both amplitude and phase changepoints. We use the method to evaluate potential changes in stratospheric temperature due to the eruption of Mt. Pinatubo in the Philippines in June 1991. Using an epidemic changepoint model, we find evidence of a increase in stratospheric temperature during a period that contains the immediate aftermath of Mt. Pinatubo, with most detected changepoints occurring in the tropics as expected.},
  copyright = {{\copyright} 2023 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {change-point-analysis,functional-data-analysis,functional-principal-components},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Environmetrics/undefined/Tucker_Yarger_Elastic functional changepoint detection of climate impacts from localized.pdf;/Users/steven/Zotero/storage/4B458IKZ/env.html}
}

@misc{tuckerFdasrvfElasticFunctional2023,
  title = {Fdasrvf: {{Elastic Functional Data Analysis}}},
  shorttitle = {Fdasrvf},
  author = {Tucker, J. Derek and Stamm, Aymeric},
  year = {2023},
  month = dec,
  urldate = {2024-01-10},
  abstract = {Performs alignment, PCA, and modeling of multidimensional and unidimensional functions using the square-root velocity framework (Srivastava et al., 2011 {$<$}arXiv:1103.3817{$>$} and Tucker et al., 2014 {$<$}doi:10.1016/j.csda.2012.12.001{$>$}). This framework allows for elastic analysis of functional data through phase and amplitude separation.},
  copyright = {GPL-3},
  keywords = {FunctionalData},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2023/Tucker_Stamm_2023_fdasrvf.pdf}
}

@book{tufteCognitiveStylePowerPoint2003,
  title = {The {{Cognitive Style}} of {{PowerPoint}}},
  author = {Tufte, Edward R.},
  year = {2003},
  publisher = {{Graphics Press}},
  abstract = {"Compares PowerPoint with alternative methods for presenting information: 10 case studies, an unbiased collection of 2,000 PP slides, and 32 control samples from non-PP presentations. The evidence indicates that PowerPoint, compared to other common presentation tools, reduces the analytical quality of serious presentations of evidence"--Page 2.},
  googlebooks = {3oNRAAAAMAAJ},
  isbn = {978-0-9613921-5-4},
  langid = {english},
  keywords = {data-visualisation},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Graphics Press/2003/Tufte_2003_The Cognitive Style of PowerPoint.pdf}
}

@book{tufteVisualDisplayQuantitative2001,
  title = {The Visual Display of Quantitative Information},
  author = {Tufte, Edward R.},
  year = {2001},
  edition = {Second edition},
  publisher = {{Graphics Press}},
  address = {{Cheshire, Conn.}},
  abstract = {"This book deals with the theory and practice in the design of data graphics and makes the point that the most effective way to describe, explore, and summarize a set of numbers is to look at pictures of those numbers, through the use of statistical graphics, charts, and tables. It includes 250 illustrations of the best (and a few of the worst) statistical graphics, with detailed analysis of how to display data for precise, effective, quick analysis. Also offered is information on the design of the high-resolution displays, small multiples, editing and improving graphics, and the data-ink ratio. Time-series, relational graphics, data maps, multivariate designs, as well as detection of graphical deception: design variation vs. data variation, and sources of deception are discussed. Information on aesthetics and data graphical displays is included. The 2nd edition provides high-resolution color reproductions of the many graphics of William Playfair (1750-1800), adds color to other images where appropriate, and includes all the changes and corrections during the 17 printings of the 1st edition"--Publisher's description},
  isbn = {978-0-9613921-4-7},
  langid = {english},
  keywords = {data-visualisation},
  annotation = {OCLC: 46932988},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Graphics Press/2001/Tufte_2001_The visual display of quantitative information.pdf}
}

@techreport{vanrossumPEP8StyleGuide2001,
  type = {{{PEP}}},
  title = {{{PEP8}} - {{Style}} Guide for {{Python}} Code},
  author = {{van Rossum}, Guido and Warsaw, Barry and Coghlan, Alyssa},
  year = {2001},
  number = {8},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/undefined/2001/van Rossum et al_2001_PEP8 - Style guide for Python code.pdf}
}

@book{vershyninHighDimensionalProbabilityIntroduction2018,
  title = {High-{{Dimensional Probability}}: {{An Introduction}} with {{Applications}} in {{Data Science}}: 47},
  shorttitle = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  year = {2018},
  month = sep,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge New York, NY Port Melbourne, VIC New Delhi Singapore}},
  abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
  isbn = {978-1-108-41519-4},
  langid = {english},
  keywords = {data-science-theory,probability-theory},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Cambridge University Press/2018/Vershynin_2018_High-Dimensional Probability.pdf}
}

@article{vonluxburgTutorialSpectralClustering2007,
  title = {A Tutorial on Spectral Clustering},
  author = {{von Luxburg}, Ulrike},
  year = {2007},
  month = dec,
  journal = {Statistics and Computing},
  volume = {17},
  number = {4},
  pages = {395--416},
  issn = {1573-1375},
  doi = {10.1007/s11222-007-9033-z},
  urldate = {2023-11-30},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  langid = {english},
  keywords = {clustering,graphs,spectral-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Statistics and Computing/2007/von Luxburg_2007_A tutorial on spectral clustering.pdf}
}

@article{wangFaultDetectionBatch2015,
  title = {Fault Detection of Batch Processes Based on Multivariate Functional Kernel Principal Component Analysis},
  author = {Wang, Huangang and Yao, Ma},
  year = {2015},
  month = dec,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {149},
  pages = {78--89},
  issn = {0169-7439},
  doi = {10.1016/j.chemolab.2015.09.018},
  urldate = {2023-11-30},
  abstract = {In some batch processes, the variables' trajectories often show obviously functional nature and can be considered as smooth functions rather than just vectors, then the collected three-way data array essentially can be transformed into a two-way function matrix. For this purpose, the approach of functional data analysis (FDA) is introduced to express the variables' functions in this article, which can highlight the subtle differences in the shape of variables' trajectories between normal batches and faulty ones, and can also easily handle the problems of irregular three-way data array, such as unequal batch length, different sampling rates, missing data, etc. Based on the function matrix, a novel monitoring method called multivariate functional kernel principal component analysis (MFKPCA) is proposed for fault detection of batch processes, which directly eigen-decomposes the two-way function matrix in the function space and uses the kernel trick to handle the nonlinear correlations. Different from the traditional PCA method, MFKPCA designs three complementary control charts for batch process monitoring based on three statistics including Hotelling's T2, squared prediction error (SPE) and mean squared error (MSE). Finally, some simulations and an industrial semiconductor etch process are used to illustrate the validity of the proposed monitoring method.},
  keywords = {fault-detection,functional-data-analysis,functional-principal-components,kernel-models,multivariate-functional-data,process-monitoring},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chemometrics and Intelligent Laboratory Systems/2015/Wang_Yao_2015_Fault detection of batch processes based on multivariate functional kernel.pdf;/Users/steven/Zotero/storage/ZTEMZ53E/S0169743915002439.html}
}

@article{wangFunctionalDataAnalysis2016,
  title = {Functional {{Data Analysis}}},
  author = {Wang, Jane-Ling and Chiou, Jeng-Min and M{\"u}ller, Hans-Georg},
  year = {2016},
  journal = {Annual Review of Statistics and Its Application},
  volume = {3},
  number = {1},
  pages = {257--295},
  doi = {10.1146/annurev-statistics-041715-033624},
  urldate = {2023-09-20},
  abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. These are both examples of functional data, which has become a commonly encountered type of data. Functional data analysis (FDA) encompasses the statistical methodology for such data. Broadly interpreted, FDA deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of FDA, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is functional principal component analysis (FPCA). FPCA is an important dimension reduction tool, and in sparse data situations it can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classification of functional data. Beyond linear and single- or multiple- index methods, we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models and models that feature time warping, manifold learning, and empirical differential equations. The paper concludes with a brief discussion of future directions.},
  keywords = {classification,clustering,functional-principal-components,functional-regression,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Annual Review of Statistics and Its Application/2016/Wang et al_2016_Functional Data Analysis.pdf}
}

@article{wangFunctionalReproducingKernel2019,
  title = {Functional Reproducing Kernel {{Hilbert}} Spaces for Non-Point-Evaluation Functional Data},
  author = {Wang, Rui and Xu, Yuesheng},
  year = {2019},
  month = may,
  journal = {Applied and Computational Harmonic Analysis},
  volume = {46},
  number = {3},
  pages = {569--623},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2017.07.003},
  urldate = {2023-10-25},
  abstract = {Motivated by the need of processing non-point-evaluation functional data, we introduce the notion of functional reproducing kernel Hilbert spaces (FRKHSs). This space admits a unique functional reproducing kernel which reproduces a family of continuous linear functionals on the space. The theory of FRKHSs and the associated functional reproducing kernels are established. A special class of FRKHSs, which we call the perfect FRKHSs, are studied, which reproduce the family of the standard point-evaluation functionals and at the same time another different family of continuous linear (non-point-evaluation) functionals. The perfect FRKHSs are characterized in terms of features, especially for those with respect to integral functionals. In particular, several specific examples of the perfect FRKHSs are presented. We apply the theory of FRKHSs to sampling and regularized learning, where non-point-evaluation functional data are used. Specifically, a general complete reconstruction formula from linear functional values is established in the framework of FRKHSs. The average sampling and the reconstruction of vector-valued functions are considered in specific FRKHSs. We also investigate in the FRKHS setting the regularized learning schemes, which learn a target element from non-point-evaluation functional data. The desired representer theorems of the learning problems are established to demonstrate the key roles played by the FRKHSs and the functional reproducing kernels in machine learning from non-point-evaluation functional data. We finally illustrate that the continuity of linear functionals, used to obtain the non-point-evaluation functional data, on an FRKHS is necessary for the stability of the numerical reconstruction algorithm using the data.},
  keywords = {functional-data-analysis,non-point-evaluation-functional-data,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Applied and Computational Harmonic Analysis/2019/Wang_Xu_2019_Functional reproducing kernel Hilbert spaces for non-point-evaluation.pdf;/Users/steven/Zotero/storage/84FZHR2A/S106352031730074X.html}
}

@article{wangLowRankCovarianceFunction2022,
  title = {Low-{{Rank Covariance Function Estimation}} for {{Multidimensional Functional Data}}},
  author = {Wang, Jiayi and Wong, Raymond K. W. and Zhang, Xiaoke},
  year = {2022},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {538},
  pages = {809--822},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1820344},
  urldate = {2023-10-27},
  abstract = {Multidimensional function data arise from many fields nowadays. The covariance function plays an important role in the analysis of such increasingly common data. In this article, we propose a novel nonparametric covariance function estimation approach under the framework of reproducing kernel Hilbert spaces (RKHS) that can handle both sparse and dense functional data. We extend multilinear rank structures for (finite-dimensional) tensors to functions, which allow for flexible modeling of both covariance operators and marginal structures. The proposed framework can guarantee that the resulting estimator is automatically semipositive definite, and can incorporate various spectral regularizations. The trace-norm regularization in particular can promote low ranks for both covariance operator and marginal structures. Despite the lack of a closed form, under mild assumptions, the proposed estimator can achieve unified theoretical results that hold for any relative magnitudes between the sample size and the number of observations per sample field, and the rate of convergence reveals the phase-transition phenomenon from sparse to dense functional data. Based on a new representer theorem, an ADMM algorithm is developed for the trace-norm regularization. The appealing numerical performance of the proposed estimator is demonstrated by a simulation study and the analysis of a dataset from the Argo project. Supplementary materials for this article are available online.},
  keywords = {covariance-operator-estimation,functional-data-analysis,multidimensional-functional-data,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2022/Wang et al_2022_Low-Rank Covariance Function Estimation for Multidimensional Functional Data.pdf}
}

@article{wangNewMethodSolving2020,
  title = {A New Method for Solving Population Balance Equations Using a Radial Basis Function Network},
  author = {Wang, Kaiyuan and Yu, Suyuan and Peng, Wei},
  year = {2020},
  month = jun,
  journal = {Aerosol Science and Technology},
  volume = {54},
  number = {6},
  pages = {644--655},
  publisher = {{Taylor \& Francis}},
  issn = {0278-6826},
  doi = {10.1080/02786826.2019.1711358},
  urldate = {2023-07-08},
  abstract = {This study presents a new method for solving the population balance equation (PBE) for particle coagulation. The method introduces a radial basis function network to approximate the number density function. The approximate solution should satisfy the PBE at the collocation points in a continuous manner. The coagulation terms in the PBE are directly handled using the Gaussian quadrature techniques. The final solution is a bivariant analytical function of particle volume and time, which can be easily used in any subsequent calculation. Then the method is validated by comparing with analytical solutions and the sectional method for four numerical test problems. These problems are selected to vary in coagulation kernels and initial conditions. The comparative results show that the present method can accurately predict the time evolution of the number density function. Moreover, the computational efficiency of the present method is quite acceptable considering the high accuracy.Copyright {\copyright} 2020 American Association for Aerosol Research},
  keywords = {population-balance-modelling},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Aerosol Science and Technology/2020/Wang et al_2020_A new method for solving population balance equations using a radial basis.pdf}
}

@article{warmenhovenBivariateFunctionalPrincipal2019,
  title = {Bivariate Functional Principal Components Analysis: Considerations for Use with Multivariate Movement Signatures in Sports Biomechanics},
  shorttitle = {Bivariate Functional Principal Components Analysis},
  author = {Warmenhoven, John and Cobley, Stephen and Draper, Conny and Harrison, Andrew and Bargary, Norma and Smith, Richard},
  year = {2019},
  month = jan,
  journal = {Sports Biomechanics},
  volume = {18},
  number = {1},
  pages = {10--27},
  publisher = {{Routledge}},
  issn = {1476-3141},
  doi = {10.1080/14763141.2017.1384050},
  urldate = {2023-09-15},
  abstract = {Sporting performance is often investigated through graphical observation of key technical variables that are representative of whole movements. The presence of differences between athletes in such variables has led to terms such as movement signatures being used. These signatures can be multivariate (multiple time-series observed concurrently), and also be composed of variables measured relative to different scales. Analytical techniques from areas of statistics such as Functional Data Analysis (FDA) present a practical alternative for analysing multivariate signatures. When applied to concurrent bivariate time-series multivariate functional principal components analysis (referred to as bivariate fPCA or bfPCA in this paper) has demonstrated preliminary application in biomechanical contexts. Despite this, given the infancy of bfPCA in sports biomechanics there are still necessary considerations for its use with non-conventional or complex bivariate structures. This paper focuses on the application of bfPCA to the force-angle graph in on-water rowing, which is a bivariate structure composed of variables with different units. A normalisation approach is proposed to investigate and standardise differences in variability between the two variables. The results of bfPCA applied to the non-normalised data and normalised data are then compared. Considerations and recommendations for the application of bfPCA in this context are also provided.},
  pmid = {29125036},
  keywords = {biomechanics,functional-data-analysis,rowing,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Sports Biomechanics/2019/Warmenhoven et al_2019_Bivariate functional principal components analysis.pdf}
}

@article{websterImproveWeatherForecasts2013,
  title = {Improve Weather Forecasts for the Developing World},
  author = {Webster, Peter J.},
  year = {2013},
  month = jan,
  journal = {Nature},
  volume = {493},
  number = {7430},
  pages = {17--19},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/493017a},
  urldate = {2023-10-02},
  abstract = {Global prediction partnerships would cost little and reduce the regional carnage caused by floods, droughts and tropical cyclones, argues Peter J. Webster.},
  copyright = {2013 Springer Nature Limited},
  langid = {english},
  keywords = {climate-science,developing-world,policy},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Nature/2013/Webster_2013_Improve weather forecasts for the developing world.pdf}
}

@misc{wenFactorguidedFunctionalPCA2022,
  title = {Factor-Guided Functional {{PCA}} for High-Dimensional Functional Data},
  author = {Wen, Shoudao and Lin, Huazhen},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12012},
  eprint = {2211.12012},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12012},
  urldate = {2023-10-24},
  abstract = {The literature on high-dimensional functional data focuses on either the dependence over time or the correlation among functional variables. In this paper, we propose a factor-guided functional principal component analysis (FaFPCA) method to consider both temporal dependence and correlation of variables so that the extracted features are as sufficient as possible. In particular, we use a factor process to consider the correlation among high-dimensional functional variables and then apply functional principal component analysis (FPCA) to the factor processes to address the dependence over time. Furthermore, to solve the computational problem arising from triple-infinite dimensions, we creatively build some moment equations to estimate loading, scores and eigenfunctions in closed form without rotation. Theoretically, we establish the asymptotical properties of the proposed estimator. Extensive simulation studies demonstrate that our proposed method outperforms other competitors in terms of accuracy and computational cost. The proposed method is applied to analyze the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, resulting in higher prediction accuracy and 41 important ROIs that are associated with Alzheimer's disease, 23 of which have been confirmed by the literature.},
  archiveprefix = {arxiv},
  keywords = {factor-analysis,functional-data-analysis,functional-principal-components,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2022/Wen_Lin_2022_Factor-guided functional PCA for high-dimensional functional data.pdf;/Users/steven/Zotero/storage/JRXXC7UN/2211.html}
}

@article{wickhamTidyData2014,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  month = sep,
  journal = {Journal of Statistical Software},
  volume = {59},
  pages = {1--23},
  issn = {1548-7660},
  doi = {10.18637/jss.v059.i10},
  urldate = {2023-10-25},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
  copyright = {Copyright (c) 2013 Hadley  Wickham},
  langid = {english},
  keywords = {data-cleaning,r-software},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Statistical Software/2014/Wickham_2014_Tidy Data.pdf}
}

@article{williamsInfluenceMatchExposure2023,
  title = {The Influence of Match Exposure on Injury Risk in Elite Men's Rugby Union},
  author = {Williams, Sean and Kay, Ella and Bryan, Richard and Lambert, Mark and Cross, Matthew and West, Stephen W. and Kemp, Simon and Stokes, Keith A.},
  year = {2023},
  month = jan,
  journal = {Journal of Science and Medicine in Sport},
  volume = {26},
  number = {1},
  pages = {25--30},
  issn = {1878-1861},
  doi = {10.1016/j.jsams.2022.10.016},
  abstract = {OBJECTIVES: To investigate the influence of previous season match exposure on injury incidence and burden in elite men's rugby union. DESIGN: A three-season (2016-17 to 2018-19) retrospective cohort design was used to collect and analyse injury and exposure data across English Premiership rugby union teams. METHODS: Generalised linear mixed-effects models were used to model the influence of match exposure (all match involvements, match involvements of {$\geq$}20 mins, and full-game equivalents) upon match and training injury incidence and burden in the following season. RESULTS: Involvement in {$\geq$}31 matches within a season was associated with substantially increased match and training injury burden in the following season. Match exposure was not clearly associated with injury incidence in the following season. The increased match injury burden associated with higher match involvements appeared to be driven by an increased risk for older ({$>$}26 y) Forwards, whilst the increased training injury burden associated with higher match involvements appeared to be driven by an increased risk for older ({$>$}26 y) Backs. CONCLUSIONS: The present study demonstrates that all match involvements, regardless of duration, should be considered when exploring associations between match exposure and injury risk. High match involvements ({$\geq$} 31 matches) are associated with elevated injury burden, in both matches and training, in the following season. The physical and psychological load of players with high previous-season match exposure should be carefully managed.},
  langid = {english},
  pmid = {36371396},
  keywords = {injury-risk,rugby,sport-science},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Science and Medicine in Sport/2023/Williams et al_2023_The influence of match exposure on injury risk in elite men's rugby union.pdf}
}

@article{wongNonparametricOperatorregularizedCovariance2019,
  title = {Nonparametric Operator-Regularized Covariance Function Estimation for Functional Data},
  author = {Wong, Raymond K. W. and Zhang, Xiaoke},
  year = {2019},
  month = mar,
  journal = {Computational Statistics \& Data Analysis},
  series = {High-Dimensional and Functional Data Analysis},
  volume = {131},
  pages = {131--144},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2018.05.013},
  urldate = {2023-10-25},
  abstract = {In functional data analysis (FDA), the covariance function is fundamental not only as a critical quantity for understanding elementary aspects of functional data but also as an indispensable ingredient for many advanced FDA methods. A new class of nonparametric covariance function estimators in terms of various spectral regularizations of an operator associated with a reproducing kernel Hilbert space is developed. Despite their nonparametric nature, the covariance estimators are automatically positive semi-definite, which is an essential property of covariance functions, via a one-step procedure. An unconventional representer theorem is established to provide a finite dimensional representation for this class of covariance estimators based on data, although the solutions are searched over infinite dimensional functional spaces. To further achieve a low-rank representation, another desirable property, e.g., for dimension reduction and easy interpretation, the trace-norm regularization is particularly studied, under which an efficient algorithm is developed based on the accelerated proximal gradient method. The outstanding practical performance of the trace-norm-regularized covariance estimator is demonstrated by a simulation study and the analysis of a traffic dataset. Under both fixed and random designs, an excellent rate of convergence is established for a broad class of operator-regularized covariance function estimators, which generalizes both the trace-norm-regularized covariance estimator and other popular alternatives.},
  keywords = {covariance-operator-estimation,functional-data-analysis,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2019/Wong_Zhang_2019_Nonparametric operator-regularized covariance function estimation for2.pdf;/Users/steven/Zotero/storage/N3S4G42L/S0167947318301221.html}
}

@article{wongPartiallyLinearFunctional2019,
  title = {Partially {{Linear Functional Additive Models}} for {{Multivariate Functional Data}}},
  author = {Wong, Raymond K. W. and Li, Yehua and Zhu, Zhengyuan},
  year = {2019},
  month = jan,
  journal = {Journal of the American Statistical Association},
  volume = {114},
  number = {525},
  pages = {406--418},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2017.1411268},
  urldate = {2023-10-27},
  abstract = {We investigate a class of partially linear functional additive models (PLFAM) that predicts a scalar response by both parametric effects of a multivariate predictor and nonparametric effects of a multivariate functional predictor. We jointly model multiple functional predictors that are cross-correlated using multivariate functional principal component analysis (mFPCA), and model the nonparametric effects of the principal component scores as additive components in the PLFAM. To address the high-dimensional nature of functional data, we let the number of mFPCA components diverge to infinity with the sample size, and adopt the component selection and smoothing operator (COSSO) penalty to select relevant components and regularize the fitting. A fundamental difference between our framework and the existing high-dimensional additive models is that the mFPCA scores are estimated with error, and the magnitude of measurement error increases with the order of mFPCA. We establish the asymptotic convergence rate for our estimator, while allowing the number of components diverge. When the number of additive components is fixed, we also establish the asymptotic distribution for the partially linear coefficients. The practical performance of the proposed methods is illustrated via simulation studies and a crop yield prediction application. Supplementary materials for this article are available online.},
  keywords = {functional-additive-model,functional-data-analysis,functional-principal-components,reproducing-kernel-hilbert-space,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2019/Wong et al_2019_Partially Linear Functional Additive Models for Multivariate Functional Data.pdf}
}

@misc{xieFunctionalPCADeep2023,
  title = {Functional {{PCA}} and {{Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification}} with {{Transient Experimental Data}}},
  author = {Xie, Ziyu and Yaseen, Mahmoud and Wu, Xu},
  year = {2023},
  month = jul,
  number = {arXiv:2307.05592},
  eprint = {2307.05592},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.05592},
  urldate = {2023-10-27},
  abstract = {Inverse UQ is the process to inversely quantify the model input uncertainties based on experimental data. This work focuses on developing an inverse UQ process for time-dependent responses, using dimensionality reduction by functional principal component analysis (PCA) and deep neural network (DNN)-based surrogate models. The demonstration is based on the inverse UQ of TRACE physical model parameters using the FEBA transient experimental data. The measurement data is time-dependent peak cladding temperature (PCT). Since the quantity-of-interest (QoI) is time-dependent that corresponds to infinite-dimensional responses, PCA is used to reduce the QoI dimension while preserving the transient profile of the PCT, in order to make the inverse UQ process more efficient. However, conventional PCA applied directly to the PCT time series profiles can hardly represent the data precisely due to the sudden temperature drop at the time of quenching. As a result, a functional alignment method is used to separate the phase and amplitude information of the transient PCT profiles before dimensionality reduction. DNNs are then trained using PC scores from functional PCA to build surrogate models of TRACE in order to reduce the computational cost in Markov Chain Monte Carlo sampling. Bayesian neural networks are used to estimate the uncertainties of DNN surrogate model predictions. In this study, we compared four different inverse UQ processes with different dimensionality reduction methods and surrogate models. The proposed approach shows an improvement in reducing the dimension of the TRACE transient simulations, and the forward propagation of inverse UQ results has a better agreement with the experimental data.},
  archiveprefix = {arxiv},
  keywords = {deep-learning,functional-principal-components,neural-network,uncertainty-quantification},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Xie et al_2023_Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty.pdf;/Users/steven/Zotero/storage/Y32WDDBP/2307.html}
}

@article{yamamotoClusteringFunctionalData2012,
  title = {Clustering of Functional Data in a Low-Dimensional Subspace},
  author = {Yamamoto, Michio},
  year = {2012},
  month = oct,
  journal = {Advances in Data Analysis and Classification},
  volume = {6},
  number = {3},
  pages = {219--247},
  issn = {1862-5355},
  doi = {10.1007/s11634-012-0113-3},
  urldate = {2023-11-21},
  abstract = {To find optimal clusters of functional objects in a lower-dimensional subspace of data, a sequential method called tandem analysis, is often used, though such a method is problematic. A new procedure is developed to find optimal clusters of functional objects and also find an optimal subspace for clustering, simultaneously. The method is based on the k-means criterion for functional data and seeks the subspace that is maximally informative about the clustering structure in the data. An efficient alternating least-squares algorithm is described, and the proposed method is extended to a regularized method. Analyses of artificial and real data examples demonstrate that the proposed method gives correct and interpretable results.},
  langid = {english},
  keywords = {clustering,dimension-reduction,functional-data-analysis,smoothing-splines},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Advances in Data Analysis and Classification/2012/Yamamoto_2012_Clustering of functional data in a low-dimensional subspace.pdf}
}

@article{yamamotoDimensionReducedClusteringFunctional2017,
  title = {Dimension-{{Reduced Clustering}} of {{Functional Data}} via {{Subspace Separation}}},
  author = {Yamamoto, Michio and Hwang, Heungsun},
  year = {2017},
  month = jul,
  journal = {Journal of Classification},
  volume = {34},
  number = {2},
  pages = {294--326},
  issn = {1432-1343},
  doi = {10.1007/s00357-017-9232-z},
  urldate = {2023-11-21},
  abstract = {We propose a new method for finding an optimal cluster structure of functions as well as an optimal subspace for clustering simultaneously. The proposed method aims to minimize a distance between functional objects and their projections with the imposition of clustering penalties. It includes existing approaches to functional cluster analysis and dimension reduction, such as functional principal component k-means (Yamamoto, 2012) and functional factorial k-means (Yamamoto and Terada, 2014), as special cases. We show that these existing methods can perform poorly when a disturbing structure exists and that the proposed method can overcome this drawback by using subspace separation. A novel model selection procedure has been proposed, which can also be applied to other joint analyses of dimension reduction and clustering. We apply the proposed method to artificial and real data to demonstrate its performance as compared to the extant approaches.},
  langid = {english},
  keywords = {clustering,dimension-reduction,functional-data-analysis,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Classification/2017/Yamamoto_Hwang_2017_Dimension-Reduced Clustering of Functional Data via Subspace Separation.pdf}
}

@article{yamamotoFunctionalFactorialKmeans2014,
  title = {Functional Factorial {{K-means}} Analysis},
  author = {Yamamoto, Michio and Terada, Yoshikazu},
  year = {2014},
  month = nov,
  journal = {Computational Statistics \& Data Analysis},
  volume = {79},
  pages = {133--148},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2014.05.010},
  urldate = {2023-11-21},
  abstract = {A new procedure for simultaneously finding the optimal cluster structure of multivariate functional objects and finding the subspace to represent the cluster structure is presented. The method is based on the k-means criterion for projected functional objects on a subspace in which a cluster structure exists. An efficient alternating least-squares algorithm is described, and the proposed method is extended to a regularized method for smoothness of weight functions. To deal with the negative effect of the correlation of the coefficient matrix of the basis function expansion in the proposed algorithm, a two-step approach to the proposed method is also described. Analyses of artificial and real data demonstrate that the proposed method gives correct and interpretable results compared with existing methods, the functional principal component k-means (FPCK) method and tandem clustering approach. It is also shown that the proposed method can be considered complementary to FPCK.},
  keywords = {clustering,dimension-reduction,functional-data-analysis,smoothness},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Computational Statistics & Data Analysis/2014/Yamamoto_Terada_2014_Functional factorial K-means analysis.pdf;/Users/steven/Zotero/storage/WEGQ9355/S0167947314001509.html}
}

@article{yangGroupStructureImportant2022,
  title = {Is the {{Group Structure Important}} in {{Grouped Functional Time Series}}?},
  author = {Yang, Yang and Shang, Han Lin},
  year = {2022},
  month = jan,
  journal = {Journal of Data Science},
  volume = {20},
  number = {3},
  pages = {303--324},
  publisher = {{School of Statistics, Renmin University of China}},
  issn = {1680-743X, 1683-8602},
  doi = {10.6339/21-JDS1031},
  urldate = {2023-10-24},
  abstract = {We study the importance of group structure in grouped functional time series. Due to the non-uniqueness of group structure, we investigate different disaggregation structures in grouped functional time series. We address a practical question on whether or not the group structure can affect forecast accuracy. Using a dynamic multivariate functional time series method, we consider joint modeling and forecasting multiple series. Illustrated by Japanese sub-national age-specific mortality rates from 1975 to 2016, we investigate one- to 15-step-ahead point and interval forecast accuracies for the two group structures.},
  langid = {english},
  keywords = {functional-data-analysis,functional-principal-components,functional-time-series,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of Data Science/2022/Yang_Shang_2022_Is the Group Structure Important in Grouped Functional Time Series.pdf}
}

@article{yangNonParametricBayesianCovariateDependent2022,
  title = {Non-{{Parametric Bayesian Covariate-Dependent Multivariate Functional Clustering}}: {{An Application}} to {{Time-Series Data}} for {{Multiple Air Pollutants}}},
  shorttitle = {Non-{{Parametric Bayesian Covariate-Dependent Multivariate Functional Clustering}}},
  author = {Yang, Daewon and Choi, Taeryon and Lavigne, Eric and Chung, Yeonseung},
  year = {2022},
  month = nov,
  journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume = {71},
  number = {5},
  pages = {1521--1542},
  issn = {0035-9254},
  doi = {10.1111/rssc.12589},
  urldate = {2023-10-24},
  abstract = {Air pollution is a major threat to public health. Understanding the spatial distribution of air pollution concentration is of great interest to government or local authorities, as it informs about target areas for implementing policies for air quality management. Cluster analysis has been popularly used to identify groups of locations with similar profiles of average levels of multiple air pollutants, efficiently summarising the spatial pattern. This study aimed to cluster locations based on the seasonal patterns of multiple air pollutants incorporating the location-specific characteristics such as socio-economic indicators. For this purpose, we proposed a novel non-parametric Bayesian sparse latent factor model for covariate-dependent multivariate functional clustering. Furthermore, we extend this model to conduct clustering with temporal dependency. The proposed methods are illustrated through a simulation study and applied to time-series data for daily mean concentrations of ozone (O3), nitrogen dioxide (NO2), and fine particulate matter (PM2.5) collected for 25 cities in Canada in 1986{\textendash}2015.},
  keywords = {bayesian-analysis,clustering,ecology,functional-data-analysis,multivariate-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society Series C Applied Statistics/2022/Yang et al_2022_Non-Parametric Bayesian Covariate-Dependent Multivariate Functional Clustering.pdf;/Users/steven/Zotero/storage/GT2IY22W/7073318.html}
}

@article{yaoFunctionalDataAnalysis2005,
  title = {Functional {{Data Analysis}} for {{Sparse Longitudinal Data}}},
  author = {Yao, Fang and M{\"u}ller, Hans-Georg and Wang, Jane-Ling},
  year = {2005},
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  eprint = {27590579},
  eprinttype = {jstor},
  pages = {577--590},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  urldate = {2023-08-17},
  abstract = {We propose a nonparametric method to perform functional principal components analysis for the case of sparse longitudinal data. The method aims at irregularly spaced longitudinal data, where the number of repeated measurements available per subject is small. In contrast, classical functional data analysis requires a large number of regularly spaced measurements per subject. We assume that the repeated measurements are located randomly with a random number of repetitions for each subject and are determined by an underlying smooth random (subject-specific) trajectory plus measurement errors. Basic elements of our approach are the parsimonious estimation of the covariance structure and mean function of the trajectories, and the estimation of the variance of the measurement errors. The eigenfunction basis is estimated from the data, and functional principal components score estimates are obtained by a conditioning step. This conditional estimation method is conceptually simple and straightforward to implement. A key step is the derivation of asymptotic consistency and distribution results under mild conditions, using tools from functional analysis. Functional data analysis for sparse longitudinal data enables prediction of individual smooth trajectories even if only one or few measurements are available for a subject. Asymptotic pointwise and simultaneous confidence bands are obtained for predicted individual trajectories, based on asymptotic distributions, for simultaneous bands under the assumption of a finite number of components. Model selection techniques, such as the Akaike information criterion, are used to choose the model dimension corresponding to the number of eigenfunctions in the model. The methods are illustrated with a simulation study, longitudinal CD4 data for a sample of AIDS patients, and time-course gene expression data for the yeast cell cycle.},
  keywords = {functional-data-analysis,functional-principal-components,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the American Statistical Association/2005/Yao et al_2005_Functional Data Analysis for Sparse Longitudinal Data.pdf}
}

@misc{yargerDetectingChangepointsGloballyindexed2023,
  title = {Detecting Changepoints in Globally-Indexed Functional Time Series},
  author = {Yarger, Drew and Tucker, J. Derek},
  year = {2023},
  month = aug,
  number = {arXiv:2308.05915},
  eprint = {2308.05915},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.05915},
  urldate = {2023-10-27},
  abstract = {In environmental and climate data, there is often an interest in determining if and when changes occur in a system. Such changes may result from localized sources in space and time like a volcanic eruption or climate geoengineering events. Detecting such events and their subsequent influence on climate has important policy implications. However, the climate system is complex, and such changes can be challenging to detect. One statistical perspective for changepoint detection is functional time series, where one observes an entire function at each time point. We will consider the context where each time point is a year, and we observe a function of temperature indexed by day of the year. Furthermore, such data is measured at many spatial locations on Earth, which motivates accommodating sets of functional time series that are spatially-indexed on a sphere. Simultaneously inferring changes that can occur at different times for different locations is challenging. We propose test statistics for detecting these changepoints, and we evaluate performance using varying levels of data complexity, including a simulation study, simplified climate model simulations, and climate reanalysis data. We evaluate changes in stratospheric temperature globally over 1984-1998. Such changes may be associated with the eruption of Mt. Pinatubo in 1991.},
  archiveprefix = {arxiv},
  keywords = {change-point-analysis,ecology,functional-data-analysis,spatial-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Yarger_Tucker_2023_Detecting changepoints in globally-indexed functional time series.pdf;/Users/steven/Zotero/storage/87U2B7CF/2308.html}
}

@misc{yingCausalityFunctionalLongitudinal2023,
  title = {Causality of {{Functional Longitudinal Data}}},
  author = {Ying, Andrew},
  year = {2023},
  month = oct,
  number = {arXiv:2206.12525},
  eprint = {2206.12525},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.12525},
  urldate = {2023-10-27},
  abstract = {"Treatment-confounder feedback" is the central complication to resolve in longitudinal studies, to infer causality. The existing frameworks for identifying causal effects for longitudinal studies with discrete repeated measures hinge heavily on assuming that time advances in discrete time steps or treatment changes as a jumping process, rendering the number of "feedbacks" finite. However, medical studies nowadays with real-time monitoring involve functional time-varying outcomes, treatment, and confounders, which leads to an uncountably infinite number of feedbacks between treatment and confounders. Therefore more general and advanced theory is needed. We generalize the definition of causal effects under user-specified stochastic treatment regimes to longitudinal studies with continuous monitoring and develop an identification framework, allowing right censoring and truncation by death. We provide sufficient identification assumptions including a generalized consistency assumption, a sequential randomization assumption, a positivity assumption, and a novel "intervenable" assumption designed for the continuous-time case. Under these assumptions, we propose a g-computation process and an inverse probability weighting process, which suggest a g-computation formula and an inverse probability weighting formula for identification. For practical purposes, we also construct two classes of population estimating equations to identify these two processes, respectively, which further suggest a doubly robust identification formula with extra robustness against process misspecification. We prove that our framework fully generalize the existing frameworks and is nonparametric.},
  archiveprefix = {arxiv},
  keywords = {causality,functional-data-analysis,stochastic-analysis},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Ying_2023_Causality of Functional Longitudinal Data.pdf;/Users/steven/Zotero/storage/P2MAC965/2206.html}
}

@article{zhangReviewClusteringMethods2023,
  title = {Review of {{Clustering Methods}} for {{Functional Data}}},
  author = {Zhang, Mimi and Parnell, Andrew},
  year = {2023},
  month = apr,
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {17},
  number = {7},
  pages = {91:1--91:34},
  issn = {1556-4681},
  doi = {10.1145/3581789},
  urldate = {2023-10-31},
  abstract = {Functional data clustering is to identify heterogeneous morphological patterns in the continuous functions underlying the discrete measurements/observations. Application of functional data clustering has appeared in many publications across various fields of sciences, including but not limited to biology, (bio)chemistry, engineering, environmental science, medical science, psychology, social science, and so on. The phenomenal growth of the application of functional data clustering indicates the urgent need for a systematic approach to develop efficient clustering methods and scalable algorithmic implementations. On the other hand, there is abundant literature on the cluster analysis of time series, trajectory data, spatio-temporal data, and so on, which are all related to functional data. Therefore, an overarching structure of existing functional data clustering methods will enable the cross-pollination of ideas across various research fields. We here conduct a comprehensive review of original clustering methods for functional data. We propose a systematic taxonomy that explores the connections and differences among the existing functional data clustering methods and relates them to the conventional multivariate clustering methods. The structure of the taxonomy is built on three main attributes of a functional data clustering method and therefore is more reliable than existing categorizations. The review aims to bridge the gap between the functional data analysis community and the clustering community and to generate new principles for functional data clustering.},
  keywords = {clustering,curve-registration,functional-data,multivariate-functional-data,review},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/ACM Transactions on Knowledge Discovery from Data/2023/Zhang_Parnell_2023_Review of Clustering Methods for Functional Data.pdf}
}

@misc{zhangRobustBayesianFunctional2023,
  title = {Robust {{Bayesian Functional Principal Component Analysis}}},
  author = {Zhang, Jiarui and Cao, Jiguo and Wang, Liangliang},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09731},
  eprint = {2307.09731},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.09731},
  urldate = {2023-10-27},
  abstract = {We develop a robust Bayesian functional principal component analysis (FPCA) by incorporating skew elliptical classes of distributions. The proposed method effectively captures the primary source of variation among curves, even when abnormal observations contaminate the data. We model the observations using skew elliptical distributions by introducing skewness with transformation and conditioning into the multivariate elliptical symmetric distribution. To recast the covariance function, we employ an approximate spectral decomposition. We discuss the selection of prior specifications and provide detailed information on posterior inference, including the forms of the full conditional distributions, choices of hyperparameters, and model selection strategies. Furthermore, we extend our model to accommodate sparse functional data with only a few observations per curve, thereby creating a more general Bayesian framework for FPCA. To assess the performance of our proposed model, we conduct simulation studies comparing it to well-known frequentist methods and conventional Bayesian methods. The results demonstrate that our method outperforms existing approaches in the presence of outliers and performs competitively in outlier-free datasets. Furthermore, we illustrate the effectiveness of our method by applying it to environmental and biological data to identify outlying functional data. The implementation of our proposed method and applications are available at https://github.com/SFU-Stat-ML/RBFPCA.},
  archiveprefix = {arxiv},
  keywords = {bayesian-analysis,functional-data-analysis,functional-principal-components,outliers-detection,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/arXiv/2023/Zhang et al_2023_Robust Bayesian Functional Principal Component Analysis.pdf;/Users/steven/Zotero/storage/2DNIXYIE/2307.html}
}

@article{zhangSparseDenseFunctional2016,
  title = {From Sparse to Dense Functional Data and Beyond},
  author = {Zhang, Xiaoke and Wang, Jane-Ling},
  year = {2016},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {44},
  number = {5},
  pages = {2281--2321},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/16-AOS1446},
  urldate = {2023-08-24},
  abstract = {Nonparametric estimation of mean and covariance functions is important in functional data analysis. We investigate the performance of local linear smoothers for both mean and covariance functions with a general weighing scheme, which includes two commonly used schemes, equal weight per observation (OBS), and equal weight per subject (SUBJ), as two special cases. We provide a comprehensive analysis of their asymptotic properties on a unified platform for all types of sampling plan, be it dense, sparse or neither. Three types of asymptotic properties are investigated in this paper: asymptotic normality, \$L\^\{2\}\$ convergence and uniform convergence. The asymptotic theories are unified on two aspects: (1) the weighing scheme is very general; (2) the magnitude of the number \$N\_\{i\}\$ of measurements for the \$i\$th subject relative to the sample size \$n\$ can vary freely. Based on the relative order of \$N\_\{i\}\$ to \$n\$, functional data are partitioned into three types: non-dense, dense and ultra-dense functional data for the OBS and SUBJ schemes. These two weighing schemes are compared both theoretically and numerically. We also propose a new class of weighing schemes in terms of a mixture of the OBS and SUBJ weights, of which theoretical and numerical performances are examined and compared.},
  keywords = {asymptotic-theory,functional-data-analysis,local-polynomial-smoothing,sparse-functional-data},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2016/Zhang_Wang_2016_From sparse to dense functional data and beyond.pdf}
}

@article{zhangStatisticalInferencesFunctional2007,
  title = {Statistical Inferences for Functional Data},
  author = {Zhang, Jin-Ting and Chen, Jianwei},
  year = {2007},
  month = jul,
  journal = {The Annals of Statistics},
  volume = {35},
  number = {3},
  pages = {1052--1079},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/009053606000001505},
  urldate = {2023-07-08},
  abstract = {With modern technology development, functional data are being observed frequently in many scientific fields. A popular method for analyzing such functional data is ``smoothing first, then estimation.'' That is, statistical inference such as estimation and hypothesis testing about functional data is conducted based on the substitution of the underlying individual functions by their reconstructions obtained by one smoothing technique or another. However, little is known about this substitution effect on functional data analysis. In this paper this problem is investigated when the local polynomial kernel (LPK) smoothing technique is used for individual function reconstructions. We find that under some mild conditions, the substitution effect can be ignored asymptotically. Based on this, we construct LPK reconstruction-based estimators for the mean, covariance and noise variance functions of a functional data set and derive their asymptotics. We also propose a GCV rule for selecting good bandwidths for the LPK reconstructions. When the mean function also depends on some time-independent covariates, we consider a functional linear model where the mean function is linearly related to the covariates but the covariate effects are functions of time. The LPK reconstruction-based estimators for the covariate effects and the covariance function are also constructed and their asymptotics are derived. Moreover, we propose a L2-norm-based global test statistic for a general hypothesis testing problem about the covariate effects and derive its asymptotic random expression. The effect of the bandwidths selected by the proposed GCV rule on the accuracy of the LPK reconstructions and the mean function estimator is investigated via a simulation study. The proposed methodologies are illustrated via an application to a real functional data set collected in climatology.},
  keywords = {functional-data-analysis,local-polynomial-smoothing,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/The Annals of Statistics/2007/Zhang_Chen_2007_Statistical inferences for functional data.pdf}
}

@article{zhongClusterNonGaussianFunctional2021,
  title = {Cluster Non-{{Gaussian}} Functional Data},
  author = {Zhong, Qingzhi and Lin, Huazhen and Li, Yi},
  year = {2021},
  journal = {Biometrics},
  volume = {77},
  number = {3},
  pages = {852--865},
  issn = {1541-0420},
  doi = {10.1111/biom.13349},
  urldate = {2023-11-21},
  abstract = {Gaussian distributions have been commonly assumed when clustering functional data. When the normality condition fails, biased results will follow. Additional challenges occur as the number of the clusters is often unknown a priori. This paper focuses on clustering non-Gaussian functional data without the prior information of the number of clusters. We introduce a semiparametric mixed normal transformation model to accommodate non-Gaussian functional data, and propose a penalized approach to simultaneously estimate the parameters, transformation function, and the number of clusters. The estimators are shown to be consistent and asymptotically normal. The practical utility of the methods is confirmed via simulations as well as an application of the analysis of Alzheimer's disease study. The proposed method yields much less classification error than the existing methods. Data used in preparation of this paper were obtained from the Alzheimer's Disease Neuroimaging Initiative database.},
  copyright = {{\copyright} 2020 The International Biometric Society},
  langid = {english},
  keywords = {clustering,functional-data-analysis,functional-principal-components,non-gaussian-model,non-parametric-statistics},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Biometrics/2021/Zhong et al_2021_Cluster non-Gaussian functional data.pdf;/Users/steven/Zotero/storage/WHWKS2HS/biom.html}
}

@article{zhouStatisticalInferenceHighdimensional2023,
  title = {Statistical Inference for High-Dimensional Panel Functional Time Series},
  author = {Zhou, Zhou and Dette, Holger},
  year = {2023},
  month = apr,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {85},
  number = {2},
  pages = {523--549},
  issn = {1369-7412},
  doi = {10.1093/jrsssb/qkad015},
  urldate = {2023-09-19},
  abstract = {In this paper, we develop statistical inference tools for high-dimensional functional time series. We introduce a new concept of physical dependent processes in the space of square integrable functions, which adopts the idea of basis decomposition of functional data in these spaces, and derive Gaussian and multiplier bootstrap approximations for sums of high-dimensional functional time series. These results have numerous important statistical consequences. Exemplarily, we consider the development of joint simultaneous confidence bands for the mean functions and the construction of tests for the hypotheses that the mean functions in the panel dimension are parallel. The results are illustrated by means of a small simulation study and in the analysis of Canadian temperature data.},
  keywords = {functional-data-analysis,functional-time-series,simultaneous-confidence-bands},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society Series B Statistical Methodology/2023/Zhou_Dette_2023_Statistical inference for high-dimensional panel functional time series.pdf;/Users/steven/Zotero/storage/4R5WMN5K/7099701.html}
}

@article{zhuStructuredFunctionalAdditive2014,
  title = {Structured Functional Additive Regression in Reproducing Kernel {{Hilbert}} Spaces},
  author = {Zhu, Hongxiao and Yao, Fang and Zhang, Hao Helen},
  year = {2014},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {76},
  number = {3},
  eprint = {24774530},
  eprinttype = {jstor},
  pages = {581--603},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  urldate = {2023-10-27},
  abstract = {Functional additive models provide a flexible yet simple framework for regressions involving functional predictors. The utilization of a data-driven basis in an additive rather than linear structure naturally extends the classical functional linear model. However, the critical issue of selecting non-linear additive components has been less studied. In this work, we propose a new regularization framework for structure estimation in the context of reproducing kernel Hilbert spaces. The approach proposed takes advantage of functional principal components which greatly facilitates implementation and theoretical analysis. The selection and estimation are achieved by penalized least squares using a penalty which encourages the sparse structure of the additive components. Theoretical properties such as the rate of convergence are investigated. The empirical performance is demonstrated through simulation studies and a real data application.},
  keywords = {functional-additive-model,functional-data-analysis,functional-principal-components,reproducing-kernel-hilbert-space},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Journal of the Royal Statistical Society. Series B (Statistical Methodology)/2014/Zhu et al_2014_Structured functional additive regression in reproducing kernel Hilbert spaces.pdf}
}

@book{zucchiniHiddenMarkovModels2016,
  title = {Hidden {{Markov Models}} for {{Time Series}}: {{An Introduction Using R}}, {{Second Edition}}},
  shorttitle = {Hidden {{Markov Models}} for {{Time Series}}},
  author = {Zucchini, Walter and MacDonald, Iain L. and Langrock, Roland},
  year = {2016},
  month = jul,
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/b20790},
  abstract = {Hidden Markov Models for Time Series: An Introduction Using R, Second Edition illustrates the great flexibility of hidden Markov models (HMMs) as general-purpose models for time series data. The book provides a broad understanding of the models and their uses. After presenting the basic model formulation, the book covers estimation, forecasting, decoding, prediction, model selection, and Bayesian inference for HMMs. Through examples and applications, the authors describe how to extend and generalize the basic model so that it can be applied in a rich variety of situations. The book demonstrates how HMMs can be applied to a wide range of types of time series: continuous-valued, circular, multivariate, binary, bounded and unbounded counts, and categorical observations. It also discusses how to employ the freely available computing environment R to carry out the computations. Features Presents an accessible overview of HMMs Explores a variety of applications in ecology, finance, epidemiology, climatology, and sociology Includes numerous theoretical and programming exercises Provides most of the analysed data sets online New to the second edition A total of five chapters on extensions, including HMMs for longitudinal data, hidden semi-Markov models and models with continuous-valued state process New case studies on animal movement, rainfall occurrence and capture-recapture data},
  isbn = {978-1-315-37248-8},
  keywords = {markov-process,r-software,time-series},
  file = {/Users/steven/Library/CloudStorage/GoogleDrive-steven.golovkine@ul.ie/My Drive/bibliography/Chapman and Hall/CRC/2016/Langrock_2016_Hidden Markov Models for Time Series.pdf}
}
